{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9127163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Đây có phải là bức ảnh chụp nhiều độ phơi sáng của vận động viên trượt tuyết mặc áo đen không?', 'image': <PIL.Image.Image image mode=RGB size=640x480 at 0x7536AE6EEB60>, 'image_path': '/mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000393271.jpg', 'explanation': ['hình người mặc cùng một bộ quần áo khi trượt xuống dốc', 'có vẻ như là cùng một người trượt tuyết làm những pha nhào lộn khác nhau', 'cùng một người trượt tuyết xuất hiện nhiều lần'], 'answer': 'có', 'question_id': '393271001'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Đường dẫn\n",
    "json_path = \"/mnt/VLAI_data/ViVQA-X/ViVQA-X_val.json\"\n",
    "coco_img_dir = \"/mnt/VLAI_data/COCO_Images/val2014/\"\n",
    "\n",
    "# Đọc file JSON\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "samples = []\n",
    "for item in data:\n",
    "    img_path = os.path.join(coco_img_dir, item[\"image_name\"])\n",
    "    # Mở ảnh thành PIL Image\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    sample = {\n",
    "        \"question\": item[\"question\"],\n",
    "        \"image\": image,  # <-- PIL Image object\n",
    "        \"image_path\": img_path,\n",
    "        \"explanation\": item[\"explanation\"],  # list\n",
    "        \"answer\": item[\"answer\"],\n",
    "        \"question_id\": item[\"question_id\"]\n",
    "    }\n",
    "    samples.append(sample)\n",
    "\n",
    "# Kiểm tra 1 sample\n",
    "print(samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f25d6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_VIVQA_ENHANCED = (\n",
    "    \"<image>\\nBạn là một trợ lý AI chuyên gia, có khả năng phân tích hình ảnh một cách cẩn thận và đa nghi. \"\n",
    "    \"Nhiệm vụ của bạn là trả lời câu hỏi của người dùng dựa trên hình ảnh được cung cấp. \"\n",
    "    \"Trước tiên, hãy thực hiện một chuỗi suy luận chi tiết bên trong cặp thẻ <thinking></thinking>. \"\n",
    "    \"Sau khi hoàn tất quá trình suy luận, hãy cung cấp câu trả lời cuối cùng theo đúng định dạng yêu cầu.\\n\\n\"\n",
    "    \"Câu hỏi: {question}\\n\\n\"\n",
    "    \"ĐỊNH DẠNG BẮT BUỘC:\\n\"\n",
    "    \"<thinking>\\n\"\n",
    "    \"<SUMMARY>[Tóm tắt ngắn gọn về hình ảnh và yêu cầu của câu hỏi]</SUMMARY>\\n\"\n",
    "    \"<ANALYSIS>[Phân tích các chi tiết, vật thể, văn bản trong ảnh có liên quan trực tiếp đến câu hỏi. Liệt kê các bằng chứng quan sát được.]</ANALYSIS>\\n\"\n",
    "    \"<REASONING_STEPS>[Trình bày quá trình lập luận logic từng bước một. Từ các bằng chứng đã phân tích, làm thế nào để đi đến câu trả lời? Giải thích các mối liên hệ.]</REASONING_STEPS>\\n\"\n",
    "    \"<CONCLUSION>[Đưa ra kết luận cuối cùng từ quá trình lập luận trên.]</CONCLUSION>\\n\"\n",
    "    \"</thinking>\\n\"\n",
    "    \"<answer>[Điền câu trả lời trực tiếp và ngắn gọn vào đây]</answer>\\n\"\n",
    "    \"<explain>[Dựa vào quá trình suy luận trong <thinking>, giải thích cực kỳ ngắn gọn (trong khoảng 10-15 từ)]</explain>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fcc77ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/minhtq-vqanle/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/opt/miniconda3/envs/minhtq-vqanle/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"5CD-AI/Vintern-3B-R-beta\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    ").eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9112f886",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers_modules._5CD_hyphen_AI.Vintern_hyphen_3B_hyphen_R_hyphen_beta.4fd34d713dfca446cdecc00d921f5038909e3efb.configuration_internvl_chat.InternVLChatConfig'> for this kind of AutoModel: AutoModelForImageTextToText.\nModel type should be one of AriaConfig, AyaVisionConfig, BlipConfig, Blip2Config, ChameleonConfig, Cohere2VisionConfig, DeepseekVLConfig, DeepseekVLHybridConfig, Emu3Config, EvollaConfig, Florence2Config, FuyuConfig, Gemma3Config, Gemma3nConfig, GitConfig, Glm4vConfig, Glm4vMoeConfig, GotOcr2Config, IdeficsConfig, Idefics2Config, Idefics3Config, InstructBlipConfig, InternVLConfig, JanusConfig, Kosmos2Config, Kosmos2_5Config, Lfm2VlConfig, Llama4Config, LlavaConfig, LlavaNextConfig, LlavaNextVideoConfig, LlavaOnevisionConfig, Mistral3Config, MllamaConfig, Ovis2Config, PaliGemmaConfig, PerceptionLMConfig, Pix2StructConfig, PixtralVisionConfig, Qwen2_5_VLConfig, Qwen2VLConfig, Qwen3VLConfig, Qwen3VLMoeConfig, ShieldGemma2Config, SmolVLMConfig, UdopConfig, VipLlavaConfig, VisionEncoderDecoderConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5CD-AI/Vintern-3B-R-beta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_checkpoint, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForImageTextToText\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     {\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     }\n\u001b[1;32m     18\u001b[0m ]\n\u001b[1;32m     20\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mapply_chat_template(messages, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/minhtq-vqanle/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:607\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    605\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    606\u001b[0m     )\n\u001b[0;32m--> 607\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers_modules._5CD_hyphen_AI.Vintern_hyphen_3B_hyphen_R_hyphen_beta.4fd34d713dfca446cdecc00d921f5038909e3efb.configuration_internvl_chat.InternVLChatConfig'> for this kind of AutoModel: AutoModelForImageTextToText.\nModel type should be one of AriaConfig, AyaVisionConfig, BlipConfig, Blip2Config, ChameleonConfig, Cohere2VisionConfig, DeepseekVLConfig, DeepseekVLHybridConfig, Emu3Config, EvollaConfig, Florence2Config, FuyuConfig, Gemma3Config, Gemma3nConfig, GitConfig, Glm4vConfig, Glm4vMoeConfig, GotOcr2Config, IdeficsConfig, Idefics2Config, Idefics3Config, InstructBlipConfig, InternVLConfig, JanusConfig, Kosmos2Config, Kosmos2_5Config, Lfm2VlConfig, Llama4Config, LlavaConfig, LlavaNextConfig, LlavaNextVideoConfig, LlavaOnevisionConfig, Mistral3Config, MllamaConfig, Ovis2Config, PaliGemmaConfig, PerceptionLMConfig, Pix2StructConfig, PixtralVisionConfig, Qwen2_5_VLConfig, Qwen2VLConfig, Qwen3VLConfig, Qwen3VLMoeConfig, ShieldGemma2Config, SmolVLMConfig, UdopConfig, VipLlavaConfig, VisionEncoderDecoderConfig."
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "model_checkpoint = \"5CD-AI/Vintern-3B-R-beta\"\n",
    "processor = AutoProcessor.from_pretrained(model_checkpoint, trust_remote_code=True)\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=\"auto\", dtype=torch.bfloat16, trust_remote_code=True)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"Please describe the image explicitly.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=50)\n",
    "decoded_output = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n",
    "\n",
    "decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d182ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "# Constants\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    \"\"\"Build image transformation pipeline\"\"\"\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    \"\"\"Find the closest aspect ratio from target ratios\"\"\"\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    \n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        \n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "                \n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    \"\"\"Dynamically preprocess image into multiple tiles\"\"\"\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "    \n",
    "    # Calculate target ratios\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) \n",
    "        for i in range(1, n + 1) \n",
    "        for j in range(1, n + 1) \n",
    "        if i * j <= max_num and i * j >= min_num\n",
    "    )\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "    \n",
    "    # Find closest aspect ratio\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size\n",
    "    )\n",
    "    \n",
    "    # Calculate target dimensions\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "    \n",
    "    # Resize and split image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    \n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    \n",
    "    # Add thumbnail if needed\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "        \n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    \"\"\"Load and preprocess image\"\"\"\n",
    "    if isinstance(image_file, str):\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    else:\n",
    "        image = image_file.convert('RGB')\n",
    "    \n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    \n",
    "    return pixel_values\n",
    "\n",
    "# Generation config\n",
    "generation_config = dict(\n",
    "    max_new_tokens=1024, \n",
    "    do_sample=False, \n",
    "    num_beams=3, \n",
    "    repetition_penalty=2.5\n",
    ")\n",
    "\n",
    "# Process your sample\n",
    "raw_image = samples[0]['image']\n",
    "question = samples[0]['question']\n",
    "\n",
    "# Load and preprocess image\n",
    "pixel_values = load_image(raw_image, max_num=6).to(torch.bfloat16).cuda()\n",
    "\n",
    "# Format question with image token\n",
    "final_prompt = f\"<image>\\n{question}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1) Lấy ID đúng của token ngữ cảnh ảnh\n",
    "img_ctx_id = tokenizer.convert_tokens_to_ids(\"<IMG_CONTEXT>\")\n",
    "model.img_context_token_id = img_ctx_id\n",
    "assert model.img_context_token_id is not None\n",
    "\n",
    "# 2) Tính số token ảnh cần chèn\n",
    "num_patches = pixel_values.shape[0]  # số “ô”/ảnh sau dynamic preprocess của bạn\n",
    "num_img_tokens = model.num_image_token * num_patches  # thường là 256 * num_patches với cấu hình mặc định\n",
    "\n",
    "# 3) Xây image span và prompt\n",
    "image_span = \"<img>\" + \"<IMG_CONTEXT>\" * num_img_tokens + \"</img>\"\n",
    "prompt = f\"<|im_start|>user\\n{image_span}\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "# 4) Tokenize + cấu hình generate\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "eos_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "gen_cfg = dict(\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    num_beams=3,\n",
    "    repetition_penalty=1.1,\n",
    "    eos_token_id=eos_id,\n",
    "    pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb4aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171c453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response using custom chat method (must model call AutoModel)\n",
    "response, history = model.chat(\n",
    "    tokenizer, \n",
    "    pixel_values, \n",
    "    final_prompt, \n",
    "    generation_config, \n",
    "    history=None, \n",
    "    return_history=True\n",
    ")\n",
    "\n",
    "print(f\"User: {question}\\nAssistant: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f6f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Gọi generate (chú ý: KHÔNG cần image_flags; model tìm theo IMG_CONTEXT)\n",
    "out = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    pixel_values=pixel_values,\n",
    "    **gen_cfg\n",
    ")\n",
    "\n",
    "# 6) Decode\n",
    "resp = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "resp = resp.split(\"</s>\")[0].split(\"<|im_end|>\")[0].strip()  # gọn gàng lại nếu cần\n",
    "print(\"Assistant:\", resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60783ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8789,  1.5703,  1.4922,  ..., -3.0312, -1.2812, -0.3125],\n",
       "         [ 6.1562, 10.3750,  8.1875,  ..., -0.1475,  1.0703,  1.4297],\n",
       "         [ 6.4688, 10.1250, 12.0000,  ..., -1.6406, -0.2715,  0.6562],\n",
       "         ...,\n",
       "         [ 0.8516, -3.7500,  0.0708,  ..., -3.9375, -1.6641, -0.9336],\n",
       "         [11.0625,  7.1875, 15.2500,  ..., -1.4062, -1.1953,  0.7695],\n",
       "         [ 4.6875, 10.8125,  8.6250,  ..., -4.0000, -2.5312, -1.5000]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "num_patches = pixel_values.shape[0]\n",
    "image_flags = torch.ones((num_patches, 1), dtype=torch.long, device=model.device)\n",
    "with torch.no_grad():\n",
    "    out = model(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        pixel_values=pixel_values,    \n",
    "        image_flags=image_flags,     \n",
    "        use_cache=False,                  \n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "logits = out.logits  # shape: [batch, seq_len, vocab_size]\n",
    "logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minhtq-vqanle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
