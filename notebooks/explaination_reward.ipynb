{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torch torchvision\n",
    "# !pip install -q torchmetrics[multimodal]\n",
    "# !pip install -q pycocoevalcap\n",
    "# !pip install -q Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='transformers.modeling_utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplanationRewardScorer:\n",
    "    def __init__(self, alpha: float = 0.5, clip_model_name: str = \"openai/clip-vit-base-patch16\"):\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(\"Alpha must be in [0, 1]\")\n",
    "            \n",
    "        self.alpha = alpha\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.cider_scorer = Cider()\n",
    "        print(\"CIDEr scorer initialized.\")\n",
    "\n",
    "        print(f\"Initializing CLIPScore on device: {self.device}\")\n",
    "        self.clip_metric = CLIPScore(model_name_or_path=clip_model_name).to(self.device)\n",
    "        print(\"CLIPScore model loaded and ready.\")\n",
    "\n",
    "    def calculate_cider_batch(self, ground_truths: dict, predictions: dict) -> dict:\n",
    "        res = {img_id: [caption] for img_id, caption in predictions.items()}\n",
    "        _, individual_scores_array = self.cider_scorer.compute_score(ground_truths, res)\n",
    "        \n",
    "        image_ids = list(predictions.keys())\n",
    "        cider_scores_dict = {img_id: score for img_id, score in zip(image_ids, individual_scores_array)}\n",
    "        return cider_scores_dict\n",
    "\n",
    "    def calculate_clip_batch(self, image_paths: dict, predictions: dict) -> dict:\n",
    "        clip_scores = {}\n",
    "        for img_id, pred_caption in predictions.items():\n",
    "            image_path = image_paths[img_id]\n",
    "            try:\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: Image not found at {image_path}. Skipping.\")\n",
    "                clip_scores[img_id] = 0.0\n",
    "                continue\n",
    "            \n",
    "            image_np = np.array(image)\n",
    "            image_tensor = torch.from_numpy(image_np).permute(2, 0, 1).unsqueeze(0)\n",
    "            \n",
    "            self.clip_metric.update(image_tensor.to(self.device), [pred_caption])\n",
    "            score_tensor = self.clip_metric.compute()\n",
    "            self.clip_metric.reset()\n",
    "            \n",
    "            clip_scores[img_id] = score_tensor.item()\n",
    "            \n",
    "        return clip_scores\n",
    "\n",
    "    def explanation_rewards(self, ground_truths: list[list[str]], predictions: list[str], image_paths: list[str]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ground_truths (list[list[str]]): Dạng [[\"caption 1a\", ...], [\"caption 2a\", ...]]\n",
    "            predictions (list[str]): Dạng [\"prediction 1\", \"prediction 2\", ...]\n",
    "            image_paths (list[str]): Dạng [\"path/to/image1.jpg\", \"path/to/image2.jpg\", ...]\n",
    "\n",
    "        Returns:\n",
    "            list[float]\n",
    "        \"\"\"\n",
    "        assert len(ground_truths) == len(predictions) == len(image_paths), \\\n",
    "            \"Input lists must have the same length.\"\n",
    "\n",
    "        gts_dict = {i: gt for i, gt in enumerate(ground_truths)}\n",
    "        preds_dict = {i: pred for i, pred in enumerate(predictions)}\n",
    "        paths_dict = {i: path for i, path in enumerate(image_paths)}\n",
    "\n",
    "        print(\"Calculating CIDEr scores...\")\n",
    "        cider_scores = self.calculate_cider_batch(gts_dict, preds_dict)\n",
    "\n",
    "        print(\"Calculating CLIP scores...\")\n",
    "        clip_scores = self.calculate_clip_batch(paths_dict, preds_dict)\n",
    "\n",
    "        print(\"Combining scores to generate final rewards...\")\n",
    "        final_rewards = []\n",
    "        for i in range(len(predictions)):\n",
    "            cider_score = cider_scores.get(i, 0.0)\n",
    "            \n",
    "            clip_score_raw = clip_scores.get(i, 0.0)\n",
    "            clip_score_normalized = max(0, (clip_score_raw - 15) / (35 - 15))\n",
    "\n",
    "            reward = self.alpha * cider_score + (1.0 - self.alpha) * clip_score_normalized\n",
    "            final_rewards.append(reward) \n",
    "            \n",
    "            print(f\"  Index {i}: CIDEr={cider_score:.2f}, CLIP={clip_score_raw:.2f} -> Reward={reward:.4f}\")\n",
    "\n",
    "        return final_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths_list = [\n",
    "    [\"một chiếc ô tô màu đỏ đậu trên đường phố\", \"siêu xe thể thao màu đỏ\"],\n",
    "    [\"chú chó đang chơi trên bãi cỏ xanh\", \"một chú chó lông vàng chạy trong công viên\"]\n",
    "]\n",
    "\n",
    "\n",
    "predictions_list = [\n",
    "    \"một chiếc xe hơi màu đỏ\",\n",
    "    \"con chó vui vẻ trên đồng cỏ\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    Image.new('RGB', (224, 224), color = 'red').save(\"car_101.jpg\")\n",
    "    Image.new('RGB', (224, 224), color = 'green').save(\"dog_102.jpg\")\n",
    "    image_paths_list = [\n",
    "        \"car_101.jpg\",\n",
    "        \"dog_102.jpg\"\n",
    "    ]\n",
    "\n",
    "    reward_scorer = ExplanationRewardScorer(alpha=0.5)\n",
    "\n",
    "    final_rewards = reward_scorer.explanation_rewards(\n",
    "        ground_truths=ground_truths_list,\n",
    "        predictions=predictions_list,\n",
    "        image_paths=image_paths_list\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Final Reward ---\")\n",
    "    print(final_rewards)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
