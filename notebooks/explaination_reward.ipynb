{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torch torchvision\n",
    "# !pip install -q torchmetrics[multimodal]\n",
    "# !pip install -q pycocoevalcap\n",
    "# !pip install -q Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='transformers.modeling_utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplanationRewardScorer:\n",
    "    def __init__(self, alpha: float = 0.5, clip_model_name: str = \"openai/clip-vit-base-patch16\"):\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(\"Alpha must be in [0, 1]\")\n",
    "            \n",
    "        self.alpha = alpha\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.cider_scorer = Cider()\n",
    "        print(\"CIDEr scorer initialized.\")\n",
    "\n",
    "        print(f\"Initializing CLIPScore on device: {self.device}\")\n",
    "        self.clip_metric = CLIPScore(model_name_or_path=clip_model_name).to(self.device)\n",
    "        print(\"CLIPScore model loaded and ready.\")\n",
    "\n",
    "    def calculate_cider_batch(self, ground_truths: dict, predictions: dict) -> dict:\n",
    "        res = {img_id: [caption] for img_id, caption in predictions.items()}\n",
    "        _, individual_scores_array = self.cider_scorer.compute_score(ground_truths, res)\n",
    "        \n",
    "        image_ids = list(predictions.keys())\n",
    "        cider_scores_dict = {img_id: score for img_id, score in zip(image_ids, individual_scores_array)}\n",
    "        return cider_scores_dict\n",
    "\n",
    "    def calculate_clip_batch(self, image_paths: dict, predictions: dict) -> dict:\n",
    "        clip_scores = {}\n",
    "        for img_id, pred_caption in predictions.items():\n",
    "            image_path = image_paths[img_id]\n",
    "            try:\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: Image not found at {image_path}. Skipping.\")\n",
    "                clip_scores[img_id] = 0.0\n",
    "                continue\n",
    "            \n",
    "            image_np = np.array(image)\n",
    "            image_tensor = torch.from_numpy(image_np).permute(2, 0, 1).unsqueeze(0)\n",
    "            \n",
    "            self.clip_metric.update(image_tensor.to(self.device), [pred_caption])\n",
    "            score_tensor = self.clip_metric.compute()\n",
    "            self.clip_metric.reset()\n",
    "            \n",
    "            clip_scores[img_id] = score_tensor.item()\n",
    "            \n",
    "        return clip_scores\n",
    "\n",
    "    def explanation_rewards(self, ground_truths: list[list[str]], predictions: list[str], image_paths: list[str]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ground_truths (list[list[str]]): Dạng [[\"caption 1a\", ...], [\"caption 2a\", ...]]\n",
    "            predictions (list[str]): Dạng [\"prediction 1\", \"prediction 2\", ...]\n",
    "            image_paths (list[str]): Dạng [\"path/to/image1.jpg\", \"path/to/image2.jpg\", ...]\n",
    "\n",
    "        Returns:\n",
    "            list[float]\n",
    "        \"\"\"\n",
    "        assert len(ground_truths) == len(predictions) == len(image_paths), \\\n",
    "            \"Input lists must have the same length.\"\n",
    "\n",
    "        gts_dict = {i: gt for i, gt in enumerate(ground_truths)}\n",
    "        preds_dict = {i: pred for i, pred in enumerate(predictions)}\n",
    "        paths_dict = {i: path for i, path in enumerate(image_paths)}\n",
    "\n",
    "        print(\"Calculating CIDEr scores...\")\n",
    "        cider_scores = self.calculate_cider_batch(gts_dict, preds_dict)\n",
    "\n",
    "        print(\"Calculating CLIP scores...\")\n",
    "        clip_scores = self.calculate_clip_batch(paths_dict, preds_dict)\n",
    "\n",
    "        print(\"Combining scores to generate final rewards...\")\n",
    "        final_rewards = []\n",
    "        for i in range(len(predictions)):\n",
    "            cider_score = cider_scores.get(i, 0.0)\n",
    "            \n",
    "            clip_score_raw = clip_scores.get(i, 0.0)\n",
    "            clip_score_normalized = max(0, (clip_score_raw - 15) / (35 - 15))\n",
    "\n",
    "            reward = self.alpha * cider_score + (1.0 - self.alpha) * clip_score_normalized\n",
    "            final_rewards.append(reward) \n",
    "            \n",
    "            print(f\"  Index {i}: CIDEr={cider_score:.2f}, CLIP={clip_score_raw:.2f} -> Reward={reward:.4f}\")\n",
    "\n",
    "        return final_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths_list = [\n",
    "    [\"một chiếc ô tô màu đỏ đậu trên đường phố\", \"siêu xe thể thao màu đỏ\"],\n",
    "    [\"chú chó đang chơi trên bãi cỏ xanh\", \"một chú chó lông vàng chạy trong công viên\"]\n",
    "]\n",
    "\n",
    "\n",
    "predictions_list = [\n",
    "    \"một chiếc xe hơi màu đỏ\",\n",
    "    \"con chó vui vẻ trên đồng cỏ\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    Image.new('RGB', (224, 224), color = 'red').save(\"car_101.jpg\")\n",
    "    Image.new('RGB', (224, 224), color = 'green').save(\"dog_102.jpg\")\n",
    "    image_paths_list = [\n",
    "        \"car_101.jpg\",\n",
    "        \"dog_102.jpg\"\n",
    "    ]\n",
    "\n",
    "    reward_scorer = ExplanationRewardScorer(alpha=0.5)\n",
    "\n",
    "    final_rewards = reward_scorer.explanation_rewards(\n",
    "        ground_truths=ground_truths_list,\n",
    "        predictions=predictions_list,\n",
    "        image_paths=image_paths_list\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Final Reward ---\")\n",
    "    print(final_rewards)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1: score = 3.000 | content = '<think>t</think>\\n<answer>a</answer>\\n<explain>e</explain>'\n",
      "Case 2: score = 2.000 | content = '<think>t</think>\\n<answer>a</answer>'\n",
      "Case 3: score = 2.000 | content = '<think>x</think><think>y</think>\\n<answer>a</answer>\\n<explain>e</explain>'\n",
      "Case 4: score = 2.500 | content = '<think>oops\\n<answer>a</answer>\\n<explain>e</explain>'\n",
      "Case 5: score = 2.500 | content = '<think>t</think>\\n</answer>\\n<explain>e</explain>'\n",
      "Case 6: score = 2.500 | content = '<think>t</think>\\n<answer>a</answer>\\n<explain>\\n<explain>\\n</explain>'\n",
      "Case 7: score = 0.000 | content = 'plain text only'\n",
      "Case 8: score = 2.000 | content = '<think> A <think> B </think> C </think>\\n<answer>a</answer>\\n<explain>e</explain>'\n",
      "Case 9: score = 2.500 | content = '<think> t </answer> u </think>\\n<answer>a</answer>\\n<explain>e</explain>'\n",
      "Case 10: score = 0.000 | content = '<THINK>t</THINK>\\n<ANSWER>a</ANSWER>\\n<EXPLAIN>e</EXPLAIN>'\n",
      "Case 11: score = 1.500 | content = '<think  >t</think>\\n<answer id=\\'1\\'>a</answer>\\n<explain class=\"x\">e</explain>'\n",
      "Case 12: score = 1.000 | content = 'prefix <think>t</think> mid <answer>a</answer> end </answer> tail <think> lone'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"Reward: +1 nếu có ít nhất 1 cặp hợp lệ cho mỗi tag think/answer/explain. \n",
    "    Lặp thêm không được cộng thêm điểm (chống spam). Tổng điểm 0..3.\"\"\"\n",
    "    # Tách nội dung completion (giữ nguyên cấu trúc đầu vào của bạn)\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    # Regex cho từng cặp thẻ\n",
    "    pat_think = re.compile(r\"<think>.*?</think>\", re.DOTALL)\n",
    "    pat_answer = re.compile(r\"<answer>.*?</answer>\", re.DOTALL)\n",
    "    pat_explain = re.compile(r\"<explain>.*?</explain>\", re.DOTALL)\n",
    "    \n",
    "    scores = []\n",
    "    for content in completion_contents:\n",
    "        n_pair_think = len(pat_think.findall(content))\n",
    "        n_pair_answer = len(pat_answer.findall(content))\n",
    "        n_pair_explain = len(pat_explain.findall(content))\n",
    "\n",
    "        n_think_open   = len(re.findall(r\"<think>\", content))\n",
    "        n_think_close  = len(re.findall(r\"</think>\", content))\n",
    "        n_answer_open  = len(re.findall(r\"<answer>\", content))\n",
    "        n_answer_close = len(re.findall(r\"</answer>\", content))\n",
    "        n_explain_open  = len(re.findall(r\"<explain>\", content))\n",
    "        n_explain_close = len(re.findall(r\"</explain>\", content))\n",
    "        # base score\n",
    "        b_think = 1.0 if n_pair_think >= 1 else (0.5 if n_think_open or n_think_close == 1 else 0.0)\n",
    "        b_answer = 1.0 if n_pair_answer >= 1 else (0.5 if n_answer_open or n_answer_close == 1 else 0.0)\n",
    "        b_explain = 1.0 if n_pair_explain >= 1 else (0.5 if n_explain_open or n_explain_close == 1 else 0.0)\n",
    "        b_total = b_think + b_answer + b_explain\n",
    "        \n",
    "        # penalty score\n",
    "        # Đếm số thẻ mở/đóng riêng lẻ\n",
    "        # Thẻ đơn dư = (mở + đóng) - 2 * số cặp  (không âm)\n",
    "        think_singles   = max(0, n_think_open   + n_think_close   - 2 )\n",
    "        answer_singles  = max(0, n_answer_open  + n_answer_close  - 2 )\n",
    "        explain_singles = max(0, n_explain_open + n_explain_close - 2 )\n",
    "\n",
    "        p_think = think_singles * 0.5\n",
    "        p_answer = answer_singles * 0.5\n",
    "        p_explain = explain_singles * 0.5\n",
    "        p_total = p_think + p_answer + p_explain\n",
    "\n",
    "        total = float(b_total - p_total)\n",
    "        scores.append(total)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Mock completions: mỗi phần tử là [ {\"content\": \"...\"} ]\n",
    "completions = [\n",
    "    # 1) Chuẩn 3 cặp, mỗi tag 1 lần → base tối đa (3); không dư cặp, không thẻ đơn\n",
    "    [ {\"content\": \"<think>t</think>\\n<answer>a</answer>\\n<explain>e</explain>\"} ],\n",
    "\n",
    "    # 2) Thiếu explain → base 2; không thẻ đơn\n",
    "    [ {\"content\": \"<think>t</think>\\n<answer>a</answer>\"} ],\n",
    "\n",
    "    # 3) think có 2 cặp (dư cặp) nhưng không có thẻ đơn; answer/explain 1 cặp\n",
    "    [ {\"content\": \"<think>x</think><think>y</think>\\n<answer>a</answer>\\n<explain>e</explain>\"} ],\n",
    "\n",
    "    # 4) Dư thẻ đơn: thiếu </think>\n",
    "    [ {\"content\": \"<think>oops\\n<answer>a</answer>\\n<explain>e</explain>\"} ],\n",
    "\n",
    "    # 5) Dư thẻ đơn: thiếu <answer> (chỉ có </answer>)\n",
    "    [ {\"content\": \"<think>t</think>\\n</answer>\\n<explain>e</explain>\"} ],\n",
    "\n",
    "    # 6) Nhiều thẻ đơn dư: thừa 2 mở explain, 1 đóng explain, không có cặp explain\n",
    "    [ {\"content\": \"<think>t</think>\\n<answer>a</answer>\\n<explain>\\n<explain>\\n</explain>\"} ],\n",
    "\n",
    "    # 7) Không có thẻ nào → base 0; không thẻ đơn\n",
    "    [ {\"content\": \"plain text only\"} ],\n",
    "\n",
    "    # 8) Lồng ghép think: non-greedy sẽ khớp 2 cặp (tùy nội dung)\n",
    "    [ {\"content\": \"<think> A <think> B </think> C </think>\\n<answer>a</answer>\\n<explain>e</explain>\"} ],\n",
    "\n",
    "    # 9) Sai thứ tự/chéo tag: có thể khiến findall tìm 0 cặp\n",
    "    [ {\"content\": \"<think> t </answer> u </think>\\n<answer>a</answer>\\n<explain>e</explain>\"} ],\n",
    "\n",
    "    # 10) Tag chữ hoa (không match vì regex phân biệt hoa/thường)\n",
    "    [ {\"content\": \"<THINK>t</THINK>\\n<ANSWER>a</ANSWER>\\n<EXPLAIN>e</EXPLAIN>\"} ],\n",
    "\n",
    "    # 11) Tag có khoảng trắng/thuộc tính (không match với pattern hiện tại)\n",
    "    [ {\"content\": \"<think  >t</think>\\n<answer id='1'>a</answer>\\n<explain class=\\\"x\\\">e</explain>\"} ],\n",
    "\n",
    "    # 12) Nhiều noise và dư đóng answer, dư mở think\n",
    "    [ {\"content\": \"prefix <think>t</think> mid <answer>a</answer> end </answer> tail <think> lone\"} ],\n",
    "]\n",
    "\n",
    "scores = format_reward(completions)\n",
    "for i, (item, s) in enumerate(zip(completions, scores), 1):\n",
    "    print(f\"Case {i}: score = {s:.3f} | content = {item[0]['content']!r}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqanle-grpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
