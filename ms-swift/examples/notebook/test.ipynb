{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "484b71ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VnCoreNLP RDRSegmenter...\n",
      "2025-11-08 23:19:14 INFO  WordSegmenter:24 - Loading Word Segmentation model\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import sys\n",
    "plugin_path = '/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin'\n",
    "\n",
    "sys.path.insert(0, plugin_path)\n",
    "from plugin import CustomExplainationReward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12cabd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacustom = []\n",
    "with open('/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/data_custom/ViVQA-X_val_grpo.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        datacustom.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0601e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "completions_data = []\n",
    "with open('/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/output/dat-vinternvl3B/v0-20251108-190728/completions.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        completions_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f62ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_calculator = CustomExplainationReward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1219084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97fcc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading CLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: text_model.embeddings.token_embedding.weight, vision_model.post_layernorm.weight, text_model.encoder.layers.*.layer_norm*.bias, vision_model.embeddings.position_embedding.weight, vision_model.encoder.layers.*.self_attn.k_proj.weight, vision_model.pre_layrnorm.bias, vision_model.encoder.layers.*.self_attn.q_proj.weight, text_model.encoder.layers.*.self_attn.k_proj.bias, vision_model.encoder.layers.*.mlp.fc*.bias, vision_model.encoder.layers.*.mlp.fc*.weight, text_model.encoder.layers.*.self_attn.v_proj.weight, vision_model.encoder.layers.*.self_attn.out_proj.bias, text_model.encoder.layers.*.mlp.fc*.weight, text_model.embeddings.position_embedding.weight, text_model.encoder.layers.*.self_attn.k_proj.weight, vision_model.encoder.layers.*.layer_norm*.weight, vision_model.encoder.layers.*.self_attn.k_proj.bias, vision_model.encoder.layers.*.self_attn.v_proj.weight, vision_model.encoder.layers.*.self_attn.v_proj.bias, vision_model.encoder.layers.*.self_attn.out_proj.weight, text_projection.weight, text_model.final_layer_norm.bias, logit_scale, vision_model.pre_layrnorm.weight, text_model.encoder.layers.*.self_attn.q_proj.weight, text_model.encoder.layers.*.mlp.fc*.bias, text_model.encoder.layers.*.self_attn.out_proj.bias, vision_model.embeddings.class_embedding, text_model.encoder.layers.*.self_attn.q_proj.bias, text_model.encoder.layers.*.self_attn.v_proj.bias, vision_model.embeddings.patch_embedding.weight, vision_model.encoder.layers.*.self_attn.q_proj.bias, vision_model.post_layernorm.bias, vision_model.encoder.layers.*.layer_norm*.bias, text_model.final_layer_norm.weight, visual_projection.weight, text_model.encoder.layers.*.layer_norm*.weight, text_model.encoder.layers.*.self_attn.out_proj.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CLIP loaded from cache\n",
      "\n",
      "Loading data...\n",
      "✓ Loaded 1459 datacustom\n",
      "✓ Loaded 243 completions\n",
      "\n",
      "Initializing reward calculator...\n",
      "\n",
      "✓ Prepared 15 valid samples\n",
      "\n",
      "============================================================\n",
      "Processing 15 samples ONE BY ONE\n",
      "(To avoid GPU OOM)\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "SAMPLE 1/15\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000393271.jpg\n",
      "Completion: <REASONING>Câu hỏi này yêu cầu xác định nguồn năng lượng được sử dụng bởi chiếc bếp.  Quan sát hình ...\n",
      "Label: <answer>gas</answer><explain>đầu đốt gas đã được lắp đặt</explain>\n",
      "✓ CDLP Score: 0.0000\n",
      "\n",
      "==================================================\n",
      "SAMPLE 2/15\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000393284.jpg\n",
      "Completion: <REASONING>Sự xuất hiện của những chiếc dùi cui trong hình ảnh, cùng với trang phục mùa đông và phon...\n",
      "Label: <answer>trượt tuyết</answer><explain>mỗi người đều có những tấm ván mỏng dài ở mỗi chân cùng với đôi...\n",
      "✓ CDLP Score: 0.0000\n",
      "\n",
      "==================================================\n",
      "SAMPLE 3/15\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000393338.jpg\n",
      "Completion: <REASONING>Hình ảnh cho thấy một người đang thực hiện một cú nhảy lớn trên tuyết. Họ đang nhảy lên c...\n",
      "Label: <answer>đúng</answer><explain>anh ấy đang ở rất cao</explain>\n",
      "✓ CDLP Score: 0.0000\n",
      "\n",
      "==================================================\n",
      "SAMPLE 4/15\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000524436.jpg\n",
      "Completion: <REASONING>Hình ảnh hiển thị một người phụ nữ đang cầm một máy ảnh kỹ thuật số chuyên dụng.  Máy ảnh...\n",
      "Label: <answer>có</answer><explain>cô ấy đang cầm một chiếc máy ảnh trong tay</explain>\n",
      "✓ CDLP Score: 0.0000\n",
      "\n",
      "==================================================\n",
      "SAMPLE 5/15\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000262531.jpg\n",
      "Completion: <REASONING>Nhìn vào hình ảnh, có một tấm thảm có in logo của một chương trình giải thưởng.  Logo này...\n",
      "Label: <answer>đúng</answer><explain>biển báo nói như vậy</explain>\n",
      "✓ CDLP Score: 0.0000\n",
      "\n",
      "==================================================\n",
      "SAMPLE 6/15\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000262608.jpg\n",
      "Completion: <REASONING>Để xác định chất liệu của sàn nhà, ta cần phân tích hình ảnh một cách cẩn thận.  Sàn nhà ...\n",
      "Label: <answer>gạch</answer><explain>gạch riêng lẻ và mạch gạch ở giữa có thể thấy rõ</explain>\n",
      "✓ CDLP Score: 0.0000\n",
      "\n",
      "==================================================\n",
      "SAMPLE 7/15\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000262651.jpg\n",
      "Completion: <REASONING>Quá trình suy luận chi tiết dẫn đến câu trả lời cuối cùng:\n",
      "\n",
      "1. **Quan sát hình ảnh:** Hìn...\n",
      "Label: <answer>trượt ván</answer><explain>cậu bé đang trượt xuống trên ván của mình</explain>\n",
      "✓ CDLP Score: 0.0000\n",
      "\n",
      "==================================================\n",
      "SAMPLE 8/15\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000524822.jpg\n",
      "Completion: <REASONING>Nước trong ảnh có màu xanh đậm và có những gợn sóng nhỏ. Những gợn sóng này cho thấy rằng...\n",
      "Label: <answer>đúng</answer><explain>nó đang chảy chậm xuống dòng</explain>\n",
      "✓ CDLP Score: 0.0000\n",
      "\n",
      "==================================================\n",
      "SAMPLE 9/15\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000000544.jpg\n",
      "Completion: <REASONING>Hình ảnh cho thấy một trận đấu bóng chày.  Một người đàn ông đang đứng trên sân khấu, cầm...\n",
      "Label: <answer>bóng chày</answer><explain>các cầu thủ đang theo dõi trận đấu từ hầm.</explain>\n",
      "✓ CDLP Score: 0.0000\n",
      "\n",
      "==================================================\n",
      "SAMPLE 10/15\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000000544.jpg\n",
      "Completion: <REASONING>Hình ảnh cho thấy trời có tuyết rơi, và mặt đường bên cạnh xe đạp cũng có tuyết. Tuy nhiê...\n",
      "Label: <answer>không</answer><explain>nó trông rất lạnh và nhiều mây</explain>\n",
      "✓ CDLP Score: 0.0000\n",
      "\n",
      "==================================================\n",
      "SAMPLE 11/15\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000187695.jpg\n",
      "Completion: <REASONING>Cả hai con ngựa vằn trong ảnh đều đang nhìn về phía trước.  Hai con ngựa vằn đứng cạnh nh...\n",
      "Label: <answer>không</answer><explain>một con hướng về bên trái, con kia hướng về bên phải</explain>\n",
      "✓ CDLP Score: 0.0000\n",
      "\n",
      "==================================================\n",
      "SAMPLE 12/15\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000131869.jpg\n",
      "Completion: <REASONING>Nhìn chung, hình ảnh cho thấy một con ngựa vằn ở phía trước đang ăn lá, với phần đầu hướn...\n",
      "Label: <answer>không</answer><explain>nó hướng xuống mặt đất</explain>\n",
      "✓ CDLP Score: 0.0000\n",
      "\n",
      "==================================================\n",
      "SAMPLE 13/15\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000525354.jpg\n",
      "Completion: <REASONING>Nhìn chung, hình ảnh cho thấy một khu vực nuôi thú có rào chắn, đàn ngựa và các con bò kh...\n",
      "Label: <answer>đúng</answer><explain>các con vật đang ở trong chuồng để mọi người ngắm nhìn</explain>\n",
      "✓ CDLP Score: 0.0000\n",
      "\n",
      "==================================================\n",
      "SAMPLE 14/15\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000263258.jpg\n",
      "Completion: <REASONING>Phía sau các xe máy là một tòa nhà lớn, kiểu kiến trúc kiểu Gothic. Điều này có thể được ...\n",
      "Label: <answer>nhà thờ</answer><explain>nó có một tháp chuông</explain>\n",
      "✓ CDLP Score: 0.0000\n",
      "\n",
      "==================================================\n",
      "SAMPLE 15/15\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000525420.jpg\n",
      "Completion: <REASONING>Hình ảnh cho thấy một chiếc ghế đá nằm trên một khu vực có nhiều cây xanh. Chiếc ghế có v...\n",
      "Label: <answer>ngồi</answer><explain>nó là một băng ghế công viên</explain>\n",
      "✓ CDLP Score: 0.0000\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Total samples: 15\n",
      "Average score: 0.0000\n",
      "Max score: 0.0000\n",
      "Min score: 0.0000\n",
      "Non-zero scores: 0/15\n",
      "============================================================\n",
      "\n",
      "Detailed scores:\n",
      "  Sample  0: 0.0000\n",
      "  Sample  1: 0.0000\n",
      "  Sample  2: 0.0000\n",
      "  Sample  3: 0.0000\n",
      "  Sample  4: 0.0000\n",
      "  Sample  5: 0.0000\n",
      "  Sample  6: 0.0000\n",
      "  Sample  7: 0.0000\n",
      "  Sample  8: 0.0000\n",
      "  Sample  9: 0.0000\n",
      "  Sample 10: 0.0000\n",
      "  Sample 11: 0.0000\n",
      "  Sample 12: 0.0000\n",
      "  Sample 13: 0.0000\n",
      "  Sample 14: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# os.environ['HF_TOKEN'] = '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'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "print(\"Pre-loading CLIP model...\")\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "try:\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\", local_files_only=True)\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\", local_files_only=True)\n",
    "    print(\"✓ CLIP loaded from cache\")\n",
    "except:\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\", token=os.getenv('HF_TOKEN'))\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\", token=os.getenv('HF_TOKEN'))\n",
    "    print(\"✓ CLIP downloaded\")\n",
    "\n",
    "del model, processor\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "ms_swift_path = '/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift'\n",
    "sys.path.insert(0, ms_swift_path)\n",
    "sys.path.insert(0, plugin_path)\n",
    "\n",
    "print(f\"✓ Loaded {len(datacustom)} datacustom\")\n",
    "print(f\"✓ Loaded {len(completions_data)} completions\")\n",
    "\n",
    "# Initialize\n",
    "print(\"\\nInitializing reward calculator...\")\n",
    "reward_calculator = CustomExplainationReward()\n",
    "\n",
    "num_samples = 15\n",
    "\n",
    "all_completions = []\n",
    "all_labels = []\n",
    "all_image_paths = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    image_data = datacustom[i].get('image') or datacustom[i].get('images')\n",
    "    if isinstance(image_data, list):\n",
    "        image_path = image_data[0]\n",
    "    else:\n",
    "        image_path = image_data\n",
    "    \n",
    "    completion = (completions_data[i].get('completions') or \n",
    "                  completions_data[i].get('completion') or\n",
    "                  completions_data[i].get('response') or\n",
    "                  completions_data[i].get('text'))\n",
    "    \n",
    "    label = (completions_data[i].get('exl_rewardsd') or \n",
    "             completions_data[i].get('label') or\n",
    "             completions_data[i].get('solution') or\n",
    "             completions_data[i].get('ground_truth') or\n",
    "             completions_data[i].get('answer'))\n",
    "    \n",
    "    if isinstance(completion, list):\n",
    "        completion = completion[0] if len(completion) > 0 else \"\"\n",
    "    \n",
    "    if isinstance(label, list):\n",
    "        label = label[0] if len(label) > 0 else \"\"\n",
    "    \n",
    "    if not completion or not label:\n",
    "        print(f\"⚠️ Skipping sample {i}: empty data\")\n",
    "        continue\n",
    "    \n",
    "    all_completions.append(completion)\n",
    "    all_labels.append(label)\n",
    "    all_image_paths.append(image_path)\n",
    "\n",
    "print(f\"\\n✓ Prepared {len(all_completions)} valid samples\")\n",
    "\n",
    "if len(all_completions) == 0:\n",
    "    print(\"\\n⚠️ ERROR: No valid samples!\")\n",
    "else:\n",
    "    # === CHẠY TỪNG SAMPLE ===\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {len(all_completions)} samples ONE BY ONE\")\n",
    "    print(f\"(To avoid GPU OOM)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    all_scores = []\n",
    "    \n",
    "    for i in range(len(all_completions)):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"SAMPLE {i+1}/{len(all_completions)}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            # Clear cache TRƯỚC\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Chạy TỪNG sample\n",
    "            scores = reward_calculator(\n",
    "                completions=[all_completions[i]],\n",
    "                solution=[all_labels[i]],\n",
    "                images=[[{'bytes': None, 'path': all_image_paths[i]}]]\n",
    "            )\n",
    "            \n",
    "            score_value = scores[0] if isinstance(scores, list) and len(scores) > 0 else 0.0\n",
    "            all_scores.append(score_value)\n",
    "            \n",
    "            # In thông tin\n",
    "            print(f\"Image: {all_image_paths[i]}\")\n",
    "            \n",
    "            comp_display = all_completions[i][:100] + \"...\" if len(all_completions[i]) > 100 else all_completions[i]\n",
    "            label_display = all_labels[i][:100] + \"...\" if len(all_labels[i]) > 100 else all_labels[i]\n",
    "            \n",
    "            print(f\"Completion: {comp_display}\")\n",
    "            print(f\"Label: {label_display}\")\n",
    "            print(f\"✓ CDLP Score: {score_value:.4f}\")\n",
    "            \n",
    "            # Clear cache SAU\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error: {str(e)[:150]}\")\n",
    "            all_scores.append(0.0)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Tổng kết\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total samples: {len(all_scores)}\")\n",
    "    print(f\"Average score: {sum(all_scores)/len(all_scores):.4f}\")\n",
    "    print(f\"Max score: {max(all_scores):.4f}\")\n",
    "    print(f\"Min score: {min(all_scores):.4f}\")\n",
    "    print(f\"Non-zero scores: {sum(1 for s in all_scores if s > 0)}/{len(all_scores)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Chi tiết scores\n",
    "    print(f\"\\nDetailed scores:\")\n",
    "    for i, score in enumerate(all_scores):\n",
    "        print(f\"  Sample {i:2d}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a0bc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ FORCED CPU MODE - All models will run on CPU\n",
      "   This is SLOW but will not crash with OOM\n",
      "\n",
      "Pre-loading CLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: text_model.embeddings.token_embedding.weight, vision_model.post_layernorm.weight, text_model.encoder.layers.*.layer_norm*.bias, vision_model.embeddings.position_embedding.weight, vision_model.encoder.layers.*.self_attn.k_proj.weight, vision_model.pre_layrnorm.bias, vision_model.encoder.layers.*.self_attn.q_proj.weight, text_model.encoder.layers.*.self_attn.k_proj.bias, vision_model.encoder.layers.*.mlp.fc*.bias, vision_model.encoder.layers.*.mlp.fc*.weight, text_model.encoder.layers.*.self_attn.v_proj.weight, vision_model.encoder.layers.*.self_attn.out_proj.bias, text_model.encoder.layers.*.mlp.fc*.weight, text_model.embeddings.position_embedding.weight, text_model.encoder.layers.*.self_attn.k_proj.weight, vision_model.encoder.layers.*.layer_norm*.weight, vision_model.encoder.layers.*.self_attn.k_proj.bias, vision_model.encoder.layers.*.self_attn.v_proj.weight, vision_model.encoder.layers.*.self_attn.v_proj.bias, vision_model.encoder.layers.*.self_attn.out_proj.weight, text_projection.weight, text_model.final_layer_norm.bias, logit_scale, vision_model.pre_layrnorm.weight, text_model.encoder.layers.*.self_attn.q_proj.weight, text_model.encoder.layers.*.mlp.fc*.bias, text_model.encoder.layers.*.self_attn.out_proj.bias, vision_model.embeddings.class_embedding, text_model.encoder.layers.*.self_attn.q_proj.bias, text_model.encoder.layers.*.self_attn.v_proj.bias, vision_model.embeddings.patch_embedding.weight, vision_model.encoder.layers.*.self_attn.q_proj.bias, vision_model.post_layernorm.bias, vision_model.encoder.layers.*.layer_norm*.bias, text_model.final_layer_norm.weight, visual_projection.weight, text_model.encoder.layers.*.layer_norm*.weight, text_model.encoder.layers.*.self_attn.out_proj.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CLIP loaded from cache\n",
      "\n",
      "Loading data...\n",
      "✓ Loaded 1459 datacustom\n",
      "✓ Loaded 246 completions\n",
      "\n",
      "Initializing reward calculator (CPU MODE)...\n",
      "\n",
      "✓ Prepared 3 valid samples\n",
      "\n",
      "============================================================\n",
      "Processing 3 samples (CPU MODE - will be slow)\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "SAMPLE 1/3\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000393271.jpg\n",
      "Completion: <REASONING>Câu hỏi này yêu cầu xác định nguồn năng lượng được sử dụng bởi chiếc ...\n",
      "Label: <answer>gas</answer><explain>đầu đốt gas đã được lắp đặt</explain>...\n",
      "✓ CDLP Score: 0.0000\n",
      "  (took 1.1s)\n",
      "\n",
      "==================================================\n",
      "SAMPLE 2/3\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000393284.jpg\n",
      "Completion: <REASONING>Sự xuất hiện của những chiếc dùi cui trong hình ảnh, cùng với trang p...\n",
      "Label: <answer>trượt tuyết</answer><explain>mỗi người đều có những tấm ván mỏng dài ở m...\n",
      "✓ CDLP Score: 0.0000\n",
      "  (took 0.9s)\n",
      "\n",
      "==================================================\n",
      "SAMPLE 3/3\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, encoder.layer.*.attention.self.key.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during BERTScore batch computation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error during CLIP batch computation: GET was unable to find an engine to execute this computation\n",
      "Error during scorer.explanation_rewards calculation: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Image: /mnt/VLAI_data/COCO_Images/val2014/COCO_val2014_000000393338.jpg\n",
      "Completion: <REASONING>Hình ảnh cho thấy một người đang thực hiện một cú nhảy lớn trên tuyết...\n",
      "Label: <answer>đúng</answer><explain>anh ấy đang ở rất cao</explain>...\n",
      "✓ CDLP Score: 0.0000\n",
      "  (took 0.9s)\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Total samples: 3\n",
      "Average score: 0.0000\n",
      "Max score: 0.0000\n",
      "Min score: 0.0000\n",
      "Non-zero scores: 0/3\n",
      "============================================================\n",
      "  Sample 0: 0.0000\n",
      "  Sample 1: 0.0000\n",
      "  Sample 2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/base_rewards.py\", line 88, in calculate_bertscore_batch\n",
      "    score_dict = bertscore_metric.compute()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/metric.py\", line 699, in wrapped_func\n",
      "    value = _squeeze_if_scalar(compute(*args, **kwargs))\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/text/bert.py\", line 284, in compute\n",
      "    output_dict = bert_score(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 556, in bert_score\n",
      "    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torchmetrics/functional/text/bert.py\", line 117, in _get_embeddings_and_idf_scale\n",
      "    out = model(batch[\"input_ids\"], batch[\"attention_mask\"], output_hidden_states=True)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 798, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 87, in forward\n",
      "    position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1548, in create_position_ids_from_input_ids\n",
      "    mask = input_ids.ne(padding_idx).int()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.AcceleratorError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# === FORCE CPU MODE ===\n",
    "# Monkey patch torch.cuda.is_available() to return False\n",
    "original_is_available = torch.cuda.is_available\n",
    "torch.cuda.is_available = lambda: False\n",
    "\n",
    "print(\"⚠️ FORCED CPU MODE - All models will run on CPU\")\n",
    "print(\"   This is SLOW but will not crash with OOM\")\n",
    "\n",
    "# Set token\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "\n",
    "print(\"\\nPre-loading CLIP model...\")\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "try:\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\", local_files_only=True)\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\", local_files_only=True)\n",
    "    print(\"✓ CLIP loaded from cache\")\n",
    "except:\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\", token=os.getenv('HF_TOKEN'))\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\", token=os.getenv('HF_TOKEN'))\n",
    "    print(\"✓ CLIP downloaded\")\n",
    "\n",
    "del model, processor\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Add paths\n",
    "ms_swift_path = '/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift'\n",
    "plugin_path = '/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin'\n",
    "sys.path.insert(0, ms_swift_path)\n",
    "sys.path.insert(0, plugin_path)\n",
    "\n",
    "from plugin import CustomExplainationReward\n",
    "\n",
    "# Load data\n",
    "print(\"\\nLoading data...\")\n",
    "datacustom = []\n",
    "with open('/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/data_custom/ViVQA-X_val_grpo.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        datacustom.append(json.loads(line))\n",
    "\n",
    "completions_data = []\n",
    "with open('/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/output/dat-vinternvl3B/v0-20251108-190728/completions.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        completions_data.append(json.loads(line))\n",
    "\n",
    "print(f\"✓ Loaded {len(datacustom)} datacustom\")\n",
    "print(f\"✓ Loaded {len(completions_data)} completions\")\n",
    "\n",
    "# Initialize\n",
    "print(\"\\nInitializing reward calculator (CPU MODE)...\")\n",
    "reward_calculator = CustomExplainationReward()\n",
    "\n",
    "# Prepare data - START WITH 3 SAMPLES for testing\n",
    "num_samples = min(3, len(datacustom), len(completions_data))\n",
    "\n",
    "all_completions = []\n",
    "all_labels = []\n",
    "all_image_paths = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    image_data = datacustom[i].get('image') or datacustom[i].get('images')\n",
    "    if isinstance(image_data, list):\n",
    "        image_path = image_data[0]\n",
    "    else:\n",
    "        image_path = image_data\n",
    "    \n",
    "    completion = (completions_data[i].get('completions') or \n",
    "                  completions_data[i].get('completion') or\n",
    "                  completions_data[i].get('response'))\n",
    "    \n",
    "    label = (completions_data[i].get('exl_rewardsd') or \n",
    "             completions_data[i].get('label') or\n",
    "             completions_data[i].get('solution'))\n",
    "    \n",
    "    if isinstance(completion, list):\n",
    "        completion = completion[0] if len(completion) > 0 else \"\"\n",
    "    \n",
    "    if isinstance(label, list):\n",
    "        label = label[0] if len(label) > 0 else \"\"\n",
    "    \n",
    "    if not completion or not label:\n",
    "        continue\n",
    "    \n",
    "    all_completions.append(completion)\n",
    "    all_labels.append(label)\n",
    "    all_image_paths.append(image_path)\n",
    "\n",
    "print(f\"\\n✓ Prepared {len(all_completions)} valid samples\")\n",
    "\n",
    "# === RUN ===\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Processing {len(all_completions)} samples (CPU MODE - will be slow)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "for i in range(len(all_completions)):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"SAMPLE {i+1}/{len(all_completions)}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scores = reward_calculator(\n",
    "            completions=[all_completions[i]],\n",
    "            solution=[all_labels[i]],\n",
    "            images=[[{'bytes': None, 'path': all_image_paths[i]}]]\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        score_value = scores[0] if isinstance(scores, list) and len(scores) > 0 else 0.0\n",
    "        all_scores.append(score_value)\n",
    "        \n",
    "        print(f\"Image: {all_image_paths[i]}\")\n",
    "        print(f\"Completion: {all_completions[i][:80]}...\")\n",
    "        print(f\"Label: {all_labels[i][:80]}...\")\n",
    "        print(f\"✓ CDLP Score: {score_value:.4f}\")\n",
    "        print(f\"  (took {elapsed:.1f}s)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error: {str(e)[:150]}\")\n",
    "        all_scores.append(0.0)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total samples: {len(all_scores)}\")\n",
    "print(f\"Average score: {sum(all_scores)/len(all_scores):.4f}\")\n",
    "print(f\"Max score: {max(all_scores):.4f}\")\n",
    "print(f\"Min score: {min(all_scores):.4f}\")\n",
    "print(f\"Non-zero scores: {sum(1 for s in all_scores if s > 0)}/{len(all_scores)}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for i, score in enumerate(all_scores):\n",
    "    print(f\"  Sample {i}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502e2e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_vivqanle_grpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
