run sh: `/opt/miniconda3/envs/project_vivqanle_grpo/bin/python3.12 /opt/miniconda3/envs/project_vivqanle_grpo/lib/python3.12/site-packages/swift/cli/rlhf.py --use_hf true --rlhf_type grpo --model_type internvl3 --model 5CD-AI/Vintern-3B-R-beta --dataset /home/vlai-vqa-nle/minhtq/vqa-nle/data/processed/ms-swift/stage2/ViVQA-X_train_msswift.jsonl --external_plugins /home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/plugin/plugin.py --reward_funcs custom_format_reward_ViVQA_X custom_accuracy_reward custom_explaination_reward --reward_weights 0.5 1.25 1.25 --train_type lora --lora_rank 64 --lora_alpha 128 --target_modules all-linear --freeze_vit True --output_dir /home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/output/dat-vinternvl3B --per_device_eval_batch_size 4 --max_completion_length 1024 --num_train_epochs 1 --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 1e-5 --save_steps 50 --max_steps 200 --logging_steps 1 --eval_strategy steps --eval_steps 1 --num_generations 4 --temperature 0.9 --top_p 0.9 --beta 0.04 --log_completions true --torch_dtype bfloat16 --save_only_model false --save_total_limit 2 --warmup_ratio 0.05 --dataloader_num_workers 4 --dataset_num_proc 1 --report_to wandb --quant_method bnb --quant_bits 8 --gradient_checkpointing true`
2025-11-10 23:56:18 INFO  WordSegmenter:24 - Loading Word Segmentation model
Loading VnCoreNLP RDRSegmenter...
FlashAttention2 is not installed.
INFO 11-10 23:56:40 [__init__.py:216] Automatically detected platform cuda.
[2025-11-10 23:56:40,949] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Initializing AccuracyRewardScorer (BERTScore)...
Initializing shared BERTScore from: /mnt/dataset1/pretrained_fm/vinai/phobert-base
Device: cuda
Shared BERTScore initialized successfully.
AccuracyRewardScorer initialized successfully!
  [Sample 0] Raw GT: 'gas', Raw Pred: 'Điện'
  [Sample 0] Cleaned GT: 'gas', Cleaned Pred: 'điện'
  [Sample 1] Raw GT: 'gas', Raw Pred: 'Gas'
  [Sample 1] Cleaned GT: 'gas', Cleaned Pred: 'gas'
  [Sample 2] Raw GT: 'gas', Raw Pred: 'Gas'
  [Sample 2] Cleaned GT: 'gas', Cleaned Pred: 'gas'
  [Sample 3] Raw GT: 'gas', Raw Pred: 'Gas'
  [Sample 3] Cleaned GT: 'gas', Cleaned Pred: 'gas'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.5942 -> Reward=0.2971
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
Initializing ExplanationRewardScorer (CLIP + BERTScore)...
CIDEr scorer initialized (not used).
Initializing BERTScore (PhoBERT) on device: cuda
Initializing shared BERTScore from: /mnt/dataset1/pretrained_fm/vinai/phobert-base
Device: cuda
Shared BERTScore initialized successfully.
BERTScore initialized.
Initializing CLIPScore on device: cuda
CLIPScore model loaded and ready.
ExplanationRewardScorer initialized successfully!
   [Batch Stats] CLIP Min: 17.1822, CLIP Max: 18.3197
   [Sample 0] BERTScore=0.7362, CLIPRaw=18.18 (CLIPNorm=0.8810) -> Reward=0.8086
   [Sample 1] BERTScore=0.7217, CLIPRaw=17.18 (CLIPNorm=0.0000) -> Reward=0.3608
   [Sample 2] BERTScore=0.6927, CLIPRaw=18.32 (CLIPNorm=1.0000) -> Reward=0.8464
   [Sample 3] BERTScore=0.7347, CLIPRaw=18.18 (CLIPNorm=0.8733) -> Reward=0.8040
{'loss': -1.2e-07, 'grad_norm': 1.45423877, 'learning_rate': 1e-06, 'reward': 2.41156006, 'reward_std': 0.44708839, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.82428002, 'rewards/CustomAccuracyReward/std': 0.35143992, 'rewards/CustomExplainationReward/mean': 0.70496804, 'rewards/CustomExplainationReward/std': 0.23019436, 'completions/mean_length': 138.5, 'completions/min_length': 107.0, 'completions/max_length': 183.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '1/200', 'percentage': '0.50%', 'elapsed_time': '1m 18s', 'remaining_time': '4h 19m 2s', 'memory(GiB)': 13.56, 'train_speed(iter/s)': 0.012804}
  [Sample 0] Raw GT: 'trượt tuyết', Raw Pred: 'Đùi cui'
  [Sample 0] Cleaned GT: 'trượt tuyết', Cleaned Pred: 'đùi cui'
  [Sample 1] Raw GT: 'trượt tuyết', Raw Pred: 'Lướt sóng'
  [Sample 1] Cleaned GT: 'trượt tuyết', Cleaned Pred: 'lướt sóng'
  [Sample 2] Raw GT: 'trượt tuyết', Raw Pred: 'Trượt tuyết'
  [Sample 2] Cleaned GT: 'trượt tuyết', Cleaned Pred: 'trượt tuyết'
  [Sample 3] Raw GT: 'trượt tuyết', Raw Pred: 'Trượt tuyết'
  [Sample 3] Cleaned GT: 'trượt tuyết', Cleaned Pred: 'trượt tuyết'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2735 -> Reward=0.1367
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.6403 -> Reward=0.3202
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 17.6740, CLIP Max: 18.3589
   [Sample 0] BERTScore=0.8092, CLIPRaw=18.27 (CLIPNorm=0.8745) -> Reward=0.8418
   [Sample 1] BERTScore=0.8061, CLIPRaw=18.36 (CLIPNorm=1.0000) -> Reward=0.9031
   [Sample 2] BERTScore=0.8254, CLIPRaw=17.67 (CLIPNorm=0.0000) -> Reward=0.4127
   [Sample 3] BERTScore=0.8257, CLIPRaw=17.91 (CLIPNorm=0.3456) -> Reward=0.5857
{'loss': 1.5e-07, 'grad_norm': 1.08551741, 'learning_rate': 2e-06, 'reward': 2.12505054, 'reward_std': 0.32559273, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.61422336, 'rewards/CustomAccuracyReward/std': 0.45170709, 'rewards/CustomExplainationReward/mean': 0.68581706, 'rewards/CustomExplainationReward/std': 0.22814073, 'completions/mean_length': 202.0, 'completions/min_length': 159.0, 'completions/max_length': 259.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '2/200', 'percentage': '1.00%', 'elapsed_time': '2m 47s', 'remaining_time': '4h 37m 11s', 'memory(GiB)': 15.65, 'train_speed(iter/s)': 0.011905}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 0.0000, CLIP Max: 17.9142
   [Sample 0] BERTScore=0.7847, CLIPRaw=17.37 (CLIPNorm=0.9695) -> Reward=0.8771
   [Sample 1] BERTScore=0.7544, CLIPRaw=17.91 (CLIPNorm=1.0000) -> Reward=0.8772
   [Sample 2] BERTScore=0.0000, CLIPRaw=0.00 (CLIPNorm=0.0000) -> Reward=0.0000
   [Sample 3] BERTScore=0.7591, CLIPRaw=16.91 (CLIPNorm=0.9438) -> Reward=0.8514
{'loss': 0.00026002, 'grad_norm': 0.8399722, 'learning_rate': 3e-06, 'reward': 2.53927946, 'reward_std': 0.59304678, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 0.94999999, 'rewards/CustomFormatReward_ViVQA_X/std': 0.1, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.65142345, 'rewards/CustomExplainationReward/std': 0.4344517, 'completions/mean_length': 233.5, 'completions/min_length': 198.0, 'completions/max_length': 262.0, 'completions/clipped_ratio': 0.0, 'kl': 0.00649635, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '3/200', 'percentage': '1.50%', 'elapsed_time': '4m 18s', 'remaining_time': '4h 42m 47s', 'memory(GiB)': 15.67, 'train_speed(iter/s)': 0.01161}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.8349, CLIP Max: 21.1490
   [Sample 0] BERTScore=0.8026, CLIPRaw=20.86 (CLIPNorm=0.0846) -> Reward=0.4436
   [Sample 1] BERTScore=0.8391, CLIPRaw=20.83 (CLIPNorm=0.0000) -> Reward=0.4196
   [Sample 2] BERTScore=0.7783, CLIPRaw=21.15 (CLIPNorm=1.0000) -> Reward=0.8891
   [Sample 3] BERTScore=0.8308, CLIPRaw=20.94 (CLIPNorm=0.3409) -> Reward=0.5858
{'loss': 0.00031182, 'grad_norm': 0.89882106, 'learning_rate': 4e-06, 'reward': 2.48067689, 'reward_std': 0.26989058, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.58454168, 'rewards/CustomExplainationReward/std': 0.21591246, 'completions/mean_length': 164.25, 'completions/min_length': 139.0, 'completions/max_length': 198.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0078065, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '4/200', 'percentage': '2.00%', 'elapsed_time': '5m 18s', 'remaining_time': '4h 19m 58s', 'memory(GiB)': 15.67, 'train_speed(iter/s)': 0.012565}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 16.1060, CLIP Max: 17.7201
   [Sample 0] BERTScore=0.7301, CLIPRaw=16.83 (CLIPNorm=0.4473) -> Reward=0.5887
   [Sample 1] BERTScore=0.7154, CLIPRaw=17.72 (CLIPNorm=1.0000) -> Reward=0.8577
   [Sample 2] BERTScore=0.7234, CLIPRaw=16.43 (CLIPNorm=0.1978) -> Reward=0.4606
   [Sample 3] BERTScore=0.7033, CLIPRaw=16.11 (CLIPNorm=0.0000) -> Reward=0.3516
{'loss': 0.00028929, 'grad_norm': 1.12676191, 'learning_rate': 5e-06, 'reward': 2.4558177, 'reward_std': 0.27257413, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.56465429, 'rewards/CustomExplainationReward/std': 0.21805926, 'completions/mean_length': 165.25, 'completions/min_length': 116.0, 'completions/max_length': 208.0, 'completions/clipped_ratio': 0.0, 'kl': 0.00724867, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '5/200', 'percentage': '2.50%', 'elapsed_time': '6m 20s', 'remaining_time': '4h 7m 2s', 'memory(GiB)': 15.67, 'train_speed(iter/s)': 0.013156}
  [Sample 0] Raw GT: 'gạch', Raw Pred: 'Gạch'
  [Sample 0] Cleaned GT: 'gạch', Cleaned Pred: 'gạch'
  [Sample 1] Raw GT: 'gạch', Raw Pred: 'Gạch men'
  [Sample 1] Cleaned GT: 'gạch', Cleaned Pred: 'gạch men'
  [Sample 2] Raw GT: 'gạch', Raw Pred: 'Gạch'
  [Sample 2] Cleaned GT: 'gạch', Cleaned Pred: 'gạch'
  [Sample 3] Raw GT: 'gạch', Raw Pred: 'Gạch'
  [Sample 3] Cleaned GT: 'gạch', Cleaned Pred: 'gạch'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.7093, BERTScore=1.0000 -> Reward=0.8547
  [Sample 2] ROUGE-L=1.0000, BERTScore=0.1770 -> Reward=0.5885
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.5783, CLIP Max: 22.3820
   [Sample 0] BERTScore=0.7804, CLIPRaw=21.73 (CLIPNorm=0.6384) -> Reward=0.7094
   [Sample 1] BERTScore=0.8049, CLIPRaw=21.70 (CLIPNorm=0.6237) -> Reward=0.7143
   [Sample 2] BERTScore=0.7854, CLIPRaw=20.58 (CLIPNorm=0.0000) -> Reward=0.3927
   [Sample 3] BERTScore=0.8005, CLIPRaw=22.38 (CLIPNorm=1.0000) -> Reward=0.9002
{'loss': 0.00027788, 'grad_norm': 1.01878619, 'learning_rate': 6e-06, 'reward': 2.42492533, 'reward_std': 0.49559066, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.86079317, 'rewards/CustomAccuracyReward/std': 0.19401591, 'rewards/CustomExplainationReward/mean': 0.67914712, 'rewards/CustomExplainationReward/std': 0.21063416, 'completions/mean_length': 184.75, 'completions/min_length': 176.0, 'completions/max_length': 201.0, 'completions/clipped_ratio': 0.0, 'kl': 0.00694733, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '6/200', 'percentage': '3.00%', 'elapsed_time': '7m 36s', 'remaining_time': '4h 6m 8s', 'memory(GiB)': 15.67, 'train_speed(iter/s)': 0.013136}
  [Sample 0] Raw GT: 'trượt ván', Raw Pred: 'Thực hiện động tác khó'
  [Sample 0] Cleaned GT: 'trượt ván', Cleaned Pred: 'thực hiện động tác khó'
  [Sample 1] Raw GT: 'trượt ván', Raw Pred: 'Cậu bé đang chơi trượt ván.'
  [Sample 1] Cleaned GT: 'trượt ván', Cleaned Pred: 'cậu bé đang chơi trượt ván'
  [Sample 2] Raw GT: 'trượt ván', Raw Pred: 'Xoay'
  [Sample 2] Cleaned GT: 'trượt ván', Cleaned Pred: 'xoay'
  [Sample 3] Raw GT: 'trượt ván', Raw Pred: 'Thao tác ván trượt'
  [Sample 3] Cleaned GT: 'trượt ván', Cleaned Pred: 'thao tác ván trượt'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.3103 -> Reward=0.1552
  [Sample 1] ROUGE-L=0.5495, BERTScore=0.3797 -> Reward=0.4646
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.3567 -> Reward=0.1783
  [Sample 3] ROUGE-L=0.3547, BERTScore=0.4937 -> Reward=0.4242
   [Batch Stats] CLIP Min: 17.8896, CLIP Max: 19.9200
   [Sample 0] BERTScore=0.8448, CLIPRaw=18.55 (CLIPNorm=0.3273) -> Reward=0.5860
   [Sample 1] BERTScore=0.8229, CLIPRaw=19.92 (CLIPNorm=1.0000) -> Reward=0.9115
   [Sample 2] BERTScore=0.8100, CLIPRaw=17.89 (CLIPNorm=0.0000) -> Reward=0.4050
   [Sample 3] BERTScore=0.8331, CLIPRaw=18.95 (CLIPNorm=0.5224) -> Reward=0.6777
{'loss': 0.00024475, 'grad_norm': 1.1384815, 'learning_rate': 7e-06, 'reward': 1.68828011, 'reward_std': 0.44643489, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.30556884, 'rewards/CustomAccuracyReward/std': 0.16142172, 'rewards/CustomExplainationReward/mean': 0.64505529, 'rewards/CustomExplainationReward/std': 0.21068272, 'completions/mean_length': 206.0, 'completions/min_length': 164.0, 'completions/max_length': 277.0, 'completions/clipped_ratio': 0.0, 'kl': 0.00612494, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '7/200', 'percentage': '3.50%', 'elapsed_time': '8m 53s', 'remaining_time': '4h 5m 22s', 'memory(GiB)': 15.67, 'train_speed(iter/s)': 0.013109}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 0.0000, CLIP Max: 19.3856
   [Sample 0] BERTScore=0.7429, CLIPRaw=19.05 (CLIPNorm=0.9829) -> Reward=0.8629
   [Sample 1] BERTScore=0.7599, CLIPRaw=19.39 (CLIPNorm=1.0000) -> Reward=0.8800
   [Sample 2] BERTScore=0.7838, CLIPRaw=18.89 (CLIPNorm=0.9742) -> Reward=0.8790
   [Sample 3] BERTScore=0.0000, CLIPRaw=0.00 (CLIPNorm=0.0000) -> Reward=0.0000
{'loss': 0.00028409, 'grad_norm': 1.35394561, 'learning_rate': 8e-06, 'reward': 1.99928498, 'reward_std': 0.56830055, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 0.94999999, 'rewards/CustomFormatReward_ViVQA_X/std': 0.1, 'rewards/CustomAccuracyReward/mean': 0.56395739, 'rewards/CustomAccuracyReward/std': 0.50349867, 'rewards/CustomExplainationReward/mean': 0.65547061, 'rewards/CustomExplainationReward/std': 0.43705034, 'completions/mean_length': 175.0, 'completions/min_length': 131.0, 'completions/max_length': 280.0, 'completions/clipped_ratio': 0.0, 'kl': 0.00710388, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '8/200', 'percentage': '4.00%', 'elapsed_time': '10m 26s', 'remaining_time': '4h 10m 25s', 'memory(GiB)': 15.67, 'train_speed(iter/s)': 0.012778}
  [Sample 0] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 0] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 1] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 1] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 2] Raw GT: 'bóng chày', Raw Pred: 'Bóng đá'
  [Sample 2] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng đá'
  [Sample 3] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 3] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=0.5000, BERTScore=0.5582 -> Reward=0.5291
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 15.6763, CLIP Max: 16.3918
   [Sample 0] BERTScore=0.8080, CLIPRaw=16.16 (CLIPNorm=0.6696) -> Reward=0.7388
   [Sample 1] BERTScore=0.7911, CLIPRaw=15.68 (CLIPNorm=0.0000) -> Reward=0.3955
   [Sample 2] BERTScore=0.7946, CLIPRaw=15.97 (CLIPNorm=0.4059) -> Reward=0.6003
   [Sample 3] BERTScore=0.7793, CLIPRaw=16.39 (CLIPNorm=1.0000) -> Reward=0.8897
{'loss': 0.00023407, 'grad_norm': 0.87006783, 'learning_rate': 9e-06, 'reward': 2.42292213, 'reward_std': 0.42774385, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.88226926, 'rewards/CustomAccuracyReward/std': 0.2354615, 'rewards/CustomExplainationReward/mean': 0.6560685, 'rewards/CustomExplainationReward/std': 0.21008597, 'completions/mean_length': 223.5, 'completions/min_length': 191.0, 'completions/max_length': 284.0, 'completions/clipped_ratio': 0.0, 'kl': 0.00585516, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '9/200', 'percentage': '4.50%', 'elapsed_time': '11m 44s', 'remaining_time': '4h 9m 12s', 'memory(GiB)': 15.67, 'train_speed(iter/s)': 0.012774}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Nhận định không rõ ràng'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'nhận định không rõ ràng'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Nhận'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'nhận'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=0.3789, BERTScore=0.2558 -> Reward=0.3174
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.1665 -> Reward=0.0833
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2014 -> Reward=0.1007
   [Batch Stats] CLIP Min: 18.7132, CLIP Max: 21.2431
   [Sample 0] BERTScore=0.7609, CLIPRaw=20.21 (CLIPNorm=0.5928) -> Reward=0.6768
   [Sample 1] BERTScore=0.7593, CLIPRaw=18.71 (CLIPNorm=0.0000) -> Reward=0.3796
   [Sample 2] BERTScore=0.7528, CLIPRaw=21.24 (CLIPNorm=1.0000) -> Reward=0.8764
   [Sample 3] BERTScore=0.7565, CLIPRaw=20.50 (CLIPNorm=0.7056) -> Reward=0.7310
{'loss': 0.00027373, 'grad_norm': 1.13025987, 'learning_rate': 1e-05, 'reward': 1.52911723, 'reward_std': 0.2772395, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.15731429, 'rewards/CustomAccuracyReward/std': 0.10826403, 'rewards/CustomExplainationReward/mean': 0.6659795, 'rewards/CustomExplainationReward/std': 0.20866913, 'completions/mean_length': 162.5, 'completions/min_length': 139.0, 'completions/max_length': 184.0, 'completions/clipped_ratio': 0.0, 'kl': 0.00684516, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '10/200', 'percentage': '5.00%', 'elapsed_time': '12m 56s', 'remaining_time': '4h 5m 58s', 'memory(GiB)': 15.67, 'train_speed(iter/s)': 0.012874}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Ngược'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'ngược'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.0880 -> Reward=0.0440
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 0.0000, CLIP Max: 21.5893
   [Sample 0] BERTScore=0.8355, CLIPRaw=21.59 (CLIPNorm=1.0000) -> Reward=0.9178
   [Sample 1] BERTScore=0.0000, CLIPRaw=0.00 (CLIPNorm=0.0000) -> Reward=0.0000
   [Sample 2] BERTScore=0.8957, CLIPRaw=21.06 (CLIPNorm=0.9756) -> Reward=0.9356
   [Sample 3] BERTScore=0.8226, CLIPRaw=20.52 (CLIPNorm=0.9507) -> Reward=0.8866
{'loss': 0.00052123, 'grad_norm': 2.81577539, 'learning_rate': 1e-05, 'reward': 1.90998316, 'reward_std': 1.22566307, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 0.75, 'rewards/CustomFormatReward_ViVQA_X/std': 0.25166115, 'rewards/CustomAccuracyReward/mean': 0.54297674, 'rewards/CustomAccuracyReward/std': 0.52883601, 'rewards/CustomExplainationReward/mean': 0.68500978, 'rewards/CustomExplainationReward/std': 0.4571223, 'completions/mean_length': 95.75, 'completions/min_length': 52.0, 'completions/max_length': 200.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01303096, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '11/200', 'percentage': '5.50%', 'elapsed_time': '13m 57s', 'remaining_time': '3h 59m 53s', 'memory(GiB)': 15.67, 'train_speed(iter/s)': 0.013131}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Nhờn'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'nhờn'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Ngược'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'ngược'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2947 -> Reward=0.1474
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.0880 -> Reward=0.0440
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.4701, CLIP Max: 21.2486
   [Sample 0] BERTScore=0.7449, CLIPRaw=20.92 (CLIPNorm=0.5825) -> Reward=0.6637
   [Sample 1] BERTScore=0.7450, CLIPRaw=21.25 (CLIPNorm=1.0000) -> Reward=0.8725
   [Sample 2] BERTScore=0.7896, CLIPRaw=20.47 (CLIPNorm=0.0000) -> Reward=0.3948
   [Sample 3] BERTScore=0.7609, CLIPRaw=20.68 (CLIPNorm=0.2633) -> Reward=0.5121
{'loss': 0.00135621, 'grad_norm': 7.79074335, 'learning_rate': 1e-05, 'reward': 1.65072775, 'reward_std': 0.60035032, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 0.94999999, 'rewards/CustomFormatReward_ViVQA_X/std': 0.1, 'rewards/CustomAccuracyReward/mean': 0.32981515, 'rewards/CustomAccuracyReward/std': 0.44903547, 'rewards/CustomExplainationReward/mean': 0.61076707, 'rewards/CustomExplainationReward/std': 0.2063051, 'completions/mean_length': 138.0, 'completions/min_length': 36.0, 'completions/max_length': 194.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03390774, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '12/200', 'percentage': '6.00%', 'elapsed_time': '14m 58s', 'remaining_time': '3h 54m 30s', 'memory(GiB)': 15.67, 'train_speed(iter/s)': 0.013362}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có thể'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có thể'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có thể'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có thể'
  [Sample 0] ROUGE-L=0.7093, BERTScore=1.0000 -> Reward=0.8547
  [Sample 1] ROUGE-L=1.0000, BERTScore=0.3096 -> Reward=0.6548
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=0.7093, BERTScore=0.3096 -> Reward=0.5095
Error during BERTScore batch computation: The expanded size of the tensor (278) must match the existing size (258) at non-singleton dimension 1.  Target sizes: [3, 278].  Tensor sizes: [1, 258]
   [Batch Stats] CLIP Min: 0.0000, CLIP Max: 19.6757
   [Sample 0] BERTScore=0.0000, CLIPRaw=19.68 (CLIPNorm=1.0000) -> Reward=0.5000
   [Sample 1] BERTScore=0.0000, CLIPRaw=18.72 (CLIPNorm=0.9513) -> Reward=0.4756
   [Sample 2] BERTScore=0.0000, CLIPRaw=0.00 (CLIPNorm=0.0000) -> Reward=0.0000
   [Sample 3] BERTScore=0.0000, CLIPRaw=19.34 (CLIPNorm=0.9830) -> Reward=0.4915
{'loss': 0.00049455, 'grad_norm': 0.81417936, 'learning_rate': 9.99e-06, 'reward': 1.87690425, 'reward_std': 0.23713534, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 0.94999999, 'rewards/CustomFormatReward_ViVQA_X/std': 0.1, 'rewards/CustomAccuracyReward/mean': 0.75473481, 'rewards/CustomAccuracyReward/std': 0.21623798, 'rewards/CustomExplainationReward/mean': 0.3667886, 'rewards/CustomExplainationReward/std': 0.24473426, 'completions/mean_length': 269.25, 'completions/min_length': 210.0, 'completions/max_length': 395.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01236927, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '13/200', 'percentage': '6.50%', 'elapsed_time': '16m 59s', 'remaining_time': '4h 4m 28s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.012748}
  [Sample 0] Raw GT: 'nhà thờ', Raw Pred: 'Thuộc về Nhà thờ'
  [Sample 0] Cleaned GT: 'nhà thờ', Cleaned Pred: 'thuộc về nhà thờ'
  [Sample 1] Raw GT: 'nhà thờ', Raw Pred: 'Tòa nhà lớn Gothic'
  [Sample 1] Cleaned GT: 'nhà thờ', Cleaned Pred: 'tòa nhà lớn gothic'
  [Sample 2] Raw GT: 'nhà thờ', Raw Pred: 'Tòa nhà'
  [Sample 2] Cleaned GT: 'nhà thờ', Cleaned Pred: 'tòa nhà'
  [Sample 3] Raw GT: 'nhà thờ', Raw Pred: 'Thánh đường'
  [Sample 3] Cleaned GT: 'nhà thờ', Cleaned Pred: 'thánh đường'
  [Sample 0] ROUGE-L=0.7093, BERTScore=0.2705 -> Reward=0.4899
  [Sample 1] ROUGE-L=0.3547, BERTScore=0.4609 -> Reward=0.4078
  [Sample 2] ROUGE-L=0.5000, BERTScore=0.2582 -> Reward=0.3791
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.3541 -> Reward=0.1770
   [Batch Stats] CLIP Min: 17.4653, CLIP Max: 17.7904
   [Sample 0] BERTScore=0.7125, CLIPRaw=17.79 (CLIPNorm=1.0000) -> Reward=0.8563
   [Sample 1] BERTScore=0.7288, CLIPRaw=17.49 (CLIPNorm=0.0695) -> Reward=0.3992
   [Sample 2] BERTScore=0.7316, CLIPRaw=17.47 (CLIPNorm=0.0000) -> Reward=0.3658
   [Sample 3] BERTScore=0.7245, CLIPRaw=17.62 (CLIPNorm=0.4867) -> Reward=0.6056
{'loss': 0.00029877, 'grad_norm': 0.90682334, 'learning_rate': 9.99e-06, 'reward': 1.65017569, 'reward_std': 0.35644066, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.36343658, 'rewards/CustomAccuracyReward/std': 0.13284688, 'rewards/CustomExplainationReward/mean': 0.55670398, 'rewards/CustomExplainationReward/std': 0.22612849, 'completions/mean_length': 185.25, 'completions/min_length': 161.0, 'completions/max_length': 209.0, 'completions/clipped_ratio': 0.0, 'kl': 0.00746963, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '14/200', 'percentage': '7.00%', 'elapsed_time': '18m 19s', 'remaining_time': '4h 3m 32s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.012729}
  [Sample 0] Raw GT: 'ngồi', Raw Pred: 'Nghỉ ngơi'
  [Sample 0] Cleaned GT: 'ngồi', Cleaned Pred: 'nghỉ ngơi'
  [Sample 1] Raw GT: 'ngồi', Raw Pred: 'Nghỉ ngơi'
  [Sample 1] Cleaned GT: 'ngồi', Cleaned Pred: 'nghỉ ngơi'
  [Sample 2] Raw GT: 'ngồi', Raw Pred: 'Có thể sử dụng để ngồi nghỉ.'
  [Sample 2] Cleaned GT: 'ngồi', Cleaned Pred: 'có thể sử dụng để ngồi nghỉ'
  [Sample 3] Raw GT: 'ngồi', Raw Pred: 'Nghỉ ngơi'
  [Sample 3] Cleaned GT: 'ngồi', Cleaned Pred: 'nghỉ ngơi'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2842 -> Reward=0.1421
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2842 -> Reward=0.1421
  [Sample 2] ROUGE-L=0.2891, BERTScore=0.2674 -> Reward=0.2783
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2842 -> Reward=0.1421
   [Batch Stats] CLIP Min: 20.9615, CLIP Max: 21.4784
   [Sample 0] BERTScore=0.7871, CLIPRaw=21.44 (CLIPNorm=0.9192) -> Reward=0.8532
   [Sample 1] BERTScore=0.7842, CLIPRaw=21.17 (CLIPNorm=0.4015) -> Reward=0.5929
   [Sample 2] BERTScore=0.7639, CLIPRaw=21.48 (CLIPNorm=1.0000) -> Reward=0.8820
   [Sample 3] BERTScore=0.8371, CLIPRaw=20.96 (CLIPNorm=0.0000) -> Reward=0.4185
{'loss': 0.00041148, 'grad_norm': 1.26885259, 'learning_rate': 9.98e-06, 'reward': 1.5784955, 'reward_std': 0.33357692, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.17615595, 'rewards/CustomAccuracyReward/std': 0.06806322, 'rewards/CustomExplainationReward/mean': 0.68664044, 'rewards/CustomExplainationReward/std': 0.22103333, 'completions/mean_length': 165.5, 'completions/min_length': 124.0, 'completions/max_length': 239.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0102881, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '15/200', 'percentage': '7.50%', 'elapsed_time': '19m 30s', 'remaining_time': '4h 0m 36s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.012814}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 17.9006, CLIP Max: 19.5067
   [Sample 0] BERTScore=0.7596, CLIPRaw=18.05 (CLIPNorm=0.0948) -> Reward=0.4272
   [Sample 1] BERTScore=0.7759, CLIPRaw=19.51 (CLIPNorm=1.0000) -> Reward=0.8879
   [Sample 2] BERTScore=0.7400, CLIPRaw=17.90 (CLIPNorm=0.0000) -> Reward=0.3700
   [Sample 3] BERTScore=0.7823, CLIPRaw=18.44 (CLIPNorm=0.3364) -> Reward=0.5593
{'loss': 0.00030066, 'grad_norm': 0.98875606, 'learning_rate': 9.98e-06, 'reward': 2.4513979, 'reward_std': 0.28982085, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.56111825, 'rewards/CustomExplainationReward/std': 0.23185667, 'completions/mean_length': 165.25, 'completions/min_length': 119.0, 'completions/max_length': 205.0, 'completions/clipped_ratio': 0.0, 'kl': 0.00751277, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '16/200', 'percentage': '8.00%', 'elapsed_time': '20m 33s', 'remaining_time': '3h 56m 26s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.01297}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.3228, CLIP Max: 20.6977
   [Sample 0] BERTScore=0.8509, CLIPRaw=18.32 (CLIPNorm=0.0000) -> Reward=0.4255
   [Sample 1] BERTScore=0.8101, CLIPRaw=19.72 (CLIPNorm=0.5874) -> Reward=0.6987
   [Sample 2] BERTScore=0.8562, CLIPRaw=20.69 (CLIPNorm=0.9960) -> Reward=0.9261
   [Sample 3] BERTScore=0.8109, CLIPRaw=20.70 (CLIPNorm=1.0000) -> Reward=0.9055
{'loss': 0.00039302, 'grad_norm': 0.94547808, 'learning_rate': 9.97e-06, 'reward': 2.67366409, 'reward_std': 0.2910527, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.73893124, 'rewards/CustomExplainationReward/std': 0.2328421, 'completions/mean_length': 161.5, 'completions/min_length': 139.0, 'completions/max_length': 202.0, 'completions/clipped_ratio': 0.0, 'kl': 0.00982045, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '17/200', 'percentage': '8.50%', 'elapsed_time': '21m 25s', 'remaining_time': '3h 50m 38s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013224}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.1474, CLIP Max: 22.3313
   [Sample 0] BERTScore=0.7902, CLIPRaw=19.15 (CLIPNorm=0.0000) -> Reward=0.3951
   [Sample 1] BERTScore=0.7868, CLIPRaw=20.66 (CLIPNorm=0.4764) -> Reward=0.6316
   [Sample 2] BERTScore=0.8133, CLIPRaw=20.09 (CLIPNorm=0.2971) -> Reward=0.5552
   [Sample 3] BERTScore=0.2736, CLIPRaw=22.33 (CLIPNorm=1.0000) -> Reward=0.6368
{'loss': 0.00046793, 'grad_norm': 1.07671678, 'learning_rate': 9.96e-06, 'reward': 2.1708312, 'reward_std': 0.49705437, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.78197873, 'rewards/CustomAccuracyReward/std': 0.43604261, 'rewards/CustomExplainationReward/mean': 0.55468625, 'rewards/CustomExplainationReward/std': 0.11274642, 'completions/mean_length': 183.25, 'completions/min_length': 137.0, 'completions/max_length': 246.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01169894, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '18/200', 'percentage': '9.00%', 'elapsed_time': '22m 37s', 'remaining_time': '3h 48m 41s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013263}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 21.0814, CLIP Max: 21.7276
   [Sample 0] BERTScore=0.8058, CLIPRaw=21.73 (CLIPNorm=1.0000) -> Reward=0.9029
   [Sample 1] BERTScore=0.7837, CLIPRaw=21.39 (CLIPNorm=0.4787) -> Reward=0.6312
   [Sample 2] BERTScore=0.8165, CLIPRaw=21.08 (CLIPNorm=0.0000) -> Reward=0.4082
   [Sample 3] BERTScore=0.7673, CLIPRaw=21.22 (CLIPNorm=0.2121) -> Reward=0.4897
{'loss': 0.00052793, 'grad_norm': 1.47631979, 'learning_rate': 9.94e-06, 'reward': 1.69243121, 'reward_std': 0.79914504, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.34593609, 'rewards/CustomAccuracyReward/std': 0.43604264, 'rewards/CustomExplainationReward/mean': 0.60800874, 'rewards/CustomExplainationReward/std': 0.21711378, 'completions/mean_length': 118.5, 'completions/min_length': 92.0, 'completions/max_length': 149.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01319511, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '19/200', 'percentage': '9.50%', 'elapsed_time': '23m 27s', 'remaining_time': '3h 43m 26s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013501}
  [Sample 0] Raw GT: 'chuối', Raw Pred: 'Chuối'
  [Sample 0] Cleaned GT: 'chuối', Cleaned Pred: 'chuối'
  [Sample 1] Raw GT: 'chuối', Raw Pred: 'Chuối'
  [Sample 1] Cleaned GT: 'chuối', Cleaned Pred: 'chuối'
  [Sample 2] Raw GT: 'chuối', Raw Pred: 'Chuối'
  [Sample 2] Cleaned GT: 'chuối', Cleaned Pred: 'chuối'
  [Sample 3] Raw GT: 'chuối', Raw Pred: 'Chuối'
  [Sample 3] Cleaned GT: 'chuối', Cleaned Pred: 'chuối'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
Error during BERTScore batch computation: The expanded size of the tensor (260) must match the existing size (258) at non-singleton dimension 1.  Target sizes: [4, 260].  Tensor sizes: [1, 258]
   [Batch Stats] CLIP Min: 19.1182, CLIP Max: 20.8068
   [Sample 0] BERTScore=0.0000, CLIPRaw=19.12 (CLIPNorm=0.0000) -> Reward=0.0000
   [Sample 1] BERTScore=0.0000, CLIPRaw=20.81 (CLIPNorm=1.0000) -> Reward=0.5000
   [Sample 2] BERTScore=0.0000, CLIPRaw=20.31 (CLIPNorm=0.7048) -> Reward=0.3524
   [Sample 3] BERTScore=0.0000, CLIPRaw=19.25 (CLIPNorm=0.0766) -> Reward=0.0383
{'loss': 0.00065586, 'grad_norm': 1.08881462, 'learning_rate': 9.93e-06, 'reward': 2.02834702, 'reward_std': 0.30388957, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.22267748, 'rewards/CustomExplainationReward/std': 0.24311166, 'completions/mean_length': 153.25, 'completions/min_length': 110.0, 'completions/max_length': 216.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01638377, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '20/200', 'percentage': '10.00%', 'elapsed_time': '24m 47s', 'remaining_time': '3h 43m 5s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013447}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 18.4344, CLIP Max: 20.2707
   [Sample 0] BERTScore=0.8404, CLIPRaw=20.27 (CLIPNorm=1.0000) -> Reward=0.9202
   [Sample 1] BERTScore=0.8289, CLIPRaw=19.50 (CLIPNorm=0.5807) -> Reward=0.7048
   [Sample 2] BERTScore=0.7933, CLIPRaw=19.73 (CLIPNorm=0.7071) -> Reward=0.7502
   [Sample 3] BERTScore=0.8209, CLIPRaw=18.43 (CLIPNorm=0.0000) -> Reward=0.4105
{'loss': 0.00032425, 'grad_norm': 1.59565854, 'learning_rate': 9.92e-06, 'reward': 2.3479681, 'reward_std': 0.79184842, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.78197873, 'rewards/CustomAccuracyReward/std': 0.43604261, 'rewards/CustomExplainationReward/mean': 0.69639575, 'rewards/CustomExplainationReward/std': 0.21196841, 'completions/mean_length': 136.25, 'completions/min_length': 73.0, 'completions/max_length': 200.0, 'completions/clipped_ratio': 0.0, 'kl': 0.008103, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '21/200', 'percentage': '10.50%', 'elapsed_time': '26m 4s', 'remaining_time': '3h 42m 18s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013419}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.3304, CLIP Max: 19.9902
   [Sample 0] BERTScore=0.8057, CLIPRaw=19.66 (CLIPNorm=0.4975) -> Reward=0.6516
   [Sample 1] BERTScore=0.7866, CLIPRaw=19.99 (CLIPNorm=1.0000) -> Reward=0.8933
   [Sample 2] BERTScore=0.7306, CLIPRaw=19.34 (CLIPNorm=0.0100) -> Reward=0.3703
   [Sample 3] BERTScore=0.7624, CLIPRaw=19.33 (CLIPNorm=0.0000) -> Reward=0.3812
{'loss': 0.00034718, 'grad_norm': 1.1597538, 'learning_rate': 9.9e-06, 'reward': 2.46761751, 'reward_std': 0.3117837, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.57409406, 'rewards/CustomExplainationReward/std': 0.24942692, 'completions/mean_length': 161.0, 'completions/min_length': 113.0, 'completions/max_length': 221.0, 'completions/clipped_ratio': 0.0, 'kl': 0.00868879, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '22/200', 'percentage': '11.00%', 'elapsed_time': '27m 11s', 'remaining_time': '3h 39m 58s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013487}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Nhận xét'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'nhận xét'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Đúng'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=0.0000, BERTScore=1.0000 -> Reward=0.5000
  [Sample 1] ROUGE-L=0.0000, BERTScore=1.0000 -> Reward=0.5000
  [Sample 2] ROUGE-L=1.0000, BERTScore=0.0907 -> Reward=0.5454
  [Sample 3] ROUGE-L=1.0000, BERTScore=0.2558 -> Reward=0.6279
   [Batch Stats] CLIP Min: 18.7117, CLIP Max: 20.7148
   [Sample 0] BERTScore=0.7404, CLIPRaw=19.57 (CLIPNorm=0.4279) -> Reward=0.5841
   [Sample 1] BERTScore=0.7502, CLIPRaw=18.71 (CLIPNorm=0.0000) -> Reward=0.3751
   [Sample 2] BERTScore=0.6862, CLIPRaw=19.55 (CLIPNorm=0.4171) -> Reward=0.5517
   [Sample 3] BERTScore=0.7571, CLIPRaw=20.71 (CLIPNorm=1.0000) -> Reward=0.8785
{'loss': 0.00032642, 'grad_norm': 1.17983341, 'learning_rate': 9.88e-06, 'reward': 1.92584848, 'reward_std': 0.33027828, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.54331684, 'rewards/CustomAccuracyReward/std': 0.06031485, 'rewards/CustomExplainationReward/mean': 0.59736198, 'rewards/CustomExplainationReward/std': 0.20875508, 'completions/mean_length': 136.25, 'completions/min_length': 128.0, 'completions/max_length': 147.0, 'completions/clipped_ratio': 0.0, 'kl': 0.00816541, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '23/200', 'percentage': '11.50%', 'elapsed_time': '28m 1s', 'remaining_time': '3h 35m 42s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013676}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Nó không'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'nó không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=0.7093, BERTScore=0.2178 -> Reward=0.4636
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.1287, CLIP Max: 21.6045
   [Sample 0] BERTScore=0.7340, CLIPRaw=21.40 (CLIPNorm=0.8594) -> Reward=0.7967
   [Sample 1] BERTScore=0.7741, CLIPRaw=21.26 (CLIPNorm=0.7665) -> Reward=0.7703
   [Sample 2] BERTScore=0.7339, CLIPRaw=21.60 (CLIPNorm=1.0000) -> Reward=0.8669
   [Sample 3] BERTScore=0.7397, CLIPRaw=20.13 (CLIPNorm=0.0000) -> Reward=0.3699
{'loss': 0.00079626, 'grad_norm': 1.07683933, 'learning_rate': 9.87e-06, 'reward': 2.45853853, 'reward_std': 0.3136504, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.86588788, 'rewards/CustomAccuracyReward/std': 0.2682243, 'rewards/CustomExplainationReward/mean': 0.70094299, 'rewards/CustomExplainationReward/std': 0.22446127, 'completions/mean_length': 141.25, 'completions/min_length': 105.0, 'completions/max_length': 203.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01990704, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '24/200', 'percentage': '12.00%', 'elapsed_time': '28m 58s', 'remaining_time': '3h 32m 30s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013804}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 17.8031, CLIP Max: 19.4981
   [Sample 0] BERTScore=0.8016, CLIPRaw=18.69 (CLIPNorm=0.5258) -> Reward=0.6637
   [Sample 1] BERTScore=0.8018, CLIPRaw=17.80 (CLIPNorm=0.0000) -> Reward=0.4009
   [Sample 2] BERTScore=0.7971, CLIPRaw=19.50 (CLIPNorm=1.0000) -> Reward=0.8985
   [Sample 3] BERTScore=0.7765, CLIPRaw=18.19 (CLIPNorm=0.2292) -> Reward=0.5028
{'loss': 0.00034823, 'grad_norm': 0.90137035, 'learning_rate': 9.85e-06, 'reward': 2.52060819, 'reward_std': 0.27116105, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.61648655, 'rewards/CustomExplainationReward/std': 0.21692881, 'completions/mean_length': 163.0, 'completions/min_length': 145.0, 'completions/max_length': 192.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0087058, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '25/200', 'percentage': '12.50%', 'elapsed_time': '30m 1s', 'remaining_time': '3h 30m 10s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013878}
  [Sample 0] Raw GT: 'nắng', Raw Pred: 'Nắng'
  [Sample 0] Cleaned GT: 'nắng', Cleaned Pred: 'nắng'
  [Sample 1] Raw GT: 'nắng', Raw Pred: 'Nắng'
  [Sample 1] Cleaned GT: 'nắng', Cleaned Pred: 'nắng'
  [Sample 2] Raw GT: 'nắng', Raw Pred: 'Nắng'
  [Sample 2] Cleaned GT: 'nắng', Cleaned Pred: 'nắng'
  [Sample 3] Raw GT: 'nắng', Raw Pred: 'Trời nắng'
  [Sample 3] Cleaned GT: 'nắng', Cleaned Pred: 'trời nắng'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=0.7093, BERTScore=0.4792 -> Reward=0.5942
   [Batch Stats] CLIP Min: 19.3543, CLIP Max: 20.1908
   [Sample 0] BERTScore=0.7979, CLIPRaw=19.63 (CLIPNorm=0.3277) -> Reward=0.5628
   [Sample 1] BERTScore=0.8166, CLIPRaw=20.19 (CLIPNorm=1.0000) -> Reward=0.9083
   [Sample 2] BERTScore=0.8149, CLIPRaw=19.35 (CLIPNorm=0.0000) -> Reward=0.4074
   [Sample 3] BERTScore=0.8167, CLIPRaw=19.80 (CLIPNorm=0.5340) -> Reward=0.6753
{'loss': 0.00046913, 'grad_norm': 1.05535746, 'learning_rate': 9.83e-06, 'reward': 2.42127991, 'reward_std': 0.34371769, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.89855802, 'rewards/CustomAccuracyReward/std': 0.20288393, 'rewards/CustomExplainationReward/mean': 0.638466, 'rewards/CustomExplainationReward/std': 0.21076886, 'completions/mean_length': 127.25, 'completions/min_length': 111.0, 'completions/max_length': 147.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01173704, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '26/200', 'percentage': '13.00%', 'elapsed_time': '30m 53s', 'remaining_time': '3h 26m 44s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.014027}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Nhận định'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'nhận định'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Nó không phải là ban ngày.'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'nó không phải là ban ngày'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.1616 -> Reward=0.0808
  [Sample 2] ROUGE-L=0.3280, BERTScore=1.0000 -> Reward=0.6640
  [Sample 3] ROUGE-L=1.0000, BERTScore=0.2051 -> Reward=0.6026
   [Batch Stats] CLIP Min: 18.7889, CLIP Max: 19.5253
   [Sample 0] BERTScore=0.8235, CLIPRaw=18.85 (CLIPNorm=0.0824) -> Reward=0.4529
   [Sample 1] BERTScore=0.8128, CLIPRaw=19.30 (CLIPNorm=0.6916) -> Reward=0.7522
   [Sample 2] BERTScore=0.8084, CLIPRaw=19.53 (CLIPNorm=1.0000) -> Reward=0.9042
   [Sample 3] BERTScore=0.8158, CLIPRaw=18.79 (CLIPNorm=0.0000) -> Reward=0.4079
{'loss': 0.00054713, 'grad_norm': 1.06525767, 'learning_rate': 9.8e-06, 'reward': 2.02018452, 'reward_std': 0.43843621, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.58684111, 'rewards/CustomAccuracyReward/std': 0.37989631, 'rewards/CustomExplainationReward/mean': 0.62930644, 'rewards/CustomExplainationReward/std': 0.23861332, 'completions/mean_length': 154.75, 'completions/min_length': 111.0, 'completions/max_length': 194.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01367487, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '27/200', 'percentage': '13.50%', 'elapsed_time': '32m 9s', 'remaining_time': '3h 26m 3s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013993}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.0920, CLIP Max: 20.2609
   [Sample 0] BERTScore=0.7779, CLIPRaw=20.26 (CLIPNorm=1.0000) -> Reward=0.8889
   [Sample 1] BERTScore=0.7911, CLIPRaw=19.33 (CLIPNorm=0.2031) -> Reward=0.4971
   [Sample 2] BERTScore=0.7988, CLIPRaw=19.60 (CLIPNorm=0.4309) -> Reward=0.6148
   [Sample 3] BERTScore=0.8206, CLIPRaw=19.09 (CLIPNorm=0.0000) -> Reward=0.4103
{'loss': 0.0006444, 'grad_norm': 1.03124058, 'learning_rate': 9.78e-06, 'reward': 2.5034883, 'reward_std': 0.26046753, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.60279059, 'rewards/CustomExplainationReward/std': 0.20837404, 'completions/mean_length': 186.5, 'completions/min_length': 159.0, 'completions/max_length': 244.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01610403, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '28/200', 'percentage': '14.00%', 'elapsed_time': '33m 22s', 'remaining_time': '3h 25m 2s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013981}
  [Sample 0] Raw GT: 'cột', Raw Pred: 'kìm'
  [Sample 0] Cleaned GT: 'cột', Cleaned Pred: 'kìm'
  [Sample 1] Raw GT: 'cột', Raw Pred: 'dùi cui tuyết'
  [Sample 1] Cleaned GT: 'cột', Cleaned Pred: 'dùi cui tuyết'
  [Sample 2] Raw GT: 'cột', Raw Pred: 'Cái gậy'
  [Sample 2] Cleaned GT: 'cột', Cleaned Pred: 'cái gậy'
  [Sample 3] Raw GT: 'cột', Raw Pred: 'Gậy chăn tuyết'
  [Sample 3] Cleaned GT: 'cột', Cleaned Pred: 'gậy chăn tuyết'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.5057 -> Reward=0.2528
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2986 -> Reward=0.1493
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.4040 -> Reward=0.2020
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.4584 -> Reward=0.2292
Error during BERTScore batch computation: The expanded size of the tensor (264) must match the existing size (258) at non-singleton dimension 1.  Target sizes: [4, 264].  Tensor sizes: [1, 258]
   [Batch Stats] CLIP Min: 18.0204, CLIP Max: 19.5432
   [Sample 0] BERTScore=0.0000, CLIPRaw=18.75 (CLIPNorm=0.4797) -> Reward=0.2398
   [Sample 1] BERTScore=0.0000, CLIPRaw=18.02 (CLIPNorm=0.0000) -> Reward=0.0000
   [Sample 2] BERTScore=0.0000, CLIPRaw=19.54 (CLIPNorm=1.0000) -> Reward=0.5000
   [Sample 3] BERTScore=0.0000, CLIPRaw=18.44 (CLIPNorm=0.2760) -> Reward=0.1380
{'loss': 0.00055128, 'grad_norm': 1.02106428, 'learning_rate': 9.76e-06, 'reward': 1.03472209, 'reward_std': 0.28924158, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.20832682, 'rewards/CustomAccuracyReward/std': 0.04449982, 'rewards/CustomExplainationReward/mean': 0.21945091, 'rewards/CustomExplainationReward/std': 0.21128134, 'completions/mean_length': 202.0, 'completions/min_length': 180.0, 'completions/max_length': 235.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01378625, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '29/200', 'percentage': '14.50%', 'elapsed_time': '34m 33s', 'remaining_time': '3h 23m 47s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013985}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Yes'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Không'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Không'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 18.4323, CLIP Max: 20.5209
   [Sample 0] BERTScore=0.7730, CLIPRaw=18.89 (CLIPNorm=0.2181) -> Reward=0.4956
   [Sample 1] BERTScore=0.7886, CLIPRaw=18.53 (CLIPNorm=0.0449) -> Reward=0.4168
   [Sample 2] BERTScore=0.7614, CLIPRaw=18.43 (CLIPNorm=0.0000) -> Reward=0.3807
   [Sample 3] BERTScore=0.7776, CLIPRaw=20.52 (CLIPNorm=1.0000) -> Reward=0.8888
{'loss': 0.00051044, 'grad_norm': 1.01874244, 'learning_rate': 9.73e-06, 'reward': 1.61422944, 'reward_std': 0.58070827, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.34593609, 'rewards/CustomAccuracyReward/std': 0.43604264, 'rewards/CustomExplainationReward/mean': 0.54544753, 'rewards/CustomExplainationReward/std': 0.23385951, 'completions/mean_length': 158.5, 'completions/min_length': 133.0, 'completions/max_length': 206.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01276445, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '30/200', 'percentage': '15.00%', 'elapsed_time': '35m 53s', 'remaining_time': '3h 23m 24s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013929}
  [Sample 0] Raw GT: 'đứng', Raw Pred: 'Đi bộ'
  [Sample 0] Cleaned GT: 'đứng', Cleaned Pred: 'đi bộ'
  [Sample 1] Raw GT: 'đứng', Raw Pred: 'Chú ý'
  [Sample 1] Cleaned GT: 'đứng', Cleaned Pred: 'chú ý'
  [Sample 2] Raw GT: 'đứng', Raw Pred: 'Quan sát'
  [Sample 2] Cleaned GT: 'đứng', Cleaned Pred: 'quan sát'
  [Sample 3] Raw GT: 'đứng', Raw Pred: 'Không có hành động cụ thể'
  [Sample 3] Cleaned GT: 'đứng', Cleaned Pred: 'không có hành động cụ thể'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2323 -> Reward=0.1161
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2110 -> Reward=0.1055
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.1924 -> Reward=0.0962
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2124 -> Reward=0.1062
   [Batch Stats] CLIP Min: 19.8100, CLIP Max: 20.7140
   [Sample 0] BERTScore=0.7832, CLIPRaw=20.71 (CLIPNorm=1.0000) -> Reward=0.8916
   [Sample 1] BERTScore=0.7675, CLIPRaw=20.28 (CLIPNorm=0.5152) -> Reward=0.6413
   [Sample 2] BERTScore=0.7662, CLIPRaw=19.81 (CLIPNorm=0.0000) -> Reward=0.3831
   [Sample 3] BERTScore=0.7831, CLIPRaw=20.38 (CLIPNorm=0.6251) -> Reward=0.7041
{'loss': 0.00090626, 'grad_norm': 1.13522732, 'learning_rate': 9.7e-06, 'reward': 1.45131481, 'reward_std': 0.27281585, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.10600862, 'rewards/CustomAccuracyReward/std': 0.00815708, 'rewards/CustomExplainationReward/mean': 0.65504324, 'rewards/CustomExplainationReward/std': 0.21018076, 'completions/mean_length': 158.0, 'completions/min_length': 151.0, 'completions/max_length': 163.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02265717, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '31/200', 'percentage': '15.50%', 'elapsed_time': '36m 48s', 'remaining_time': '3h 20m 39s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.014037}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 21.6214, CLIP Max: 22.0586
   [Sample 0] BERTScore=0.8242, CLIPRaw=22.06 (CLIPNorm=1.0000) -> Reward=0.9121
   [Sample 1] BERTScore=0.8051, CLIPRaw=21.72 (CLIPNorm=0.2257) -> Reward=0.5154
   [Sample 2] BERTScore=0.8218, CLIPRaw=21.66 (CLIPNorm=0.0955) -> Reward=0.4587
   [Sample 3] BERTScore=0.8339, CLIPRaw=21.62 (CLIPNorm=0.0000) -> Reward=0.4169
{'loss': 0.0003611, 'grad_norm': 0.76806527, 'learning_rate': 9.67e-06, 'reward': 2.46972227, 'reward_std': 0.28477851, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.57577777, 'rewards/CustomExplainationReward/std': 0.22782278, 'completions/mean_length': 228.75, 'completions/min_length': 186.0, 'completions/max_length': 262.0, 'completions/clipped_ratio': 0.0, 'kl': 0.00901769, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '32/200', 'percentage': '16.00%', 'elapsed_time': '38m 25s', 'remaining_time': '3h 21m 42s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013881}
  [Sample 0] Raw GT: 'tủ lạnh', Raw Pred: 'Tủ lạnh'
  [Sample 0] Cleaned GT: 'tủ lạnh', Cleaned Pred: 'tủ lạnh'
  [Sample 1] Raw GT: 'tủ lạnh', Raw Pred: 'Thùng rác'
  [Sample 1] Cleaned GT: 'tủ lạnh', Cleaned Pred: 'thùng rác'
  [Sample 2] Raw GT: 'tủ lạnh', Raw Pred: 'Chiếc tủ lạnh'
  [Sample 2] Cleaned GT: 'tủ lạnh', Cleaned Pred: 'chiếc tủ lạnh'
  [Sample 3] Raw GT: 'tủ lạnh', Raw Pred: 'Tủ lạnh'
  [Sample 3] Cleaned GT: 'tủ lạnh', Cleaned Pred: 'tủ lạnh'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.3558 -> Reward=0.1779
  [Sample 2] ROUGE-L=0.8299, BERTScore=0.5944 -> Reward=0.7122
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.4098, CLIP Max: 19.4610
   [Sample 0] BERTScore=0.8164, CLIPRaw=19.46 (CLIPNorm=1.0000) -> Reward=0.9082
   [Sample 1] BERTScore=0.8585, CLIPRaw=18.41 (CLIPNorm=0.0000) -> Reward=0.4292
   [Sample 2] BERTScore=0.8177, CLIPRaw=19.44 (CLIPNorm=0.9772) -> Reward=0.8975
   [Sample 3] BERTScore=0.8179, CLIPRaw=19.04 (CLIPNorm=0.6024) -> Reward=0.7101
{'loss': 0.00065043, 'grad_norm': 1.06777668, 'learning_rate': 9.64e-06, 'reward': 2.32349014, 'reward_std': 0.72643864, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.72252506, 'rewards/CustomAccuracyReward/std': 0.38759366, 'rewards/CustomExplainationReward/mean': 0.73626715, 'rewards/CustomExplainationReward/std': 0.22398265, 'completions/mean_length': 146.5, 'completions/min_length': 132.0, 'completions/max_length': 173.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01626353, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '33/200', 'percentage': '16.50%', 'elapsed_time': '39m 38s', 'remaining_time': '3h 20m 36s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013874}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.0658, CLIP Max: 19.3792
   [Sample 0] BERTScore=0.7606, CLIPRaw=19.09 (CLIPNorm=0.0633) -> Reward=0.4120
   [Sample 1] BERTScore=0.7471, CLIPRaw=19.07 (CLIPNorm=0.0000) -> Reward=0.3735
   [Sample 2] BERTScore=0.7965, CLIPRaw=19.29 (CLIPNorm=0.7170) -> Reward=0.7568
   [Sample 3] BERTScore=0.7464, CLIPRaw=19.38 (CLIPNorm=1.0000) -> Reward=0.8732
{'loss': 0.00086594, 'grad_norm': 1.38389063, 'learning_rate': 9.61e-06, 'reward': 2.50483513, 'reward_std': 0.31107304, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.60386813, 'rewards/CustomExplainationReward/std': 0.2488585, 'completions/mean_length': 119.25, 'completions/min_length': 102.0, 'completions/max_length': 140.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02164795, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '34/200', 'percentage': '17.00%', 'elapsed_time': '40m 46s', 'remaining_time': '3h 19m 5s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013896}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Yes'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Không'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Đúng'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Đúng'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 21.1471, CLIP Max: 21.9054
   [Sample 0] BERTScore=0.8196, CLIPRaw=21.78 (CLIPNorm=0.8341) -> Reward=0.8268
   [Sample 1] BERTScore=0.8174, CLIPRaw=21.91 (CLIPNorm=1.0000) -> Reward=0.9087
   [Sample 2] BERTScore=0.8072, CLIPRaw=21.53 (CLIPNorm=0.5059) -> Reward=0.6565
   [Sample 3] BERTScore=0.7868, CLIPRaw=21.15 (CLIPNorm=0.0000) -> Reward=0.3934
{'loss': 0.00057531, 'grad_norm': 1.03371775, 'learning_rate': 9.58e-06, 'reward': 2.34793854, 'reward_std': 0.43032685, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.78197873, 'rewards/CustomAccuracyReward/std': 0.43604261, 'rewards/CustomExplainationReward/mean': 0.69637215, 'rewards/CustomExplainationReward/std': 0.22765701, 'completions/mean_length': 181.25, 'completions/min_length': 167.0, 'completions/max_length': 202.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01438252, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '35/200', 'percentage': '17.50%', 'elapsed_time': '42m 6s', 'remaining_time': '3h 18m 32s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013851}
  [Sample 0] Raw GT: 'ăn', Raw Pred: 'Nhận thức về thức ăn'
  [Sample 0] Cleaned GT: 'ăn', Cleaned Pred: 'nhận thức về thức ăn'
  [Sample 1] Raw GT: 'ăn', Raw Pred: 'Ngựa hươu đang ăn.'
  [Sample 1] Cleaned GT: 'ăn', Cleaned Pred: 'ngựa hươu đang ăn'
  [Sample 2] Raw GT: 'ăn', Raw Pred: 'Chạy'
  [Sample 2] Cleaned GT: 'ăn', Cleaned Pred: 'chạy'
  [Sample 3] Raw GT: 'ăn', Raw Pred: 'Ngựa hươu đang ăn cỏ.'
  [Sample 3] Cleaned GT: 'ăn', Cleaned Pred: 'ngựa hươu đang ăn cỏ'
  [Sample 0] ROUGE-L=0.3789, BERTScore=0.3661 -> Reward=0.3725
  [Sample 1] ROUGE-L=0.4485, BERTScore=0.2743 -> Reward=0.3614
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.4285 -> Reward=0.2143
  [Sample 3] ROUGE-L=0.3789, BERTScore=0.2302 -> Reward=0.3045
   [Batch Stats] CLIP Min: 19.5509, CLIP Max: 20.4088
   [Sample 0] BERTScore=0.8043, CLIPRaw=19.66 (CLIPNorm=0.1244) -> Reward=0.4643
   [Sample 1] BERTScore=0.8294, CLIPRaw=20.41 (CLIPNorm=1.0000) -> Reward=0.9147
   [Sample 2] BERTScore=0.7883, CLIPRaw=19.81 (CLIPNorm=0.3002) -> Reward=0.5442
   [Sample 3] BERTScore=0.8174, CLIPRaw=19.55 (CLIPNorm=0.0000) -> Reward=0.4087
{'loss': 0.00074844, 'grad_norm': 1.13430798, 'learning_rate': 9.55e-06, 'reward': 1.62021005, 'reward_std': 0.32296914, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.31317198, 'rewards/CustomAccuracyReward/std': 0.07234211, 'rewards/CustomExplainationReward/mean': 0.58299601, 'rewards/CustomExplainationReward/std': 0.22802584, 'completions/mean_length': 173.5, 'completions/min_length': 127.0, 'completions/max_length': 204.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01870801, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '36/200', 'percentage': '18.00%', 'elapsed_time': '43m 37s', 'remaining_time': '3h 18m 41s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013756}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Không'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Không'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.4081, CLIP Max: 22.4348
   [Sample 0] BERTScore=0.8243, CLIPRaw=20.41 (CLIPNorm=0.0000) -> Reward=0.4122
   [Sample 1] BERTScore=0.7854, CLIPRaw=21.40 (CLIPNorm=0.4896) -> Reward=0.6375
   [Sample 2] BERTScore=0.8126, CLIPRaw=21.44 (CLIPNorm=0.5095) -> Reward=0.6611
   [Sample 3] BERTScore=0.7924, CLIPRaw=22.43 (CLIPNorm=1.0000) -> Reward=0.8962
{'loss': 0.00105184, 'grad_norm': 1.29821491, 'learning_rate': 9.51e-06, 'reward': 2.01961398, 'reward_std': 0.82935965, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.56395739, 'rewards/CustomAccuracyReward/std': 0.50349867, 'rewards/CustomExplainationReward/mean': 0.65173382, 'rewards/CustomExplainationReward/std': 0.19786137, 'completions/mean_length': 132.25, 'completions/min_length': 114.0, 'completions/max_length': 150.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0262983, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '37/200', 'percentage': '18.50%', 'elapsed_time': '44m 32s', 'remaining_time': '3h 16m 13s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013844}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.3813, CLIP Max: 20.2019
   [Sample 0] BERTScore=0.7403, CLIPRaw=19.38 (CLIPNorm=0.0000) -> Reward=0.3702
   [Sample 1] BERTScore=0.7464, CLIPRaw=19.82 (CLIPNorm=0.5312) -> Reward=0.6388
   [Sample 2] BERTScore=0.7601, CLIPRaw=20.14 (CLIPNorm=0.9285) -> Reward=0.8443
   [Sample 3] BERTScore=0.7444, CLIPRaw=20.20 (CLIPNorm=1.0000) -> Reward=0.8722
{'loss': 0.00043046, 'grad_norm': 1.31767833, 'learning_rate': 9.47e-06, 'reward': 2.6017065, 'reward_std': 0.29013765, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.68136525, 'rewards/CustomExplainationReward/std': 0.23211005, 'completions/mean_length': 161.5, 'completions/min_length': 132.0, 'completions/max_length': 187.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01077238, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '38/200', 'percentage': '19.00%', 'elapsed_time': '45m 33s', 'remaining_time': '3h 14m 13s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013902}
  [Sample 0] Raw GT: 'frisbee', Raw Pred: 'Bóng bay'
  [Sample 0] Cleaned GT: 'frisbee', Cleaned Pred: 'bóng bay'
  [Sample 1] Raw GT: 'frisbee', Raw Pred: 'Dế yêu đĩa bay'
  [Sample 1] Cleaned GT: 'frisbee', Cleaned Pred: 'dế yêu đĩa bay'
  [Sample 2] Raw GT: 'frisbee', Raw Pred: 'Frisbee'
  [Sample 2] Cleaned GT: 'frisbee', Cleaned Pred: 'frisbee'
  [Sample 3] Raw GT: 'frisbee', Raw Pred: 'cầu lông'
  [Sample 3] Cleaned GT: 'frisbee', Cleaned Pred: 'cầu lông'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2874 -> Reward=0.1437
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2386 -> Reward=0.1193
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2520 -> Reward=0.1260
   [Batch Stats] CLIP Min: 18.0932, CLIP Max: 20.0627
   [Sample 0] BERTScore=0.7621, CLIPRaw=18.09 (CLIPNorm=0.0000) -> Reward=0.3811
   [Sample 1] BERTScore=0.8164, CLIPRaw=19.47 (CLIPNorm=0.7000) -> Reward=0.7582
   [Sample 2] BERTScore=0.7679, CLIPRaw=20.06 (CLIPNorm=1.0000) -> Reward=0.8840
   [Sample 3] BERTScore=0.7605, CLIPRaw=18.14 (CLIPNorm=0.0240) -> Reward=0.3922
{'loss': 0.0006474, 'grad_norm': 0.7941218, 'learning_rate': 9.44e-06, 'reward': 1.68888855, 'reward_std': 0.80518287, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.34724587, 'rewards/CustomAccuracyReward/std': 0.43529108, 'rewards/CustomExplainationReward/mean': 0.60386497, 'rewards/CustomExplainationReward/std': 0.25605398, 'completions/mean_length': 239.0, 'completions/min_length': 187.0, 'completions/max_length': 290.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01618655, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '39/200', 'percentage': '19.50%', 'elapsed_time': '46m 48s', 'remaining_time': '3h 13m 14s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013886}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Đúng'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.0003, CLIP Max: 18.5769
   [Sample 0] BERTScore=0.7063, CLIPRaw=18.58 (CLIPNorm=1.0000) -> Reward=0.8531
   [Sample 1] BERTScore=0.7168, CLIPRaw=18.00 (CLIPNorm=0.0000) -> Reward=0.3584
   [Sample 2] BERTScore=0.7160, CLIPRaw=18.42 (CLIPNorm=0.7269) -> Reward=0.7215
   [Sample 3] BERTScore=0.7133, CLIPRaw=18.04 (CLIPNorm=0.0689) -> Reward=0.3911
{'loss': 0.00133389, 'grad_norm': 0.99077731, 'learning_rate': 9.4e-06, 'reward': 2.20373297, 'reward_std': 0.76991099, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.78197873, 'rewards/CustomAccuracyReward/std': 0.43604261, 'rewards/CustomExplainationReward/mean': 0.58100772, 'rewards/CustomExplainationReward/std': 0.24455495, 'completions/mean_length': 211.0, 'completions/min_length': 169.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03334898, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '40/200', 'percentage': '20.00%', 'elapsed_time': '47m 54s', 'remaining_time': '3h 11m 38s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013914}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 17.0965, CLIP Max: 17.9656
   [Sample 0] BERTScore=0.6705, CLIPRaw=17.24 (CLIPNorm=0.1664) -> Reward=0.4184
   [Sample 1] BERTScore=0.5904, CLIPRaw=17.97 (CLIPNorm=1.0000) -> Reward=0.7952
   [Sample 2] BERTScore=0.6585, CLIPRaw=17.10 (CLIPNorm=0.0000) -> Reward=0.3293
   [Sample 3] BERTScore=0.6232, CLIPRaw=17.60 (CLIPNorm=0.5805) -> Reward=0.6019
{'loss': 0.00067609, 'grad_norm': 1.17548764, 'learning_rate': 9.36e-06, 'reward': 2.42022347, 'reward_std': 0.25827321, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.53617895, 'rewards/CustomExplainationReward/std': 0.20661864, 'completions/mean_length': 168.25, 'completions/min_length': 120.0, 'completions/max_length': 210.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01691405, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '41/200', 'percentage': '20.50%', 'elapsed_time': '49m 15s', 'remaining_time': '3h 10m 59s', 'memory(GiB)': 15.68, 'train_speed(iter/s)': 0.013875}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Đúng'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 17.1440, CLIP Max: 18.9129
   [Sample 0] BERTScore=0.8343, CLIPRaw=17.14 (CLIPNorm=0.0000) -> Reward=0.4172
   [Sample 1] BERTScore=0.8248, CLIPRaw=18.91 (CLIPNorm=1.0000) -> Reward=0.9124
   [Sample 2] BERTScore=0.8355, CLIPRaw=17.40 (CLIPNorm=0.1448) -> Reward=0.4901
   [Sample 3] BERTScore=0.8255, CLIPRaw=18.54 (CLIPNorm=0.7881) -> Reward=0.8068
{'loss': 0.00099584, 'grad_norm': 0.93860024, 'learning_rate': 9.32e-06, 'reward': 2.29824758, 'reward_std': 0.5007633, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.78197873, 'rewards/CustomAccuracyReward/std': 0.43604261, 'rewards/CustomExplainationReward/mean': 0.65661943, 'rewards/CustomExplainationReward/std': 0.2401603, 'completions/mean_length': 192.0, 'completions/min_length': 169.0, 'completions/max_length': 222.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02489774, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '42/200', 'percentage': '21.00%', 'elapsed_time': '50m 39s', 'remaining_time': '3h 10m 33s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01382}
  [Sample 0] Raw GT: 'lướt sóng', Raw Pred: 'Có thể được sử dụng cho bãi cỏ.'
  [Sample 0] Cleaned GT: 'lướt sóng', Cleaned Pred: 'có thể được sử dụng cho bãi cỏ'
  [Sample 1] Raw GT: 'lướt sóng', Raw Pred: 'Biển hiệu'
  [Sample 1] Cleaned GT: 'lướt sóng', Cleaned Pred: 'biển hiệu'
  [Sample 2] Raw GT: 'lướt sóng', Raw Pred: 'Không thể xác định'
  [Sample 2] Cleaned GT: 'lướt sóng', Cleaned Pred: 'không thể xác định'
  [Sample 3] Raw GT: 'lướt sóng', Raw Pred: 'Để giới thiệu'
  [Sample 3] Cleaned GT: 'lướt sóng', Cleaned Pred: 'để giới thiệu'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2404 -> Reward=0.1202
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2991 -> Reward=0.1495
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.3026 -> Reward=0.1513
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.3235 -> Reward=0.1617
   [Batch Stats] CLIP Min: 17.7134, CLIP Max: 19.3134
   [Sample 0] BERTScore=0.7618, CLIPRaw=17.71 (CLIPNorm=0.0000) -> Reward=0.3809
   [Sample 1] BERTScore=0.7512, CLIPRaw=19.00 (CLIPNorm=0.8069) -> Reward=0.7791
   [Sample 2] BERTScore=0.7551, CLIPRaw=19.31 (CLIPNorm=1.0000) -> Reward=0.8775
   [Sample 3] BERTScore=0.7874, CLIPRaw=19.02 (CLIPNorm=0.8191) -> Reward=0.8032
{'loss': 0.00120036, 'grad_norm': 0.919406, 'learning_rate': 9.27e-06, 'reward': 1.56985593, 'reward_std': 0.30015737, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.14569682, 'rewards/CustomAccuracyReward/std': 0.01782499, 'rewards/CustomExplainationReward/mean': 0.71018785, 'rewards/CustomExplainationReward/std': 0.22349291, 'completions/mean_length': 179.75, 'completions/min_length': 150.0, 'completions/max_length': 226.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03000114, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '43/200', 'percentage': '21.50%', 'elapsed_time': '52m 7s', 'remaining_time': '3h 10m 18s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013749}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Đúng'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.5508, CLIP Max: 21.2821
   [Sample 0] BERTScore=0.7029, CLIPRaw=21.28 (CLIPNorm=1.0000) -> Reward=0.8515
   [Sample 1] BERTScore=0.7358, CLIPRaw=20.78 (CLIPNorm=0.3104) -> Reward=0.5231
   [Sample 2] BERTScore=0.6982, CLIPRaw=20.59 (CLIPNorm=0.0574) -> Reward=0.3778
   [Sample 3] BERTScore=0.7349, CLIPRaw=20.55 (CLIPNorm=0.0000) -> Reward=0.3675
{'loss': 0.00062703, 'grad_norm': 1.03608155, 'learning_rate': 9.23e-06, 'reward': 2.41243696, 'reward_std': 0.28226, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.52994943, 'rewards/CustomExplainationReward/std': 0.22580798, 'completions/mean_length': 146.25, 'completions/min_length': 128.0, 'completions/max_length': 185.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01565964, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '44/200', 'percentage': '22.00%', 'elapsed_time': '53m 22s', 'remaining_time': '3h 9m 15s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013738}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 19.4607, CLIP Max: 20.2927
   [Sample 0] BERTScore=0.7886, CLIPRaw=19.86 (CLIPNorm=0.4745) -> Reward=0.6316
   [Sample 1] BERTScore=0.8020, CLIPRaw=19.46 (CLIPNorm=0.0000) -> Reward=0.4010
   [Sample 2] BERTScore=0.8304, CLIPRaw=20.29 (CLIPNorm=1.0000) -> Reward=0.9152
   [Sample 3] BERTScore=0.8140, CLIPRaw=19.76 (CLIPNorm=0.3641) -> Reward=0.5890
{'loss': 0.00070558, 'grad_norm': 0.94355869, 'learning_rate': 9.19e-06, 'reward': 1.45264173, 'reward_std': 0.26552334, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.12791477, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.63419867, 'rewards/CustomExplainationReward/std': 0.21241868, 'completions/mean_length': 182.75, 'completions/min_length': 150.0, 'completions/max_length': 231.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01764853, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '45/200', 'percentage': '22.50%', 'elapsed_time': '54m 32s', 'remaining_time': '3h 7m 52s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01375}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.2730, CLIP Max: 23.1068
   [Sample 0] BERTScore=0.7447, CLIPRaw=20.83 (CLIPNorm=0.1978) -> Reward=0.4712
   [Sample 1] BERTScore=0.7667, CLIPRaw=23.11 (CLIPNorm=1.0000) -> Reward=0.8833
   [Sample 2] BERTScore=0.7762, CLIPRaw=22.57 (CLIPNorm=0.8109) -> Reward=0.7936
   [Sample 3] BERTScore=0.8042, CLIPRaw=20.27 (CLIPNorm=0.0000) -> Reward=0.4021
{'loss': 0.00065832, 'grad_norm': 1.36798251, 'learning_rate': 9.14e-06, 'reward': 2.5469532, 'reward_std': 0.29567441, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.63756263, 'rewards/CustomExplainationReward/std': 0.2365395, 'completions/mean_length': 145.0, 'completions/min_length': 113.0, 'completions/max_length': 168.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01646291, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '46/200', 'percentage': '23.00%', 'elapsed_time': '55m 28s', 'remaining_time': '3h 5m 41s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013822}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.4581, CLIP Max: 21.0613
   [Sample 0] BERTScore=0.8293, CLIPRaw=20.11 (CLIPNorm=0.4042) -> Reward=0.6168
   [Sample 1] BERTScore=0.8159, CLIPRaw=20.86 (CLIPNorm=0.8724) -> Reward=0.8441
   [Sample 2] BERTScore=0.7738, CLIPRaw=21.06 (CLIPNorm=1.0000) -> Reward=0.8869
   [Sample 3] BERTScore=0.7774, CLIPRaw=19.46 (CLIPNorm=0.0000) -> Reward=0.3887
{'loss': 0.00050664, 'grad_norm': 0.99664611, 'learning_rate': 9.09e-06, 'reward': 2.60515308, 'reward_std': 0.28734267, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.68412256, 'rewards/CustomExplainationReward/std': 0.2298741, 'completions/mean_length': 151.25, 'completions/min_length': 112.0, 'completions/max_length': 216.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01267188, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '47/200', 'percentage': '23.50%', 'elapsed_time': '56m 33s', 'remaining_time': '3h 4m 6s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01385}
  [Sample 0] Raw GT: 'bóng chày', Raw Pred: 'Bóng đá'
  [Sample 0] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng đá'
  [Sample 1] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 1] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 2] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 2] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 3] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 3] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 0] ROUGE-L=0.5000, BERTScore=0.5582 -> Reward=0.5291
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 17.0949, CLIP Max: 18.5598
   [Sample 0] BERTScore=0.7635, CLIPRaw=17.09 (CLIPNorm=0.0000) -> Reward=0.3817
   [Sample 1] BERTScore=0.8013, CLIPRaw=18.56 (CLIPNorm=1.0000) -> Reward=0.9007
   [Sample 2] BERTScore=0.8008, CLIPRaw=18.02 (CLIPNorm=0.6296) -> Reward=0.7152
   [Sample 3] BERTScore=0.8184, CLIPRaw=18.29 (CLIPNorm=0.8178) -> Reward=0.8181
{'loss': 0.00043692, 'grad_norm': 0.83181953, 'learning_rate': 9.05e-06, 'reward': 2.48273778, 'reward_std': 0.57075304, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.88226926, 'rewards/CustomAccuracyReward/std': 0.2354615, 'rewards/CustomExplainationReward/mean': 0.70392096, 'rewards/CustomExplainationReward/std': 0.22780076, 'completions/mean_length': 199.0, 'completions/min_length': 156.0, 'completions/max_length': 235.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01092297, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '48/200', 'percentage': '24.00%', 'elapsed_time': '58m 0s', 'remaining_time': '3h 3m 40s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013793}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Không thể xác định'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'không thể xác định'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Không'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2335 -> Reward=0.1168
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 18.7382, CLIP Max: 19.4191
   [Sample 0] BERTScore=0.7407, CLIPRaw=19.42 (CLIPNorm=1.0000) -> Reward=0.8704
   [Sample 1] BERTScore=0.7780, CLIPRaw=19.10 (CLIPNorm=0.5330) -> Reward=0.6555
   [Sample 2] BERTScore=0.7592, CLIPRaw=19.31 (CLIPNorm=0.8410) -> Reward=0.8001
   [Sample 3] BERTScore=0.7362, CLIPRaw=18.74 (CLIPNorm=0.0000) -> Reward=0.3681
{'loss': 0.00071338, 'grad_norm': 0.97782689, 'learning_rate': 9e-06, 'reward': 2.0433445, 'reward_std': 0.79968941, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.56116843, 'rewards/CustomAccuracyReward/std': 0.5067395, 'rewards/CustomExplainationReward/mean': 0.67350709, 'rewards/CustomExplainationReward/std': 0.2223997, 'completions/mean_length': 172.0, 'completions/min_length': 153.0, 'completions/max_length': 208.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01783006, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '49/200', 'percentage': '24.50%', 'elapsed_time': '59m 4s', 'remaining_time': '3h 2m 3s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013824}
  [Sample 0] Raw GT: 'bóng đá', Raw Pred: 'Bóng đá'
  [Sample 0] Cleaned GT: 'bóng đá', Cleaned Pred: 'bóng đá'
  [Sample 1] Raw GT: 'bóng đá', Raw Pred: 'Bóng đá'
  [Sample 1] Cleaned GT: 'bóng đá', Cleaned Pred: 'bóng đá'
  [Sample 2] Raw GT: 'bóng đá', Raw Pred: 'Bóng đá'
  [Sample 2] Cleaned GT: 'bóng đá', Cleaned Pred: 'bóng đá'
  [Sample 3] Raw GT: 'bóng đá', Raw Pred: 'Bóng chuyền'
  [Sample 3] Cleaned GT: 'bóng đá', Cleaned Pred: 'bóng chuyền'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=0.5000, BERTScore=0.6146 -> Reward=0.5573
   [Batch Stats] CLIP Min: 20.2889, CLIP Max: 20.7569
   [Sample 0] BERTScore=0.8430, CLIPRaw=20.45 (CLIPNorm=0.3499) -> Reward=0.5965
   [Sample 1] BERTScore=0.8140, CLIPRaw=20.29 (CLIPNorm=0.0000) -> Reward=0.4070
   [Sample 2] BERTScore=0.8049, CLIPRaw=20.29 (CLIPNorm=0.0000) -> Reward=0.4024
   [Sample 3] BERTScore=0.8278, CLIPRaw=20.76 (CLIPNorm=1.0000) -> Reward=0.9139
{'loss': 0.00055338, 'grad_norm': 0.87182045, 'learning_rate': 8.95e-06, 'reward': 2.3365941, 'reward_std': 0.11301593, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.88932204, 'rewards/CustomAccuracyReward/std': 0.22135589, 'rewards/CustomExplainationReward/mean': 0.57995319, 'rewards/CustomExplainationReward/std': 0.24027757, 'completions/mean_length': 147.0, 'completions/min_length': 136.0, 'completions/max_length': 159.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01382048, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '50/200', 'percentage': '25.00%', 'elapsed_time': '59m 58s', 'remaining_time': '2h 59m 54s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013896}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.1866, CLIP Max: 20.3495
   [Sample 0] BERTScore=0.7396, CLIPRaw=18.19 (CLIPNorm=0.0000) -> Reward=0.3698
   [Sample 1] BERTScore=0.7485, CLIPRaw=20.35 (CLIPNorm=1.0000) -> Reward=0.8743
   [Sample 2] BERTScore=0.7262, CLIPRaw=19.04 (CLIPNorm=0.3964) -> Reward=0.5613
   [Sample 3] BERTScore=0.7313, CLIPRaw=19.50 (CLIPNorm=0.6062) -> Reward=0.6688
{'loss': 0.00119317, 'grad_norm': 0.85055667, 'learning_rate': 8.89e-06, 'reward': 1.70558214, 'reward_std': 0.64187658, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.34593609, 'rewards/CustomAccuracyReward/std': 0.43604264, 'rewards/CustomExplainationReward/mean': 0.61852962, 'rewards/CustomExplainationReward/std': 0.21059994, 'completions/mean_length': 185.25, 'completions/min_length': 128.0, 'completions/max_length': 241.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02982946, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '51/200', 'percentage': '25.50%', 'elapsed_time': '1h 1m 1s', 'remaining_time': '2h 58m 17s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013928}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.8977, CLIP Max: 20.6601
   [Sample 0] BERTScore=0.7907, CLIPRaw=20.04 (CLIPNorm=0.6476) -> Reward=0.7191
   [Sample 1] BERTScore=0.7806, CLIPRaw=18.90 (CLIPNorm=0.0000) -> Reward=0.3903
   [Sample 2] BERTScore=0.7921, CLIPRaw=20.66 (CLIPNorm=1.0000) -> Reward=0.8961
   [Sample 3] BERTScore=0.7918, CLIPRaw=20.17 (CLIPNorm=0.7236) -> Reward=0.7577
{'loss': 0.00094049, 'grad_norm': 1.10598719, 'learning_rate': 8.84e-06, 'reward': 2.61350012, 'reward_std': 0.26782659, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.69080019, 'rewards/CustomExplainationReward/std': 0.21426125, 'completions/mean_length': 140.75, 'completions/min_length': 99.0, 'completions/max_length': 165.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02351838, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '52/200', 'percentage': '26.00%', 'elapsed_time': '1h 1m 54s', 'remaining_time': '2h 56m 13s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013998}
  [Sample 0] Raw GT: 'ngựa vằn', Raw Pred: 'Ngựa vằn'
  [Sample 0] Cleaned GT: 'ngựa vằn', Cleaned Pred: 'ngựa vằn'
  [Sample 1] Raw GT: 'ngựa vằn', Raw Pred: 'Vằn tím'
  [Sample 1] Cleaned GT: 'ngựa vằn', Cleaned Pred: 'vằn tím'
  [Sample 2] Raw GT: 'ngựa vằn', Raw Pred: 'Ngựa vằn'
  [Sample 2] Cleaned GT: 'ngựa vằn', Cleaned Pred: 'ngựa vằn'
  [Sample 3] Raw GT: 'ngựa vằn', Raw Pred: 'Vằn sọc'
  [Sample 3] Cleaned GT: 'ngựa vằn', Cleaned Pred: 'vằn sọc'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.5000, BERTScore=0.5807 -> Reward=0.5404
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=0.5000, BERTScore=0.5220 -> Reward=0.5110
   [Batch Stats] CLIP Min: 20.9966, CLIP Max: 21.7788
   [Sample 0] BERTScore=0.8221, CLIPRaw=21.19 (CLIPNorm=0.2435) -> Reward=0.5328
   [Sample 1] BERTScore=0.8101, CLIPRaw=21.00 (CLIPNorm=0.0000) -> Reward=0.4051
   [Sample 2] BERTScore=0.8277, CLIPRaw=21.78 (CLIPNorm=1.0000) -> Reward=0.9138
   [Sample 3] BERTScore=0.8175, CLIPRaw=21.27 (CLIPNorm=0.3545) -> Reward=0.5860
{'loss': 0.00092316, 'grad_norm': 0.84968334, 'learning_rate': 8.79e-06, 'reward': 2.21531534, 'reward_std': 0.5482108, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.76284146, 'rewards/CustomAccuracyReward/std': 0.27410886, 'rewards/CustomExplainationReward/mean': 0.60941076, 'rewards/CustomExplainationReward/std': 0.21668306, 'completions/mean_length': 186.5, 'completions/min_length': 155.0, 'completions/max_length': 218.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02307873, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '53/200', 'percentage': '26.50%', 'elapsed_time': '1h 3m 21s', 'remaining_time': '2h 55m 43s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013943}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.0164, CLIP Max: 19.3693
   [Sample 0] BERTScore=0.7337, CLIPRaw=19.37 (CLIPNorm=1.0000) -> Reward=0.8669
   [Sample 1] BERTScore=0.7426, CLIPRaw=19.04 (CLIPNorm=0.0795) -> Reward=0.4110
   [Sample 2] BERTScore=0.7277, CLIPRaw=19.02 (CLIPNorm=0.0000) -> Reward=0.3639
   [Sample 3] BERTScore=0.7267, CLIPRaw=19.07 (CLIPNorm=0.1424) -> Reward=0.4346
{'loss': 0.00151029, 'grad_norm': 1.10348272, 'learning_rate': 8.73e-06, 'reward': 1.5812633, 'reward_std': 0.55285561, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.34593609, 'rewards/CustomAccuracyReward/std': 0.43604264, 'rewards/CustomExplainationReward/mean': 0.51907456, 'rewards/CustomExplainationReward/std': 0.23371473, 'completions/mean_length': 124.0, 'completions/min_length': 100.0, 'completions/max_length': 142.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03776099, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '54/200', 'percentage': '27.00%', 'elapsed_time': '1h 4m 5s', 'remaining_time': '2h 53m 17s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014041}
  [Sample 0] Raw GT: 'mới', Raw Pred: 'Mới'
  [Sample 0] Cleaned GT: 'mới', Cleaned Pred: 'mới'
  [Sample 1] Raw GT: 'mới', Raw Pred: 'Đã qua sử dụng'
  [Sample 1] Cleaned GT: 'mới', Cleaned Pred: 'đã qua sử dụng'
  [Sample 2] Raw GT: 'mới', Raw Pred: 'Sử dụng'
  [Sample 2] Cleaned GT: 'mới', Cleaned Pred: 'sử dụng'
  [Sample 3] Raw GT: 'mới', Raw Pred: 'Đã qua sử dụng'
  [Sample 3] Cleaned GT: 'mới', Cleaned Pred: 'đã qua sử dụng'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2810 -> Reward=0.1405
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2886 -> Reward=0.1443
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2810 -> Reward=0.1405
   [Batch Stats] CLIP Min: 20.9062, CLIP Max: 21.1635
   [Sample 0] BERTScore=0.7775, CLIPRaw=21.12 (CLIPNorm=0.8181) -> Reward=0.7978
   [Sample 1] BERTScore=0.7648, CLIPRaw=20.91 (CLIPNorm=0.0000) -> Reward=0.3824
   [Sample 2] BERTScore=0.7527, CLIPRaw=21.16 (CLIPNorm=1.0000) -> Reward=0.8763
   [Sample 3] BERTScore=0.7738, CLIPRaw=21.07 (CLIPNorm=0.6385) -> Reward=0.7062
{'loss': 0.00119845, 'grad_norm': 0.9432286, 'learning_rate': 8.68e-06, 'reward': 1.80874705, 'reward_std': 0.67672443, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.35632268, 'rewards/CustomAccuracyReward/std': 0.429122, 'rewards/CustomExplainationReward/mean': 0.69067502, 'rewards/CustomExplainationReward/std': 0.21697193, 'completions/mean_length': 148.75, 'completions/min_length': 111.0, 'completions/max_length': 168.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02996219, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '55/200', 'percentage': '27.50%', 'elapsed_time': '1h 5m 15s', 'remaining_time': '2h 52m 3s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014045}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.2142, CLIP Max: 19.1642
   [Sample 0] BERTScore=0.7936, CLIPRaw=18.21 (CLIPNorm=0.0000) -> Reward=0.3968
   [Sample 1] BERTScore=0.7422, CLIPRaw=18.82 (CLIPNorm=0.6378) -> Reward=0.6900
   [Sample 2] BERTScore=0.7452, CLIPRaw=19.16 (CLIPNorm=1.0000) -> Reward=0.8726
   [Sample 3] BERTScore=0.7577, CLIPRaw=18.67 (CLIPNorm=0.4826) -> Reward=0.6202
{'loss': 0.00091133, 'grad_norm': 0.87287766, 'learning_rate': 8.62e-06, 'reward': 1.73854041, 'reward_std': 0.57884115, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.34593609, 'rewards/CustomAccuracyReward/std': 0.43604264, 'rewards/CustomExplainationReward/mean': 0.64489621, 'rewards/CustomExplainationReward/std': 0.19668886, 'completions/mean_length': 192.25, 'completions/min_length': 174.0, 'completions/max_length': 236.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02278316, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '56/200', 'percentage': '28.00%', 'elapsed_time': '1h 6m 26s', 'remaining_time': '2h 50m 51s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014046}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 16.4841, CLIP Max: 18.7994
   [Sample 0] BERTScore=0.7733, CLIPRaw=16.48 (CLIPNorm=0.0000) -> Reward=0.3866
   [Sample 1] BERTScore=0.7811, CLIPRaw=18.54 (CLIPNorm=0.8888) -> Reward=0.8349
   [Sample 2] BERTScore=0.7554, CLIPRaw=18.56 (CLIPNorm=0.8969) -> Reward=0.8261
   [Sample 3] BERTScore=0.7658, CLIPRaw=18.80 (CLIPNorm=1.0000) -> Reward=0.8829
{'loss': 0.00087923, 'grad_norm': 0.9267385, 'learning_rate': 8.56e-06, 'reward': 2.66582203, 'reward_std': 0.2900278, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.73265755, 'rewards/CustomExplainationReward/std': 0.23202223, 'completions/mean_length': 158.5, 'completions/min_length': 135.0, 'completions/max_length': 188.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02197523, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '57/200', 'percentage': '28.50%', 'elapsed_time': '1h 7m 37s', 'remaining_time': '2h 49m 39s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014048}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.6330, CLIP Max: 21.7618
   [Sample 0] BERTScore=0.8454, CLIPRaw=20.62 (CLIPNorm=0.4613) -> Reward=0.6533
   [Sample 1] BERTScore=0.7934, CLIPRaw=21.76 (CLIPNorm=1.0000) -> Reward=0.8967
   [Sample 2] BERTScore=0.8088, CLIPRaw=19.63 (CLIPNorm=0.0000) -> Reward=0.4044
   [Sample 3] BERTScore=0.7998, CLIPRaw=20.11 (CLIPNorm=0.2226) -> Reward=0.5112
{'loss': 0.00124939, 'grad_norm': 1.22232544, 'learning_rate': 8.51e-06, 'reward': 2.52051258, 'reward_std': 0.26606828, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.61641002, 'rewards/CustomExplainationReward/std': 0.21285467, 'completions/mean_length': 136.5, 'completions/min_length': 99.0, 'completions/max_length': 177.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03122826, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '58/200', 'percentage': '29.00%', 'elapsed_time': '1h 8m 52s', 'remaining_time': '2h 48m 37s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014035}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.2496, CLIP Max: 19.9188
   [Sample 0] BERTScore=0.7756, CLIPRaw=19.76 (CLIPNorm=0.7636) -> Reward=0.7696
   [Sample 1] BERTScore=0.7239, CLIPRaw=19.25 (CLIPNorm=0.0000) -> Reward=0.3620
   [Sample 2] BERTScore=0.7770, CLIPRaw=19.46 (CLIPNorm=0.3140) -> Reward=0.5455
   [Sample 3] BERTScore=0.7858, CLIPRaw=19.92 (CLIPNorm=1.0000) -> Reward=0.8929
{'loss': 0.00166538, 'grad_norm': 1.1551398, 'learning_rate': 8.45e-06, 'reward': 2.28058743, 'reward_std': 0.51826227, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.78197873, 'rewards/CustomAccuracyReward/std': 0.43604261, 'rewards/CustomExplainationReward/mean': 0.64249134, 'rewards/CustomExplainationReward/std': 0.23591287, 'completions/mean_length': 134.25, 'completions/min_length': 111.0, 'completions/max_length': 182.0, 'completions/clipped_ratio': 0.0, 'kl': 0.04164052, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '59/200', 'percentage': '29.50%', 'elapsed_time': '1h 10m 9s', 'remaining_time': '2h 47m 38s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014017}
  [Sample 0] Raw GT: 'trượt tuyết', Raw Pred: 'Cuộc thi'
  [Sample 0] Cleaned GT: 'trượt tuyết', Cleaned Pred: 'cuộc thi'
  [Sample 1] Raw GT: 'trượt tuyết', Raw Pred: 'Chạy trượt tuyết'
  [Sample 1] Cleaned GT: 'trượt tuyết', Cleaned Pred: 'chạy trượt tuyết'
  [Sample 2] Raw GT: 'trượt tuyết', Raw Pred: 'Cuộc thi đua'
  [Sample 2] Cleaned GT: 'trượt tuyết', Cleaned Pred: 'cuộc thi đua'
  [Sample 3] Raw GT: 'trượt tuyết', Raw Pred: 'Trượt tuyết tốc độ'
  [Sample 3] Cleaned GT: 'trượt tuyết', Cleaned Pred: 'trượt tuyết tốc độ'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.3726 -> Reward=0.1863
  [Sample 1] ROUGE-L=0.8299, BERTScore=0.7603 -> Reward=0.7951
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2946 -> Reward=0.1473
  [Sample 3] ROUGE-L=0.7093, BERTScore=0.4781 -> Reward=0.5937
   [Batch Stats] CLIP Min: 17.1745, CLIP Max: 17.7843
   [Sample 0] BERTScore=0.8483, CLIPRaw=17.30 (CLIPNorm=0.2035) -> Reward=0.5259
   [Sample 1] BERTScore=0.8550, CLIPRaw=17.17 (CLIPNorm=0.0000) -> Reward=0.4275
   [Sample 2] BERTScore=0.8555, CLIPRaw=17.30 (CLIPNorm=0.2122) -> Reward=0.5338
   [Sample 3] BERTScore=0.8401, CLIPRaw=17.78 (CLIPNorm=1.0000) -> Reward=0.9200
{'loss': 0.0011526, 'grad_norm': 0.88424259, 'learning_rate': 8.39e-06, 'reward': 1.7905252, 'reward_std': 0.50712192, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.4305988, 'rewards/CustomAccuracyReward/std': 0.31592101, 'rewards/CustomExplainationReward/mean': 0.60182142, 'rewards/CustomExplainationReward/std': 0.2175938, 'completions/mean_length': 189.5, 'completions/min_length': 179.0, 'completions/max_length': 195.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02881726, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '60/200', 'percentage': '30.00%', 'elapsed_time': '1h 11m 27s', 'remaining_time': '2h 46m 45s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013993}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.5208, CLIP Max: 19.8894
   [Sample 0] BERTScore=0.7941, CLIPRaw=19.89 (CLIPNorm=1.0000) -> Reward=0.8970
   [Sample 1] BERTScore=0.8085, CLIPRaw=18.98 (CLIPNorm=0.3323) -> Reward=0.5704
   [Sample 2] BERTScore=0.8183, CLIPRaw=18.52 (CLIPNorm=0.0000) -> Reward=0.4091
   [Sample 3] BERTScore=0.8615, CLIPRaw=18.80 (CLIPNorm=0.2009) -> Reward=0.5312
{'loss': 0.00079411, 'grad_norm': 0.93611062, 'learning_rate': 8.33e-06, 'reward': 2.50243378, 'reward_std': 0.26045752, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.60194695, 'rewards/CustomExplainationReward/std': 0.20836604, 'completions/mean_length': 178.75, 'completions/min_length': 150.0, 'completions/max_length': 232.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01984733, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '61/200', 'percentage': '30.50%', 'elapsed_time': '1h 12m 39s', 'remaining_time': '2h 45m 33s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013993}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
Error during BERTScore batch computation: The expanded size of the tensor (321) must match the existing size (258) at non-singleton dimension 1.  Target sizes: [4, 321].  Tensor sizes: [1, 258]
   [Batch Stats] CLIP Min: 14.7257, CLIP Max: 16.1945
   [Sample 0] BERTScore=0.0000, CLIPRaw=14.73 (CLIPNorm=0.0000) -> Reward=0.0000
   [Sample 1] BERTScore=0.0000, CLIPRaw=15.33 (CLIPNorm=0.4098) -> Reward=0.2049
   [Sample 2] BERTScore=0.0000, CLIPRaw=14.78 (CLIPNorm=0.0345) -> Reward=0.0172
   [Sample 3] BERTScore=0.0000, CLIPRaw=16.19 (CLIPNorm=1.0000) -> Reward=0.5000
{'loss': 0.00090119, 'grad_norm': 0.90627623, 'learning_rate': 8.26e-06, 'reward': 1.97566795, 'reward_std': 0.29039606, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.18053439, 'rewards/CustomExplainationReward/std': 0.23231679, 'completions/mean_length': 210.5, 'completions/min_length': 162.0, 'completions/max_length': 267.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02252963, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '62/200', 'percentage': '31.00%', 'elapsed_time': '1h 14m 20s', 'remaining_time': '2h 45m 27s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.0139}
  [Sample 0] Raw GT: 'pizza', Raw Pred: 'Pizza'
  [Sample 0] Cleaned GT: 'pizza', Cleaned Pred: 'pizza'
  [Sample 1] Raw GT: 'pizza', Raw Pred: 'Pizza'
  [Sample 1] Cleaned GT: 'pizza', Cleaned Pred: 'pizza'
  [Sample 2] Raw GT: 'pizza', Raw Pred: 'Pizza'
  [Sample 2] Cleaned GT: 'pizza', Cleaned Pred: 'pizza'
  [Sample 3] Raw GT: 'pizza', Raw Pred: 'Pizza'
  [Sample 3] Cleaned GT: 'pizza', Cleaned Pred: 'pizza'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.2260, CLIP Max: 19.0205
   [Sample 0] BERTScore=0.7895, CLIPRaw=18.33 (CLIPNorm=0.1285) -> Reward=0.4590
   [Sample 1] BERTScore=0.8312, CLIPRaw=18.23 (CLIPNorm=0.0000) -> Reward=0.4156
   [Sample 2] BERTScore=0.7996, CLIPRaw=18.92 (CLIPNorm=0.8740) -> Reward=0.8368
   [Sample 3] BERTScore=0.7707, CLIPRaw=19.02 (CLIPNorm=1.0000) -> Reward=0.8854
{'loss': 0.00097878, 'grad_norm': 1.14797187, 'learning_rate': 8.2e-06, 'reward': 2.56149459, 'reward_std': 0.30765203, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.64919567, 'rewards/CustomExplainationReward/std': 0.24612166, 'completions/mean_length': 148.0, 'completions/min_length': 116.0, 'completions/max_length': 176.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02446449, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '63/200', 'percentage': '31.50%', 'elapsed_time': '1h 15m 19s', 'remaining_time': '2h 43m 47s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013941}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 15.2110, CLIP Max: 15.4671
   [Sample 0] BERTScore=0.7831, CLIPRaw=15.47 (CLIPNorm=1.0000) -> Reward=0.8916
   [Sample 1] BERTScore=0.7585, CLIPRaw=15.28 (CLIPNorm=0.2798) -> Reward=0.5191
   [Sample 2] BERTScore=0.7527, CLIPRaw=15.27 (CLIPNorm=0.2119) -> Reward=0.4823
   [Sample 3] BERTScore=0.7763, CLIPRaw=15.21 (CLIPNorm=0.0000) -> Reward=0.3881
{'loss': 0.00079289, 'grad_norm': 0.94983631, 'learning_rate': 8.14e-06, 'reward': 2.46286821, 'reward_std': 0.27646381, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.57029462, 'rewards/CustomExplainationReward/std': 0.2211712, 'completions/mean_length': 145.0, 'completions/min_length': 125.0, 'completions/max_length': 156.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01982837, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '64/200', 'percentage': '32.00%', 'elapsed_time': '1h 16m 28s', 'remaining_time': '2h 42m 30s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013948}
  [Sample 0] Raw GT: 'con mèo', Raw Pred: 'Mèo'
  [Sample 0] Cleaned GT: 'con mèo', Cleaned Pred: 'mèo'
  [Sample 1] Raw GT: 'con mèo', Raw Pred: 'Mèo'
  [Sample 1] Cleaned GT: 'con mèo', Cleaned Pred: 'mèo'
  [Sample 2] Raw GT: 'con mèo', Raw Pred: 'Mèo'
  [Sample 2] Cleaned GT: 'con mèo', Cleaned Pred: 'mèo'
  [Sample 3] Raw GT: 'con mèo', Raw Pred: 'Mèo'
  [Sample 3] Cleaned GT: 'con mèo', Cleaned Pred: 'mèo'
  [Sample 0] ROUGE-L=0.6289, BERTScore=0.5905 -> Reward=0.6097
  [Sample 1] ROUGE-L=0.6289, BERTScore=0.5905 -> Reward=0.6097
  [Sample 2] ROUGE-L=0.6289, BERTScore=0.5905 -> Reward=0.6097
  [Sample 3] ROUGE-L=0.6289, BERTScore=0.5905 -> Reward=0.6097
   [Batch Stats] CLIP Min: 19.6270, CLIP Max: 20.2754
   [Sample 0] BERTScore=0.7751, CLIPRaw=19.99 (CLIPNorm=0.5565) -> Reward=0.6658
   [Sample 1] BERTScore=0.7886, CLIPRaw=19.63 (CLIPNorm=0.0000) -> Reward=0.3943
   [Sample 2] BERTScore=0.7990, CLIPRaw=20.28 (CLIPNorm=1.0000) -> Reward=0.8995
   [Sample 3] BERTScore=0.7710, CLIPRaw=20.09 (CLIPNorm=0.7123) -> Reward=0.7417
{'loss': 0.00088304, 'grad_norm': 0.90676856, 'learning_rate': 8.07e-06, 'reward': 2.10625219, 'reward_std': 0.26390147, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.60968989, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.6753118, 'rewards/CustomExplainationReward/std': 0.21112117, 'completions/mean_length': 184.75, 'completions/min_length': 159.0, 'completions/max_length': 196.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02207099, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '65/200', 'percentage': '32.50%', 'elapsed_time': '1h 17m 46s', 'remaining_time': '2h 41m 31s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01393}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 16.5960, CLIP Max: 18.8430
   [Sample 0] BERTScore=0.8054, CLIPRaw=17.31 (CLIPNorm=0.3195) -> Reward=0.5624
   [Sample 1] BERTScore=0.8093, CLIPRaw=17.76 (CLIPNorm=0.5166) -> Reward=0.6629
   [Sample 2] BERTScore=0.8163, CLIPRaw=18.84 (CLIPNorm=1.0000) -> Reward=0.9082
   [Sample 3] BERTScore=0.8021, CLIPRaw=16.60 (CLIPNorm=0.0000) -> Reward=0.4011
{'loss': 0.00090677, 'grad_norm': 0.87543106, 'learning_rate': 8.01e-06, 'reward': 2.54206228, 'reward_std': 0.26553157, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.63364983, 'rewards/CustomExplainationReward/std': 0.21242535, 'completions/mean_length': 151.75, 'completions/min_length': 137.0, 'completions/max_length': 184.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02266896, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '66/200', 'percentage': '33.00%', 'elapsed_time': '1h 19m 1s', 'remaining_time': '2h 40m 26s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01392}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.3127, CLIP Max: 18.5732
   [Sample 0] BERTScore=0.8575, CLIPRaw=18.57 (CLIPNorm=0.9810) -> Reward=0.9193
   [Sample 1] BERTScore=0.8203, CLIPRaw=18.31 (CLIPNorm=0.0000) -> Reward=0.4102
   [Sample 2] BERTScore=0.8344, CLIPRaw=18.40 (CLIPNorm=0.3433) -> Reward=0.5888
   [Sample 3] BERTScore=0.8318, CLIPRaw=18.57 (CLIPNorm=1.0000) -> Reward=0.9159
{'loss': 0.00142851, 'grad_norm': 0.95951843, 'learning_rate': 7.94e-06, 'reward': 2.63567352, 'reward_std': 0.31519514, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.70853865, 'rewards/CustomExplainationReward/std': 0.25215617, 'completions/mean_length': 151.25, 'completions/min_length': 131.0, 'completions/max_length': 169.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03569854, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '67/200', 'percentage': '33.50%', 'elapsed_time': '1h 20m 12s', 'remaining_time': '2h 39m 13s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013922}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 17.7476, CLIP Max: 18.6241
   [Sample 0] BERTScore=0.7377, CLIPRaw=17.75 (CLIPNorm=0.0000) -> Reward=0.3689
   [Sample 1] BERTScore=0.7727, CLIPRaw=18.62 (CLIPNorm=1.0000) -> Reward=0.8863
   [Sample 2] BERTScore=0.7524, CLIPRaw=18.47 (CLIPNorm=0.8238) -> Reward=0.7881
   [Sample 3] BERTScore=0.7531, CLIPRaw=18.17 (CLIPNorm=0.4871) -> Reward=0.6201
{'loss': 0.00092381, 'grad_norm': 0.96181679, 'learning_rate': 7.87e-06, 'reward': 2.58229613, 'reward_std': 0.28307065, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.66583693, 'rewards/CustomExplainationReward/std': 0.22645654, 'completions/mean_length': 181.0, 'completions/min_length': 137.0, 'completions/max_length': 208.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02308967, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '68/200', 'percentage': '34.00%', 'elapsed_time': '1h 21m 18s', 'remaining_time': '2h 37m 50s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013938}
  [Sample 0] Raw GT: 'hươu cao cổ', Raw Pred: 'Ngựa hươu'
  [Sample 0] Cleaned GT: 'hươu cao cổ', Cleaned Pred: 'ngựa hươu'
  [Sample 1] Raw GT: 'hươu cao cổ', Raw Pred: 'Ngựa hươu cao cổ'
  [Sample 1] Cleaned GT: 'hươu cao cổ', Cleaned Pred: 'ngựa hươu cao cổ'
  [Sample 2] Raw GT: 'hươu cao cổ', Raw Pred: 'Hươu cao cổ'
  [Sample 2] Cleaned GT: 'hươu cao cổ', Cleaned Pred: 'hươu cao cổ'
  [Sample 3] Raw GT: 'hươu cao cổ', Raw Pred: 'Gấu chó'
  [Sample 3] Cleaned GT: 'hươu cao cổ', Cleaned Pred: 'gấu chó'
  [Sample 0] ROUGE-L=0.3861, BERTScore=0.5382 -> Reward=0.4622
  [Sample 1] ROUGE-L=0.8798, BERTScore=0.8351 -> Reward=0.8575
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.4992 -> Reward=0.2496
   [Batch Stats] CLIP Min: 20.0059, CLIP Max: 20.7932
   [Sample 0] BERTScore=0.8049, CLIPRaw=20.34 (CLIPNorm=0.4269) -> Reward=0.6159
   [Sample 1] BERTScore=0.7677, CLIPRaw=20.14 (CLIPNorm=0.1699) -> Reward=0.4688
   [Sample 2] BERTScore=0.7843, CLIPRaw=20.79 (CLIPNorm=1.0000) -> Reward=0.8922
   [Sample 3] BERTScore=0.7825, CLIPRaw=20.01 (CLIPNorm=0.0000) -> Reward=0.3912
{'loss': 0.00132355, 'grad_norm': 0.9795174, 'learning_rate': 7.8e-06, 'reward': 2.04292154, 'reward_std': 0.65264785, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.6423068, 'rewards/CustomAccuracyReward/std': 0.34684035, 'rewards/CustomExplainationReward/mean': 0.59203047, 'rewards/CustomExplainationReward/std': 0.22070971, 'completions/mean_length': 171.0, 'completions/min_length': 149.0, 'completions/max_length': 225.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03309191, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '69/200', 'percentage': '34.50%', 'elapsed_time': '1h 22m 41s', 'remaining_time': '2h 37m 0s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013906}
  [Sample 0] Raw GT: 'orioles', Raw Pred: 'Orioles'
  [Sample 0] Cleaned GT: 'orioles', Cleaned Pred: 'orioles'
  [Sample 1] Raw GT: 'orioles', Raw Pred: 'Baltimore Orioles'
  [Sample 1] Cleaned GT: 'orioles', Cleaned Pred: 'baltimore orioles'
  [Sample 2] Raw GT: 'orioles', Raw Pred: 'Boston Red Sox'
  [Sample 2] Cleaned GT: 'orioles', Cleaned Pred: 'boston red sox'
  [Sample 3] Raw GT: 'orioles', Raw Pred: 'Orioles'
  [Sample 3] Cleaned GT: 'orioles', Cleaned Pred: 'orioles'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.7093, BERTScore=0.7111 -> Reward=0.7102
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2977 -> Reward=0.1488
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 14.5484, CLIP Max: 15.6586
   [Sample 0] BERTScore=0.8151, CLIPRaw=14.55 (CLIPNorm=0.0000) -> Reward=0.4075
   [Sample 1] BERTScore=0.8191, CLIPRaw=15.66 (CLIPNorm=1.0000) -> Reward=0.9095
   [Sample 2] BERTScore=0.8056, CLIPRaw=15.60 (CLIPNorm=0.9462) -> Reward=0.8759
   [Sample 3] BERTScore=0.8253, CLIPRaw=15.52 (CLIPNorm=0.8730) -> Reward=0.8492
{'loss': 0.00099218, 'grad_norm': 1.03097832, 'learning_rate': 7.73e-06, 'reward': 2.34410954, 'reward_std': 0.4379425, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.71475661, 'rewards/CustomAccuracyReward/std': 0.40125543, 'rewards/CustomExplainationReward/mean': 0.76053107, 'rewards/CustomExplainationReward/std': 0.23662271, 'completions/mean_length': 122.25, 'completions/min_length': 104.0, 'completions/max_length': 146.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02481414, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '70/200', 'percentage': '35.00%', 'elapsed_time': '1h 23m 28s', 'remaining_time': '2h 35m 1s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013977}
  [Sample 0] Raw GT: 'gạch', Raw Pred: 'Sàn nhà tắm'
  [Sample 0] Cleaned GT: 'gạch', Cleaned Pred: 'sàn nhà tắm'
  [Sample 1] Raw GT: 'gạch', Raw Pred: 'Sàn là gạch men'
  [Sample 1] Cleaned GT: 'gạch', Cleaned Pred: 'sàn là gạch men'
  [Sample 2] Raw GT: 'gạch', Raw Pred: 'Gạch'
  [Sample 2] Cleaned GT: 'gạch', Cleaned Pred: 'gạch'
  [Sample 3] Raw GT: 'gạch', Raw Pred: 'Sàn lát gạch'
  [Sample 3] Cleaned GT: 'gạch', Cleaned Pred: 'sàn lát gạch'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.5358 -> Reward=0.2679
  [Sample 1] ROUGE-L=0.4485, BERTScore=1.0000 -> Reward=0.7243
  [Sample 2] ROUGE-L=1.0000, BERTScore=0.4195 -> Reward=0.7097
  [Sample 3] ROUGE-L=0.5495, BERTScore=0.4511 -> Reward=0.5003
   [Batch Stats] CLIP Min: 22.0442, CLIP Max: 22.9355
   [Sample 0] BERTScore=0.7879, CLIPRaw=22.16 (CLIPNorm=0.1321) -> Reward=0.4600
   [Sample 1] BERTScore=0.8085, CLIPRaw=22.73 (CLIPNorm=0.7666) -> Reward=0.7875
   [Sample 2] BERTScore=0.7923, CLIPRaw=22.04 (CLIPNorm=0.0000) -> Reward=0.3961
   [Sample 3] BERTScore=0.8063, CLIPRaw=22.94 (CLIPNorm=1.0000) -> Reward=0.9031
{'loss': 0.00149672, 'grad_norm': 1.21997464, 'learning_rate': 7.67e-06, 'reward': 1.98409617, 'reward_std': 0.43881094, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.55056417, 'rewards/CustomAccuracyReward/std': 0.21442507, 'rewards/CustomExplainationReward/mean': 0.63671279, 'rewards/CustomExplainationReward/std': 0.24686703, 'completions/mean_length': 121.75, 'completions/min_length': 99.0, 'completions/max_length': 145.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03741809, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '71/200', 'percentage': '35.50%', 'elapsed_time': '1h 24m 34s', 'remaining_time': '2h 33m 40s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01399}
  [Sample 0] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 0] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 1] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 1] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 2] Raw GT: 'bóng chày', Raw Pred: 'Bóng đá'
  [Sample 2] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng đá'
  [Sample 3] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 3] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=0.5000, BERTScore=0.5582 -> Reward=0.5291
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.0079, CLIP Max: 18.6920
   [Sample 0] BERTScore=0.7879, CLIPRaw=18.37 (CLIPNorm=0.5362) -> Reward=0.6621
   [Sample 1] BERTScore=0.7620, CLIPRaw=18.69 (CLIPNorm=1.0000) -> Reward=0.8810
   [Sample 2] BERTScore=0.7499, CLIPRaw=18.16 (CLIPNorm=0.2167) -> Reward=0.4833
   [Sample 3] BERTScore=0.7766, CLIPRaw=18.01 (CLIPNorm=0.0000) -> Reward=0.3883
{'loss': 0.00084976, 'grad_norm': 1.09544361, 'learning_rate': 7.59e-06, 'reward': 2.35741782, 'reward_std': 0.46819642, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.88226926, 'rewards/CustomAccuracyReward/std': 0.2354615, 'rewards/CustomExplainationReward/mean': 0.60366493, 'rewards/CustomExplainationReward/std': 0.21694143, 'completions/mean_length': 161.75, 'completions/min_length': 126.0, 'completions/max_length': 200.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02123998, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '72/200', 'percentage': '36.00%', 'elapsed_time': '1h 25m 55s', 'remaining_time': '2h 32m 45s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013966}
  [Sample 0] Raw GT: 'có', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'có', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'có', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'có', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 19.7697, CLIP Max: 20.9973
   [Sample 0] BERTScore=0.7575, CLIPRaw=20.17 (CLIPNorm=0.3266) -> Reward=0.5420
   [Sample 1] BERTScore=0.7578, CLIPRaw=21.00 (CLIPNorm=1.0000) -> Reward=0.8789
   [Sample 2] BERTScore=0.7916, CLIPRaw=20.14 (CLIPNorm=0.3020) -> Reward=0.5468
   [Sample 3] BERTScore=0.7506, CLIPRaw=19.77 (CLIPNorm=0.0000) -> Reward=0.3753
{'loss': 0.00123884, 'grad_norm': 0.96016824, 'learning_rate': 7.52e-06, 'reward': 1.39209723, 'reward_std': 0.26386014, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.12791477, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.58576304, 'rewards/CustomExplainationReward/std': 0.21108812, 'completions/mean_length': 153.75, 'completions/min_length': 138.0, 'completions/max_length': 174.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03097072, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '73/200', 'percentage': '36.50%', 'elapsed_time': '1h 26m 52s', 'remaining_time': '2h 31m 8s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014005}
  [Sample 0] Raw GT: 'phòng tắm', Raw Pred: 'Phòng vệ sinh'
  [Sample 0] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng vệ sinh'
  [Sample 1] Raw GT: 'phòng tắm', Raw Pred: 'Nhà vệ sinh'
  [Sample 1] Cleaned GT: 'phòng tắm', Cleaned Pred: 'nhà vệ sinh'
  [Sample 2] Raw GT: 'phòng tắm', Raw Pred: 'Phòng tắm'
  [Sample 2] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng tắm'
  [Sample 3] Raw GT: 'phòng tắm', Raw Pred: 'Nhà vệ sinh'
  [Sample 3] Cleaned GT: 'phòng tắm', Cleaned Pred: 'nhà vệ sinh'
  [Sample 0] ROUGE-L=0.4150, BERTScore=0.3888 -> Reward=0.4019
  [Sample 1] ROUGE-L=0.0000, BERTScore=1.0000 -> Reward=0.5000
  [Sample 2] ROUGE-L=1.0000, BERTScore=0.5730 -> Reward=0.7865
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.3888 -> Reward=0.1944
   [Batch Stats] CLIP Min: 18.5026, CLIP Max: 21.0281
   [Sample 0] BERTScore=0.7427, CLIPRaw=18.50 (CLIPNorm=0.0000) -> Reward=0.3714
   [Sample 1] BERTScore=0.7670, CLIPRaw=21.03 (CLIPNorm=1.0000) -> Reward=0.8835
   [Sample 2] BERTScore=0.7545, CLIPRaw=19.50 (CLIPNorm=0.3947) -> Reward=0.5746
   [Sample 3] BERTScore=0.7553, CLIPRaw=20.23 (CLIPNorm=0.6824) -> Reward=0.7188
{'loss': 0.00118543, 'grad_norm': 0.82207626, 'learning_rate': 7.45e-06, 'reward': 1.88473558, 'reward_std': 0.38861677, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.47070664, 'rewards/CustomAccuracyReward/std': 0.24608044, 'rewards/CustomExplainationReward/mean': 0.63708186, 'rewards/CustomExplainationReward/std': 0.21750255, 'completions/mean_length': 159.25, 'completions/min_length': 123.0, 'completions/max_length': 186.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0296358, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '74/200', 'percentage': '37.00%', 'elapsed_time': '1h 28m 7s', 'remaining_time': '2h 30m 3s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.013995}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.9499, CLIP Max: 21.4409
   [Sample 0] BERTScore=0.7826, CLIPRaw=20.95 (CLIPNorm=0.0000) -> Reward=0.3913
   [Sample 1] BERTScore=0.7894, CLIPRaw=21.44 (CLIPNorm=1.0000) -> Reward=0.8947
   [Sample 2] BERTScore=0.8313, CLIPRaw=21.34 (CLIPNorm=0.7981) -> Reward=0.8147
   [Sample 3] BERTScore=0.8513, CLIPRaw=21.03 (CLIPNorm=0.1639) -> Reward=0.5076
{'loss': 0.00139186, 'grad_norm': 1.10418367, 'learning_rate': 7.38e-06, 'reward': 2.56510544, 'reward_std': 0.30120584, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.65208435, 'rewards/CustomExplainationReward/std': 0.2409647, 'completions/mean_length': 151.25, 'completions/min_length': 125.0, 'completions/max_length': 189.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03479678, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '75/200', 'percentage': '37.50%', 'elapsed_time': '1h 28m 59s', 'remaining_time': '2h 28m 18s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014047}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 17.2671, CLIP Max: 18.9585
   [Sample 0] BERTScore=0.8226, CLIPRaw=17.63 (CLIPNorm=0.2160) -> Reward=0.5193
   [Sample 1] BERTScore=0.8158, CLIPRaw=18.96 (CLIPNorm=1.0000) -> Reward=0.9079
   [Sample 2] BERTScore=0.8503, CLIPRaw=17.27 (CLIPNorm=0.0000) -> Reward=0.4251
   [Sample 3] BERTScore=0.8393, CLIPRaw=17.96 (CLIPNorm=0.4123) -> Reward=0.6258
{'loss': 0.00190642, 'grad_norm': 1.10215819, 'learning_rate': 7.31e-06, 'reward': 2.25188446, 'reward_std': 0.59970146, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.78197873, 'rewards/CustomAccuracyReward/std': 0.43604261, 'rewards/CustomExplainationReward/mean': 0.61952889, 'rewards/CustomExplainationReward/std': 0.20900355, 'completions/mean_length': 117.25, 'completions/min_length': 104.0, 'completions/max_length': 142.0, 'completions/clipped_ratio': 0.0, 'kl': 0.04765772, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '76/200', 'percentage': '38.00%', 'elapsed_time': '1h 29m 47s', 'remaining_time': '2h 26m 30s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014107}
  [Sample 0] Raw GT: 'gấu', Raw Pred: 'Con chó'
  [Sample 0] Cleaned GT: 'gấu', Cleaned Pred: 'con chó'
  [Sample 1] Raw GT: 'gấu', Raw Pred: 'Công chúa'
  [Sample 1] Cleaned GT: 'gấu', Cleaned Pred: 'công chúa'
  [Sample 2] Raw GT: 'gấu', Raw Pred: 'Cá súc'
  [Sample 2] Cleaned GT: 'gấu', Cleaned Pred: 'cá súc'
  [Sample 3] Raw GT: 'gấu', Raw Pred: 'Chó da đen'
  [Sample 3] Cleaned GT: 'gấu', Cleaned Pred: 'chó da đen'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.3370 -> Reward=0.1685
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2788 -> Reward=0.1394
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.4133 -> Reward=0.2066
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.3577 -> Reward=0.1789
   [Batch Stats] CLIP Min: 19.8807, CLIP Max: 21.0544
   [Sample 0] BERTScore=0.8294, CLIPRaw=19.88 (CLIPNorm=0.0000) -> Reward=0.4147
   [Sample 1] BERTScore=0.7789, CLIPRaw=20.78 (CLIPNorm=0.7704) -> Reward=0.7746
   [Sample 2] BERTScore=0.7427, CLIPRaw=21.05 (CLIPNorm=1.0000) -> Reward=0.8714
   [Sample 3] BERTScore=0.7754, CLIPRaw=20.68 (CLIPNorm=0.6849) -> Reward=0.7301
{'loss': 0.00143332, 'grad_norm': 1.05602694, 'learning_rate': 7.23e-06, 'reward': 1.58880329, 'reward_std': 0.25918129, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.17334713, 'rewards/CustomAccuracyReward/std': 0.02778359, 'rewards/CustomExplainationReward/mean': 0.69769555, 'rewards/CustomExplainationReward/std': 0.19766669, 'completions/mean_length': 149.0, 'completions/min_length': 136.0, 'completions/max_length': 169.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03583554, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '77/200', 'percentage': '38.50%', 'elapsed_time': '1h 30m 41s', 'remaining_time': '2h 24m 53s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014149}
  [Sample 0] Raw GT: 'thả diều', Raw Pred: 'Xả lân'
  [Sample 0] Cleaned GT: 'thả diều', Cleaned Pred: 'xả lân'
  [Sample 1] Raw GT: 'thả diều', Raw Pred: 'Thả diều'
  [Sample 1] Cleaned GT: 'thả diều', Cleaned Pred: 'thả diều'
  [Sample 2] Raw GT: 'thả diều', Raw Pred: 'Thả diều'
  [Sample 2] Cleaned GT: 'thả diều', Cleaned Pred: 'thả diều'
  [Sample 3] Raw GT: 'thả diều', Raw Pred: 'Chơi diều'
  [Sample 3] Cleaned GT: 'thả diều', Cleaned Pred: 'chơi diều'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.5560 -> Reward=0.2780
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=0.5000, BERTScore=0.6877 -> Reward=0.5938
   [Batch Stats] CLIP Min: 20.8327, CLIP Max: 22.8712
   [Sample 0] BERTScore=0.8151, CLIPRaw=22.01 (CLIPNorm=0.5783) -> Reward=0.6967
   [Sample 1] BERTScore=0.7698, CLIPRaw=20.83 (CLIPNorm=0.0000) -> Reward=0.3849
   [Sample 2] BERTScore=0.8277, CLIPRaw=22.07 (CLIPNorm=0.6045) -> Reward=0.7161
   [Sample 3] BERTScore=0.7959, CLIPRaw=22.87 (CLIPNorm=1.0000) -> Reward=0.8980
{'loss': 0.00204837, 'grad_norm': 1.13614571, 'learning_rate': 7.16e-06, 'reward': 2.23984218, 'reward_std': 0.38808054, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.71795928, 'rewards/CustomAccuracyReward/std': 0.35026991, 'rewards/CustomExplainationReward/mean': 0.67391455, 'rewards/CustomExplainationReward/std': 0.21293838, 'completions/mean_length': 143.0, 'completions/min_length': 134.0, 'completions/max_length': 150.0, 'completions/clipped_ratio': 0.0, 'kl': 0.05121704, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '78/200', 'percentage': '39.00%', 'elapsed_time': '1h 31m 31s', 'remaining_time': '2h 23m 9s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014203}
  [Sample 0] Raw GT: 'bồn rửa', Raw Pred: 'Định danh vật thể'
  [Sample 0] Cleaned GT: 'bồn rửa', Cleaned Pred: 'định danh vật thể'
  [Sample 1] Raw GT: 'bồn rửa', Raw Pred: 'Máy làm lạnh'
  [Sample 1] Cleaned GT: 'bồn rửa', Cleaned Pred: 'máy làm lạnh'
  [Sample 2] Raw GT: 'bồn rửa', Raw Pred: 'Thiết bị vệ sinh gia dụng'
  [Sample 2] Cleaned GT: 'bồn rửa', Cleaned Pred: 'thiết bị vệ sinh gia dụng'
  [Sample 3] Raw GT: 'bồn rửa', Raw Pred: 'Máy xả nước'
  [Sample 3] Cleaned GT: 'bồn rửa', Cleaned Pred: 'máy xả nước'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.4852 -> Reward=0.2426
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2373 -> Reward=0.1186
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.4073 -> Reward=0.2036
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2903 -> Reward=0.1452
   [Batch Stats] CLIP Min: 20.7901, CLIP Max: 21.4005
   [Sample 0] BERTScore=0.7828, CLIPRaw=21.40 (CLIPNorm=1.0000) -> Reward=0.8914
   [Sample 1] BERTScore=0.7591, CLIPRaw=21.01 (CLIPNorm=0.3627) -> Reward=0.5609
   [Sample 2] BERTScore=0.7773, CLIPRaw=20.79 (CLIPNorm=0.0000) -> Reward=0.3887
   [Sample 3] BERTScore=0.7807, CLIPRaw=20.88 (CLIPNorm=0.1423) -> Reward=0.4615
{'loss': 0.00139114, 'grad_norm': 0.91670758, 'learning_rate': 7.08e-06, 'reward': 1.44137681, 'reward_std': 0.32096049, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.17750472, 'rewards/CustomAccuracyReward/std': 0.05606935, 'rewards/CustomExplainationReward/mean': 0.57559675, 'rewards/CustomExplainationReward/std': 0.22204213, 'completions/mean_length': 163.5, 'completions/min_length': 153.0, 'completions/max_length': 170.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03477662, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '79/200', 'percentage': '39.50%', 'elapsed_time': '1h 32m 25s', 'remaining_time': '2h 21m 33s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014246}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 14.0898, CLIP Max: 15.1497
   [Sample 0] BERTScore=0.8219, CLIPRaw=14.93 (CLIPNorm=0.7927) -> Reward=0.8073
   [Sample 1] BERTScore=0.8746, CLIPRaw=14.19 (CLIPNorm=0.0938) -> Reward=0.4842
   [Sample 2] BERTScore=0.8217, CLIPRaw=15.15 (CLIPNorm=1.0000) -> Reward=0.9108
   [Sample 3] BERTScore=0.8577, CLIPRaw=14.09 (CLIPNorm=0.0000) -> Reward=0.4288
{'loss': 0.0010315, 'grad_norm': 1.04394794, 'learning_rate': 7.01e-06, 'reward': 2.57224441, 'reward_std': 0.29663804, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.65779543, 'rewards/CustomExplainationReward/std': 0.23731048, 'completions/mean_length': 134.0, 'completions/min_length': 115.0, 'completions/max_length': 165.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0257774, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '80/200', 'percentage': '40.00%', 'elapsed_time': '1h 33m 34s', 'remaining_time': '2h 20m 22s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014248}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.1766, CLIP Max: 21.2818
   [Sample 0] BERTScore=0.7833, CLIPRaw=21.28 (CLIPNorm=1.0000) -> Reward=0.8916
   [Sample 1] BERTScore=0.8089, CLIPRaw=20.18 (CLIPNorm=0.0000) -> Reward=0.4045
   [Sample 2] BERTScore=0.8025, CLIPRaw=21.13 (CLIPNorm=0.8624) -> Reward=0.8324
   [Sample 3] BERTScore=0.8342, CLIPRaw=20.82 (CLIPNorm=0.5853) -> Reward=0.7097
{'loss': 0.0017148, 'grad_norm': 0.89834821, 'learning_rate': 6.93e-06, 'reward': 2.63696265, 'reward_std': 0.27131552, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.70957005, 'rewards/CustomExplainationReward/std': 0.21705246, 'completions/mean_length': 132.5, 'completions/min_length': 124.0, 'completions/max_length': 146.0, 'completions/clipped_ratio': 0.0, 'kl': 0.04286476, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '81/200', 'percentage': '40.50%', 'elapsed_time': '1h 34m 38s', 'remaining_time': '2h 19m 2s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014265}
  [Sample 0] Raw GT: 'hươu cao cổ', Raw Pred: 'Hươu cao cổ'
  [Sample 0] Cleaned GT: 'hươu cao cổ', Cleaned Pred: 'hươu cao cổ'
  [Sample 1] Raw GT: 'hươu cao cổ', Raw Pred: 'Hươu cao cổ'
  [Sample 1] Cleaned GT: 'hươu cao cổ', Cleaned Pred: 'hươu cao cổ'
  [Sample 2] Raw GT: 'hươu cao cổ', Raw Pred: 'Ngựa vằn'
  [Sample 2] Cleaned GT: 'hươu cao cổ', Cleaned Pred: 'ngựa vằn'
  [Sample 3] Raw GT: 'hươu cao cổ', Raw Pred: 'Hươu cao cổ'
  [Sample 3] Cleaned GT: 'hươu cao cổ', Cleaned Pred: 'hươu cao cổ'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=0.4228 -> Reward=0.7114
  [Sample 2] ROUGE-L=0.0000, BERTScore=1.0000 -> Reward=0.5000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.6657, CLIP Max: 21.1131
   [Sample 0] BERTScore=0.6931, CLIPRaw=21.11 (CLIPNorm=1.0000) -> Reward=0.8466
   [Sample 1] BERTScore=0.6439, CLIPRaw=20.94 (CLIPNorm=0.6154) -> Reward=0.6297
   [Sample 2] BERTScore=0.7068, CLIPRaw=20.67 (CLIPNorm=0.0000) -> Reward=0.3534
   [Sample 3] BERTScore=0.7619, CLIPRaw=20.76 (CLIPNorm=0.2070) -> Reward=0.4844
{'loss': 0.0011601, 'grad_norm': 1.00726676, 'learning_rate': 6.86e-06, 'reward': 2.22669077, 'reward_std': 0.5140841, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.80284882, 'rewards/CustomAccuracyReward/std': 0.24346001, 'rewards/CustomExplainationReward/mean': 0.57850391, 'rewards/CustomExplainationReward/std': 0.21134964, 'completions/mean_length': 161.0, 'completions/min_length': 147.0, 'completions/max_length': 173.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02901069, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '82/200', 'percentage': '41.00%', 'elapsed_time': '1h 35m 47s', 'remaining_time': '2h 17m 51s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014266}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 16.3486, CLIP Max: 18.3557
   [Sample 0] BERTScore=0.7326, CLIPRaw=16.35 (CLIPNorm=0.0000) -> Reward=0.3663
   [Sample 1] BERTScore=0.7354, CLIPRaw=17.98 (CLIPNorm=0.8145) -> Reward=0.7749
   [Sample 2] BERTScore=0.7505, CLIPRaw=18.19 (CLIPNorm=0.9184) -> Reward=0.8345
   [Sample 3] BERTScore=0.7319, CLIPRaw=18.36 (CLIPNorm=1.0000) -> Reward=0.8659
{'loss': 0.00084449, 'grad_norm': 1.09802878, 'learning_rate': 6.78e-06, 'reward': 2.6380105, 'reward_std': 0.29061037, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.71040827, 'rewards/CustomExplainationReward/std': 0.23248835, 'completions/mean_length': 157.25, 'completions/min_length': 142.0, 'completions/max_length': 175.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02110199, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '83/200', 'percentage': '41.50%', 'elapsed_time': '1h 36m 58s', 'remaining_time': '2h 16m 42s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014265}
  [Sample 0] Raw GT: 'con mèo', Raw Pred: 'Mèo'
  [Sample 0] Cleaned GT: 'con mèo', Cleaned Pred: 'mèo'
  [Sample 1] Raw GT: 'con mèo', Raw Pred: 'Mèo Bengal'
  [Sample 1] Cleaned GT: 'con mèo', Cleaned Pred: 'mèo bengal'
  [Sample 2] Raw GT: 'con mèo', Raw Pred: 'Mèo'
  [Sample 2] Cleaned GT: 'con mèo', Cleaned Pred: 'mèo'
  [Sample 3] Raw GT: 'con mèo', Raw Pred: 'Mèo'
  [Sample 3] Cleaned GT: 'con mèo', Cleaned Pred: 'mèo'
  [Sample 0] ROUGE-L=0.6289, BERTScore=0.5905 -> Reward=0.6097
  [Sample 1] ROUGE-L=0.5000, BERTScore=0.5905 -> Reward=0.5453
  [Sample 2] ROUGE-L=0.6289, BERTScore=0.4341 -> Reward=0.5315
  [Sample 3] ROUGE-L=0.6289, BERTScore=0.5905 -> Reward=0.6097
   [Batch Stats] CLIP Min: 19.1447, CLIP Max: 19.8589
   [Sample 0] BERTScore=0.7220, CLIPRaw=19.68 (CLIPNorm=0.7451) -> Reward=0.7336
   [Sample 1] BERTScore=0.7656, CLIPRaw=19.39 (CLIPNorm=0.3373) -> Reward=0.5514
   [Sample 2] BERTScore=0.7254, CLIPRaw=19.86 (CLIPNorm=1.0000) -> Reward=0.8627
   [Sample 3] BERTScore=0.7294, CLIPRaw=19.14 (CLIPNorm=0.0000) -> Reward=0.3647
{'loss': 0.00157496, 'grad_norm': 1.07947016, 'learning_rate': 6.7e-06, 'reward': 2.00267959, 'reward_std': 0.24978903, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.57403576, 'rewards/CustomAccuracyReward/std': 0.04155074, 'rewards/CustomExplainationReward/mean': 0.62810796, 'rewards/CustomExplainationReward/std': 0.21711919, 'completions/mean_length': 152.75, 'completions/min_length': 128.0, 'completions/max_length': 182.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03937764, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '84/200', 'percentage': '42.00%', 'elapsed_time': '1h 38m 10s', 'remaining_time': '2h 15m 34s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01426}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'không', Raw Pred: 'TÍN'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'tín'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2391 -> Reward=0.1196
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 18.7206, CLIP Max: 19.8615
   [Sample 0] BERTScore=0.7890, CLIPRaw=19.86 (CLIPNorm=1.0000) -> Reward=0.8945
   [Sample 1] BERTScore=0.8300, CLIPRaw=19.10 (CLIPNorm=0.3312) -> Reward=0.5806
   [Sample 2] BERTScore=0.8196, CLIPRaw=18.72 (CLIPNorm=0.0000) -> Reward=0.4098
   [Sample 3] BERTScore=0.8307, CLIPRaw=18.93 (CLIPNorm=0.1840) -> Reward=0.5074
{'loss': 0.00093558, 'grad_norm': 1.37617218, 'learning_rate': 6.62e-06, 'reward': 1.40487647, 'reward_std': 0.26520064, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.12582496, 'rewards/CustomAccuracyReward/std': 0.00417965, 'rewards/CustomExplainationReward/mean': 0.59807622, 'rewards/CustomExplainationReward/std': 0.20963183, 'completions/mean_length': 125.5, 'completions/min_length': 112.0, 'completions/max_length': 143.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02338475, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '85/200', 'percentage': '42.50%', 'elapsed_time': '1h 38m 58s', 'remaining_time': '2h 13m 54s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014313}
  [Sample 0] Raw GT: 'có', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Không'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Không'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 18.8587, CLIP Max: 20.0520
   [Sample 0] BERTScore=0.7577, CLIPRaw=20.05 (CLIPNorm=1.0000) -> Reward=0.8788
   [Sample 1] BERTScore=0.7555, CLIPRaw=20.03 (CLIPNorm=0.9796) -> Reward=0.8675
   [Sample 2] BERTScore=0.7471, CLIPRaw=19.98 (CLIPNorm=0.9364) -> Reward=0.8417
   [Sample 3] BERTScore=0.7194, CLIPRaw=18.86 (CLIPNorm=0.0000) -> Reward=0.3597
{'loss': 0.00145638, 'grad_norm': 1.14630139, 'learning_rate': 6.55e-06, 'reward': 1.85361385, 'reward_std': 0.7010482, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.34593609, 'rewards/CustomAccuracyReward/std': 0.43604264, 'rewards/CustomExplainationReward/mean': 0.73695493, 'rewards/CustomExplainationReward/std': 0.25196901, 'completions/mean_length': 137.75, 'completions/min_length': 90.0, 'completions/max_length': 173.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03640858, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '86/200', 'percentage': '43.00%', 'elapsed_time': '1h 39m 53s', 'remaining_time': '2h 12m 24s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01435}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 17.4813, CLIP Max: 18.2877
   [Sample 0] BERTScore=0.8378, CLIPRaw=18.29 (CLIPNorm=1.0000) -> Reward=0.9189
   [Sample 1] BERTScore=0.8317, CLIPRaw=17.51 (CLIPNorm=0.0327) -> Reward=0.4322
   [Sample 2] BERTScore=0.8273, CLIPRaw=17.48 (CLIPNorm=0.0000) -> Reward=0.4136
   [Sample 3] BERTScore=0.8274, CLIPRaw=17.87 (CLIPNorm=0.4867) -> Reward=0.6571
{'loss': 0.00086716, 'grad_norm': 0.99868011, 'learning_rate': 6.47e-06, 'reward': 2.50681496, 'reward_std': 0.29554784, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.60545188, 'rewards/CustomExplainationReward/std': 0.23643821, 'completions/mean_length': 126.5, 'completions/min_length': 98.0, 'completions/max_length': 148.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02167452, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '87/200', 'percentage': '43.50%', 'elapsed_time': '1h 40m 57s', 'remaining_time': '2h 11m 7s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014362}
  [Sample 0] Raw GT: 'lướt sóng', Raw Pred: 'Trượt ván'
  [Sample 0] Cleaned GT: 'lướt sóng', Cleaned Pred: 'trượt ván'
  [Sample 1] Raw GT: 'lướt sóng', Raw Pred: 'Đang trượt ván'
  [Sample 1] Cleaned GT: 'lướt sóng', Cleaned Pred: 'đang trượt ván'
  [Sample 2] Raw GT: 'lướt sóng', Raw Pred: 'Wakeboarding'
  [Sample 2] Cleaned GT: 'lướt sóng', Cleaned Pred: 'wakeboarding'
  [Sample 3] Raw GT: 'lướt sóng', Raw Pred: 'Trượt ván'
  [Sample 3] Cleaned GT: 'lướt sóng', Cleaned Pred: 'trượt ván'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.5818 -> Reward=0.2909
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2456 -> Reward=0.1228
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.5818 -> Reward=0.2909
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.3768 -> Reward=0.1884
   [Batch Stats] CLIP Min: 17.9848, CLIP Max: 20.1660
   [Sample 0] BERTScore=0.8131, CLIPRaw=19.29 (CLIPNorm=0.5979) -> Reward=0.7055
   [Sample 1] BERTScore=0.8391, CLIPRaw=19.45 (CLIPNorm=0.6695) -> Reward=0.7543
   [Sample 2] BERTScore=0.7596, CLIPRaw=20.17 (CLIPNorm=1.0000) -> Reward=0.8798
   [Sample 3] BERTScore=0.8266, CLIPRaw=17.98 (CLIPNorm=0.0000) -> Reward=0.4133
{'loss': 0.00122708, 'grad_norm': 1.45503604, 'learning_rate': 6.39e-06, 'reward': 1.63933563, 'reward_std': 0.29889435, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.22325161, 'rewards/CustomAccuracyReward/std': 0.08257276, 'rewards/CustomExplainationReward/mean': 0.68821687, 'rewards/CustomExplainationReward/std': 0.19743212, 'completions/mean_length': 162.0, 'completions/min_length': 120.0, 'completions/max_length': 203.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03067261, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '88/200', 'percentage': '44.00%', 'elapsed_time': '1h 42m 0s', 'remaining_time': '2h 9m 49s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014379}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Nhận xét'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'nhận xét'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2691 -> Reward=0.1346
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.1839, CLIP Max: 22.8923
   [Sample 0] BERTScore=0.7068, CLIPRaw=21.52 (CLIPNorm=0.4924) -> Reward=0.5996
   [Sample 1] BERTScore=0.6803, CLIPRaw=22.89 (CLIPNorm=1.0000) -> Reward=0.8402
   [Sample 2] BERTScore=0.7048, CLIPRaw=21.76 (CLIPNorm=0.5811) -> Reward=0.6429
   [Sample 3] BERTScore=0.7049, CLIPRaw=20.18 (CLIPNorm=0.0000) -> Reward=0.3524
{'loss': 0.00140113, 'grad_norm': 1.08261585, 'learning_rate': 6.31e-06, 'reward': 1.96799397, 'reward_std': 0.46355626, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.56562197, 'rewards/CustomAccuracyReward/std': 0.50158387, 'rewards/CustomExplainationReward/mean': 0.60877323, 'rewards/CustomExplainationReward/std': 0.20041956, 'completions/mean_length': 158.5, 'completions/min_length': 132.0, 'completions/max_length': 177.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03503293, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '89/200', 'percentage': '44.50%', 'elapsed_time': '1h 42m 46s', 'remaining_time': '2h 8m 11s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014432}
  [Sample 0] Raw GT: 'lễ cưới', Raw Pred: 'Sự kiện đang được tổ chức là một tiệc cưới hoặc một sự kiện ăn uống quan trọng.'
  [Sample 0] Cleaned GT: 'lễ cưới', Cleaned Pred: 'sự kiện đang được tổ chức là một tiệc cưới hoặc một sự kiện ăn uống quan trọng'
  [Sample 1] Raw GT: 'lễ cưới', Raw Pred: 'Chỉ có thể ăn được bánh kem.'
  [Sample 1] Cleaned GT: 'lễ cưới', Cleaned Pred: 'chỉ có thể ăn được bánh kem'
  [Sample 2] Raw GT: 'lễ cưới', Raw Pred: 'Bữa tiệc'
  [Sample 2] Cleaned GT: 'lễ cưới', Cleaned Pred: 'bữa tiệc'
  [Sample 3] Raw GT: 'lễ cưới', Raw Pred: 'Tiệc cưới'
  [Sample 3] Cleaned GT: 'lễ cưới', Cleaned Pred: 'tiệc cưới'
  [Sample 0] ROUGE-L=0.1169, BERTScore=0.2836 -> Reward=0.2002
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.4261 -> Reward=0.2130
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.3810 -> Reward=0.1905
  [Sample 3] ROUGE-L=0.5000, BERTScore=0.5559 -> Reward=0.5279
   [Batch Stats] CLIP Min: 21.3602, CLIP Max: 21.7566
   [Sample 0] BERTScore=0.7842, CLIPRaw=21.37 (CLIPNorm=0.0323) -> Reward=0.4082
   [Sample 1] BERTScore=0.7272, CLIPRaw=21.76 (CLIPNorm=1.0000) -> Reward=0.8636
   [Sample 2] BERTScore=0.7362, CLIPRaw=21.36 (CLIPNorm=0.0000) -> Reward=0.3681
   [Sample 3] BERTScore=0.7386, CLIPRaw=21.63 (CLIPNorm=0.6761) -> Reward=0.7073
{'loss': 0.00131381, 'grad_norm': 0.97395647, 'learning_rate': 6.23e-06, 'reward': 1.58717799, 'reward_std': 0.42174408, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.28291407, 'rewards/CustomAccuracyReward/std': 0.16360173, 'rewards/CustomExplainationReward/mean': 0.58682823, 'rewards/CustomExplainationReward/std': 0.23865685, 'completions/mean_length': 162.75, 'completions/min_length': 141.0, 'completions/max_length': 175.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03284181, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '90/200', 'percentage': '45.00%', 'elapsed_time': '1h 43m 57s', 'remaining_time': '2h 7m 3s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014428}
  [Sample 0] Raw GT: 'phòng tắm', Raw Pred: 'Phòng tắm'
  [Sample 0] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng tắm'
  [Sample 1] Raw GT: 'phòng tắm', Raw Pred: 'Phòng tắm'
  [Sample 1] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng tắm'
  [Sample 2] Raw GT: 'phòng tắm', Raw Pred: 'Vệ sinh'
  [Sample 2] Cleaned GT: 'phòng tắm', Cleaned Pred: 'vệ sinh'
  [Sample 3] Raw GT: 'phòng tắm', Raw Pred: 'Phòng tắm'
  [Sample 3] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng tắm'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.3524 -> Reward=0.1762
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.7286, CLIP Max: 20.7159
   [Sample 0] BERTScore=0.7552, CLIPRaw=20.39 (CLIPNorm=0.6738) -> Reward=0.7145
   [Sample 1] BERTScore=0.7353, CLIPRaw=20.53 (CLIPNorm=0.8167) -> Reward=0.7760
   [Sample 2] BERTScore=0.7756, CLIPRaw=19.73 (CLIPNorm=0.0000) -> Reward=0.3878
   [Sample 3] BERTScore=0.7605, CLIPRaw=20.72 (CLIPNorm=1.0000) -> Reward=0.8803
{'loss': 0.00098097, 'grad_norm': 0.95154119, 'learning_rate': 6.15e-06, 'reward': 2.35462403, 'reward_std': 0.77116537, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.79405439, 'rewards/CustomAccuracyReward/std': 0.41189128, 'rewards/CustomExplainationReward/mean': 0.68964493, 'rewards/CustomExplainationReward/std': 0.21254615, 'completions/mean_length': 154.0, 'completions/min_length': 132.0, 'completions/max_length': 192.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02452269, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '91/200', 'percentage': '45.50%', 'elapsed_time': '1h 45m 13s', 'remaining_time': '2h 6m 2s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014412}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.0004, CLIP Max: 21.8917
   [Sample 0] BERTScore=0.7036, CLIPRaw=21.89 (CLIPNorm=1.0000) -> Reward=0.8518
   [Sample 1] BERTScore=0.7100, CLIPRaw=20.00 (CLIPNorm=0.0000) -> Reward=0.3550
   [Sample 2] BERTScore=0.7422, CLIPRaw=21.25 (CLIPNorm=0.6620) -> Reward=0.7021
   [Sample 3] BERTScore=0.7407, CLIPRaw=20.78 (CLIPNorm=0.4130) -> Reward=0.5768
{'loss': 0.00153677, 'grad_norm': 1.21902716, 'learning_rate': 6.07e-06, 'reward': 1.98173499, 'reward_std': 0.70564008, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.56395739, 'rewards/CustomAccuracyReward/std': 0.50349867, 'rewards/CustomExplainationReward/mean': 0.62143052, 'rewards/CustomExplainationReward/std': 0.21021011, 'completions/mean_length': 124.5, 'completions/min_length': 104.0, 'completions/max_length': 145.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03841688, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '92/200', 'percentage': '46.00%', 'elapsed_time': '1h 46m 17s', 'remaining_time': '2h 4m 47s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014425}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.8967, CLIP Max: 21.6384
   [Sample 0] BERTScore=0.7955, CLIPRaw=20.90 (CLIPNorm=0.0000) -> Reward=0.3978
   [Sample 1] BERTScore=0.7772, CLIPRaw=21.18 (CLIPNorm=0.3824) -> Reward=0.5798
   [Sample 2] BERTScore=0.7783, CLIPRaw=21.64 (CLIPNorm=1.0000) -> Reward=0.8892
   [Sample 3] BERTScore=0.8148, CLIPRaw=21.06 (CLIPNorm=0.2268) -> Reward=0.5208
{'loss': 0.00139783, 'grad_norm': 0.99743533, 'learning_rate': 5.99e-06, 'reward': 1.95106065, 'reward_std': 0.46305928, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.56395739, 'rewards/CustomAccuracyReward/std': 0.50349867, 'rewards/CustomExplainationReward/mean': 0.59689116, 'rewards/CustomExplainationReward/std': 0.20908289, 'completions/mean_length': 159.75, 'completions/min_length': 141.0, 'completions/max_length': 194.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03494668, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '93/200', 'percentage': '46.50%', 'elapsed_time': '1h 47m 33s', 'remaining_time': '2h 3m 44s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014411}
  [Sample 0] Raw GT: 'quần vợt', Raw Pred: 'Tennis'
  [Sample 0] Cleaned GT: 'quần vợt', Cleaned Pred: 'tennis'
  [Sample 1] Raw GT: 'quần vợt', Raw Pred: 'Tennis'
  [Sample 1] Cleaned GT: 'quần vợt', Cleaned Pred: 'tennis'
  [Sample 2] Raw GT: 'quần vợt', Raw Pred: 'Tennis'
  [Sample 2] Cleaned GT: 'quần vợt', Cleaned Pred: 'tennis'
  [Sample 3] Raw GT: 'quần vợt', Raw Pred: 'Tennis'
  [Sample 3] Cleaned GT: 'quần vợt', Cleaned Pred: 'tennis'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.4712 -> Reward=0.2356
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.4712 -> Reward=0.2356
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.4712 -> Reward=0.2356
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.4712 -> Reward=0.2356
   [Batch Stats] CLIP Min: 17.8373, CLIP Max: 18.8406
   [Sample 0] BERTScore=0.8277, CLIPRaw=17.84 (CLIPNorm=0.0000) -> Reward=0.4139
   [Sample 1] BERTScore=0.8290, CLIPRaw=18.47 (CLIPNorm=0.6351) -> Reward=0.7320
   [Sample 2] BERTScore=0.8319, CLIPRaw=18.84 (CLIPNorm=1.0000) -> Reward=0.9159
   [Sample 3] BERTScore=0.7884, CLIPRaw=18.16 (CLIPNorm=0.3232) -> Reward=0.5558
{'loss': 0.00086317, 'grad_norm': 1.40371406, 'learning_rate': 5.9e-06, 'reward': 1.61254382, 'reward_std': 0.27196273, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.23562318, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.65441179, 'rewards/CustomExplainationReward/std': 0.21757017, 'completions/mean_length': 105.25, 'completions/min_length': 93.0, 'completions/max_length': 115.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02157199, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '94/200', 'percentage': '47.00%', 'elapsed_time': '1h 48m 8s', 'remaining_time': '2h 1m 57s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014487}
  [Sample 0] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 0] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 1] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 1] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 2] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 2] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 3] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 3] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.4969, CLIP Max: 19.0703
   [Sample 0] BERTScore=0.7937, CLIPRaw=19.07 (CLIPNorm=1.0000) -> Reward=0.8968
   [Sample 1] BERTScore=0.8250, CLIPRaw=18.50 (CLIPNorm=0.0000) -> Reward=0.4125
   [Sample 2] BERTScore=0.8172, CLIPRaw=19.03 (CLIPNorm=0.9382) -> Reward=0.8777
   [Sample 3] BERTScore=0.8400, CLIPRaw=18.83 (CLIPNorm=0.5784) -> Reward=0.7092
{'loss': 0.00085926, 'grad_norm': 0.91926676, 'learning_rate': 5.82e-06, 'reward': 2.65508413, 'reward_std': 0.2802259, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.72406727, 'rewards/CustomExplainationReward/std': 0.22418068, 'completions/mean_length': 174.75, 'completions/min_length': 146.0, 'completions/max_length': 216.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02147691, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '95/200', 'percentage': '47.50%', 'elapsed_time': '1h 49m 34s', 'remaining_time': '2h 1m 6s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01445}
  [Sample 0] Raw GT: 'bikini', Raw Pred: 'Bộ đồ bơi hai mảnh'
  [Sample 0] Cleaned GT: 'bikini', Cleaned Pred: 'bộ đồ bơi hai mảnh'
  [Sample 1] Raw GT: 'bikini', Raw Pred: 'Bộ bikini'
  [Sample 1] Cleaned GT: 'bikini', Cleaned Pred: 'bộ bikini'
  [Sample 2] Raw GT: 'bikini', Raw Pred: 'Đồ bơi'
  [Sample 2] Cleaned GT: 'bikini', Cleaned Pred: 'đồ bơi'
  [Sample 3] Raw GT: 'bikini', Raw Pred: 'Bộ đồ bơi'
  [Sample 3] Cleaned GT: 'bikini', Cleaned Pred: 'bộ đồ bơi'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.3975 -> Reward=0.1987
  [Sample 1] ROUGE-L=0.7093, BERTScore=0.3465 -> Reward=0.5279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.3564 -> Reward=0.1782
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.5118 -> Reward=0.2559
   [Batch Stats] CLIP Min: 18.9471, CLIP Max: 19.1268
   [Sample 0] BERTScore=0.7317, CLIPRaw=18.95 (CLIPNorm=0.0000) -> Reward=0.3659
   [Sample 1] BERTScore=0.7461, CLIPRaw=19.13 (CLIPNorm=1.0000) -> Reward=0.8731
   [Sample 2] BERTScore=0.7390, CLIPRaw=18.95 (CLIPNorm=0.0403) -> Reward=0.3896
   [Sample 3] BERTScore=0.7278, CLIPRaw=19.02 (CLIPNorm=0.4186) -> Reward=0.5732
{'loss': 0.00125464, 'grad_norm': 0.87717801, 'learning_rate': 5.74e-06, 'reward': 1.55079758, 'reward_std': 0.49196526, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.29019156, 'rewards/CustomAccuracyReward/std': 0.16183895, 'rewards/CustomExplainationReward/mean': 0.55044651, 'rewards/CustomExplainationReward/std': 0.23419063, 'completions/mean_length': 161.5, 'completions/min_length': 158.0, 'completions/max_length': 164.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03136558, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '96/200', 'percentage': '48.00%', 'elapsed_time': '1h 50m 38s', 'remaining_time': '1h 59m 52s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01446}
  [Sample 0] Raw GT: 'phòng bếp', Raw Pred: 'Căn phòng là một phòng bếp.'
  [Sample 0] Cleaned GT: 'phòng bếp', Cleaned Pred: 'căn phòng là một phòng bếp'
  [Sample 1] Raw GT: 'phòng bếp', Raw Pred: 'Căn bếp'
  [Sample 1] Cleaned GT: 'phòng bếp', Cleaned Pred: 'căn bếp'
  [Sample 2] Raw GT: 'phòng bếp', Raw Pred: 'Phòng bếp'
  [Sample 2] Cleaned GT: 'phòng bếp', Cleaned Pred: 'phòng bếp'
  [Sample 3] Raw GT: 'phòng bếp', Raw Pred: 'Căn phòng này là một phòng bếp.'
  [Sample 3] Cleaned GT: 'phòng bếp', Cleaned Pred: 'căn phòng này là một phòng bếp'
  [Sample 0] ROUGE-L=0.5495, BERTScore=1.0000 -> Reward=0.7748
  [Sample 1] ROUGE-L=0.5000, BERTScore=0.4964 -> Reward=0.4982
  [Sample 2] ROUGE-L=1.0000, BERTScore=0.8172 -> Reward=0.9086
  [Sample 3] ROUGE-L=0.4939, BERTScore=0.4866 -> Reward=0.4903
   [Batch Stats] CLIP Min: 20.4183, CLIP Max: 21.5592
   [Sample 0] BERTScore=0.7543, CLIPRaw=20.42 (CLIPNorm=0.0000) -> Reward=0.3772
   [Sample 1] BERTScore=0.7440, CLIPRaw=21.43 (CLIPNorm=0.8854) -> Reward=0.8147
   [Sample 2] BERTScore=0.7634, CLIPRaw=21.56 (CLIPNorm=1.0000) -> Reward=0.8817
   [Sample 3] BERTScore=0.7425, CLIPRaw=21.14 (CLIPNorm=0.6294) -> Reward=0.6860
{'loss': 0.00137794, 'grad_norm': 1.09076703, 'learning_rate': 5.66e-06, 'reward': 2.19732666, 'reward_std': 0.37111557, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.66796321, 'rewards/CustomAccuracyReward/std': 0.20794009, 'rewards/CustomExplainationReward/mean': 0.68989801, 'rewards/CustomExplainationReward/std': 0.2237512, 'completions/mean_length': 180.75, 'completions/min_length': 151.0, 'completions/max_length': 227.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03444023, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '97/200', 'percentage': '48.50%', 'elapsed_time': '1h 51m 42s', 'remaining_time': '1h 58m 37s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014472}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.6132, CLIP Max: 20.9202
   [Sample 0] BERTScore=0.7495, CLIPRaw=20.61 (CLIPNorm=0.0000) -> Reward=0.3747
   [Sample 1] BERTScore=0.7602, CLIPRaw=20.75 (CLIPNorm=0.4367) -> Reward=0.5985
   [Sample 2] BERTScore=0.7259, CLIPRaw=20.76 (CLIPNorm=0.4920) -> Reward=0.6089
   [Sample 3] BERTScore=0.7671, CLIPRaw=20.92 (CLIPNorm=1.0000) -> Reward=0.8835
{'loss': 0.00094199, 'grad_norm': 0.93180031, 'learning_rate': 5.58e-06, 'reward': 2.52052736, 'reward_std': 0.26035231, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.61642182, 'rewards/CustomExplainationReward/std': 0.2082818, 'completions/mean_length': 165.75, 'completions/min_length': 145.0, 'completions/max_length': 203.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02353764, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '98/200', 'percentage': '49.00%', 'elapsed_time': '1h 53m 1s', 'remaining_time': '1h 57m 38s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014451}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 17.5134, CLIP Max: 18.4471
   [Sample 0] BERTScore=0.7537, CLIPRaw=17.51 (CLIPNorm=0.0000) -> Reward=0.3769
   [Sample 1] BERTScore=0.7550, CLIPRaw=17.98 (CLIPNorm=0.5024) -> Reward=0.6287
   [Sample 2] BERTScore=0.8011, CLIPRaw=18.45 (CLIPNorm=1.0000) -> Reward=0.9005
   [Sample 3] BERTScore=0.7671, CLIPRaw=17.58 (CLIPNorm=0.0743) -> Reward=0.4207
{'loss': 0.00138636, 'grad_norm': 0.95565295, 'learning_rate': 5.5e-06, 'reward': 2.47711945, 'reward_std': 0.29909936, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.5816955, 'rewards/CustomExplainationReward/std': 0.2392796, 'completions/mean_length': 136.5, 'completions/min_length': 122.0, 'completions/max_length': 149.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03465373, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '99/200', 'percentage': '49.50%', 'elapsed_time': '1h 53m 52s', 'remaining_time': '1h 56m 10s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014489}
  [Sample 0] Raw GT: 'váy', Raw Pred: 'Váy ngắn'
  [Sample 0] Cleaned GT: 'váy', Cleaned Pred: 'váy ngắn'
  [Sample 1] Raw GT: 'váy', Raw Pred: 'Váy'
  [Sample 1] Cleaned GT: 'váy', Cleaned Pred: 'váy'
  [Sample 2] Raw GT: 'váy', Raw Pred: 'Cô ấy mặc váy.'
  [Sample 2] Cleaned GT: 'váy', Cleaned Pred: 'cô ấy mặc váy'
  [Sample 3] Raw GT: 'váy', Raw Pred: 'Váy ngắn'
  [Sample 3] Cleaned GT: 'váy', Cleaned Pred: 'váy ngắn'
  [Sample 0] ROUGE-L=0.7093, BERTScore=0.5029 -> Reward=0.6061
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=0.4485, BERTScore=0.3630 -> Reward=0.4058
  [Sample 3] ROUGE-L=0.7093, BERTScore=0.5029 -> Reward=0.6061
   [Batch Stats] CLIP Min: 19.4707, CLIP Max: 20.0030
   [Sample 0] BERTScore=0.8189, CLIPRaw=19.47 (CLIPNorm=0.0000) -> Reward=0.4095
   [Sample 1] BERTScore=0.8240, CLIPRaw=20.00 (CLIPNorm=1.0000) -> Reward=0.9120
   [Sample 2] BERTScore=0.8109, CLIPRaw=19.54 (CLIPNorm=0.1303) -> Reward=0.4706
   [Sample 3] BERTScore=0.8197, CLIPRaw=19.67 (CLIPNorm=0.3668) -> Reward=0.5933
{'loss': 0.00332761, 'grad_norm': 0.8948316, 'learning_rate': 5.41e-06, 'reward': 2.06353307, 'reward_std': 0.57526982, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.65448987, 'rewards/CustomAccuracyReward/std': 0.24895048, 'rewards/CustomExplainationReward/mean': 0.59633654, 'rewards/CustomExplainationReward/std': 0.2238991, 'completions/mean_length': 170.5, 'completions/min_length': 146.0, 'completions/max_length': 205.0, 'completions/clipped_ratio': 0.0, 'kl': 0.08319041, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '100/200', 'percentage': '50.00%', 'elapsed_time': '1h 55m 14s', 'remaining_time': '1h 55m 14s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014463}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.3087, CLIP Max: 21.1829
   [Sample 0] BERTScore=0.8313, CLIPRaw=21.18 (CLIPNorm=1.0000) -> Reward=0.9157
   [Sample 1] BERTScore=0.8181, CLIPRaw=20.31 (CLIPNorm=0.0000) -> Reward=0.4090
   [Sample 2] BERTScore=0.8154, CLIPRaw=20.67 (CLIPNorm=0.4155) -> Reward=0.6155
   [Sample 3] BERTScore=0.8225, CLIPRaw=20.42 (CLIPNorm=0.1224) -> Reward=0.4725
{'loss': 0.0013395, 'grad_norm': 1.00874555, 'learning_rate': 5.33e-06, 'reward': 2.5039463, 'reward_std': 0.28188741, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.60315692, 'rewards/CustomExplainationReward/std': 0.22550988, 'completions/mean_length': 146.75, 'completions/min_length': 117.0, 'completions/max_length': 165.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03347651, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '101/200', 'percentage': '50.50%', 'elapsed_time': '1h 56m 3s', 'remaining_time': '1h 53m 45s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014505}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 16.7147, CLIP Max: 17.8696
   [Sample 0] BERTScore=0.7505, CLIPRaw=17.87 (CLIPNorm=1.0000) -> Reward=0.8752
   [Sample 1] BERTScore=0.7470, CLIPRaw=17.05 (CLIPNorm=0.2925) -> Reward=0.5198
   [Sample 2] BERTScore=0.7423, CLIPRaw=16.71 (CLIPNorm=0.0000) -> Reward=0.3711
   [Sample 3] BERTScore=0.7575, CLIPRaw=17.67 (CLIPNorm=0.8244) -> Reward=0.7909
{'loss': 0.00097318, 'grad_norm': 0.96098858, 'learning_rate': 5.25e-06, 'reward': 2.54908752, 'reward_std': 0.29303479, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.63926995, 'rewards/CustomExplainationReward/std': 0.23442787, 'completions/mean_length': 177.75, 'completions/min_length': 166.0, 'completions/max_length': 190.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02431942, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '102/200', 'percentage': '51.00%', 'elapsed_time': '1h 57m 13s', 'remaining_time': '1h 52m 37s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014501}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 17.6558, CLIP Max: 18.3755
   [Sample 0] BERTScore=0.8205, CLIPRaw=17.98 (CLIPNorm=0.4559) -> Reward=0.6382
   [Sample 1] BERTScore=0.8331, CLIPRaw=17.76 (CLIPNorm=0.1450) -> Reward=0.4891
   [Sample 2] BERTScore=0.8202, CLIPRaw=17.66 (CLIPNorm=0.0000) -> Reward=0.4101
   [Sample 3] BERTScore=0.8099, CLIPRaw=18.38 (CLIPNorm=1.0000) -> Reward=0.9050
{'loss': 0.00171816, 'grad_norm': 1.10069561, 'learning_rate': 5.17e-06, 'reward': 2.51322317, 'reward_std': 0.27233112, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.61057854, 'rewards/CustomExplainationReward/std': 0.21786501, 'completions/mean_length': 132.25, 'completions/min_length': 111.0, 'completions/max_length': 155.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0429541, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '103/200', 'percentage': '51.50%', 'elapsed_time': '1h 58m 7s', 'remaining_time': '1h 51m 14s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014533}
  [Sample 0] Raw GT: 'mèo', Raw Pred: 'Mèo'
  [Sample 0] Cleaned GT: 'mèo', Cleaned Pred: 'mèo'
  [Sample 1] Raw GT: 'mèo', Raw Pred: 'Mèo'
  [Sample 1] Cleaned GT: 'mèo', Cleaned Pred: 'mèo'
  [Sample 2] Raw GT: 'mèo', Raw Pred: 'Mèo'
  [Sample 2] Cleaned GT: 'mèo', Cleaned Pred: 'mèo'
  [Sample 3] Raw GT: 'mèo', Raw Pred: 'Mèo'
  [Sample 3] Cleaned GT: 'mèo', Cleaned Pred: 'mèo'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.2931, CLIP Max: 20.3961
   [Sample 0] BERTScore=0.8080, CLIPRaw=19.29 (CLIPNorm=0.0000) -> Reward=0.4040
   [Sample 1] BERTScore=0.8473, CLIPRaw=19.51 (CLIPNorm=0.1923) -> Reward=0.5198
   [Sample 2] BERTScore=0.7978, CLIPRaw=20.40 (CLIPNorm=1.0000) -> Reward=0.8989
   [Sample 3] BERTScore=0.8240, CLIPRaw=20.11 (CLIPNorm=0.7389) -> Reward=0.7814
{'loss': 0.0015856, 'grad_norm': 1.17400074, 'learning_rate': 5.08e-06, 'reward': 2.56380153, 'reward_std': 0.28568646, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.65104115, 'rewards/CustomExplainationReward/std': 0.22854917, 'completions/mean_length': 133.0, 'completions/min_length': 108.0, 'completions/max_length': 159.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03963945, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '104/200', 'percentage': '52.00%', 'elapsed_time': '1h 59m 15s', 'remaining_time': '1h 50m 5s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014533}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 22.0836, CLIP Max: 23.1470
   [Sample 0] BERTScore=0.8209, CLIPRaw=22.43 (CLIPNorm=0.3295) -> Reward=0.5752
   [Sample 1] BERTScore=0.8016, CLIPRaw=23.15 (CLIPNorm=1.0000) -> Reward=0.9008
   [Sample 2] BERTScore=0.8174, CLIPRaw=22.08 (CLIPNorm=0.0000) -> Reward=0.4087
   [Sample 3] BERTScore=0.8157, CLIPRaw=22.20 (CLIPNorm=0.1094) -> Reward=0.4626
{'loss': 0.00077756, 'grad_norm': 0.8398999, 'learning_rate': 5e-06, 'reward': 1.93846571, 'reward_std': 0.86416948, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.56395739, 'rewards/CustomAccuracyReward/std': 0.50349867, 'rewards/CustomExplainationReward/mean': 0.58681512, 'rewards/CustomExplainationReward/std': 0.22052422, 'completions/mean_length': 180.75, 'completions/min_length': 130.0, 'completions/max_length': 240.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01943886, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '105/200', 'percentage': '52.50%', 'elapsed_time': '2h 0m 27s', 'remaining_time': '1h 48m 59s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014528}
  [Sample 0] Raw GT: 'phòng tắm', Raw Pred: 'Phòng tắm'
  [Sample 0] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng tắm'
  [Sample 1] Raw GT: 'phòng tắm', Raw Pred: 'Phòng tắm'
  [Sample 1] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng tắm'
  [Sample 2] Raw GT: 'phòng tắm', Raw Pred: 'Phòng vệ sinh'
  [Sample 2] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng vệ sinh'
  [Sample 3] Raw GT: 'phòng tắm', Raw Pred: 'Phòng tắm'
  [Sample 3] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng tắm'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=0.4150, BERTScore=0.5730 -> Reward=0.4940
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 21.0418, CLIP Max: 21.6045
   [Sample 0] BERTScore=0.7853, CLIPRaw=21.50 (CLIPNorm=0.8187) -> Reward=0.8020
   [Sample 1] BERTScore=0.7670, CLIPRaw=21.04 (CLIPNorm=0.0000) -> Reward=0.3835
   [Sample 2] BERTScore=0.7386, CLIPRaw=21.60 (CLIPNorm=1.0000) -> Reward=0.8693
   [Sample 3] BERTScore=0.7692, CLIPRaw=21.35 (CLIPNorm=0.5415) -> Reward=0.6553
{'loss': 0.0009153, 'grad_norm': 0.92712933, 'learning_rate': 4.92e-06, 'reward': 2.43878722, 'reward_std': 0.26728395, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.87349987, 'rewards/CustomAccuracyReward/std': 0.25300026, 'rewards/CustomExplainationReward/mean': 0.67752993, 'rewards/CustomExplainationReward/std': 0.21541657, 'completions/mean_length': 168.25, 'completions/min_length': 149.0, 'completions/max_length': 187.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02288792, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '106/200', 'percentage': '53.00%', 'elapsed_time': '2h 1m 29s', 'remaining_time': '1h 47m 43s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014542}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Không'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 17.5895, CLIP Max: 19.3680
   [Sample 0] BERTScore=0.7610, CLIPRaw=17.87 (CLIPNorm=0.1554) -> Reward=0.4582
   [Sample 1] BERTScore=0.7835, CLIPRaw=19.11 (CLIPNorm=0.8522) -> Reward=0.8178
   [Sample 2] BERTScore=0.7690, CLIPRaw=17.59 (CLIPNorm=0.0000) -> Reward=0.3845
   [Sample 3] BERTScore=0.7841, CLIPRaw=19.37 (CLIPNorm=1.0000) -> Reward=0.8921
{'loss': 0.00124697, 'grad_norm': 0.9438808, 'learning_rate': 4.83e-06, 'reward': 1.73011827, 'reward_std': 0.7490344, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.34593609, 'rewards/CustomAccuracyReward/std': 0.43604264, 'rewards/CustomExplainationReward/mean': 0.63815844, 'rewards/CustomExplainationReward/std': 0.25393334, 'completions/mean_length': 188.25, 'completions/min_length': 145.0, 'completions/max_length': 224.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03117383, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '107/200', 'percentage': '53.50%', 'elapsed_time': '2h 2m 53s', 'remaining_time': '1h 46m 48s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014512}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.0141, CLIP Max: 19.9604
   [Sample 0] BERTScore=0.7799, CLIPRaw=19.42 (CLIPNorm=0.4340) -> Reward=0.6070
   [Sample 1] BERTScore=0.7859, CLIPRaw=19.96 (CLIPNorm=1.0000) -> Reward=0.8929
   [Sample 2] BERTScore=0.8141, CLIPRaw=19.01 (CLIPNorm=0.0000) -> Reward=0.4070
   [Sample 3] BERTScore=0.7993, CLIPRaw=19.75 (CLIPNorm=0.7782) -> Reward=0.7888
{'loss': 0.00155623, 'grad_norm': 0.96521854, 'learning_rate': 4.75e-06, 'reward': 2.5924015, 'reward_std': 0.26699883, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.67392123, 'rewards/CustomExplainationReward/std': 0.21359909, 'completions/mean_length': 154.25, 'completions/min_length': 123.0, 'completions/max_length': 178.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03891043, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '108/200', 'percentage': '54.00%', 'elapsed_time': '2h 3m 55s', 'remaining_time': '1h 45m 33s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014525}
  [Sample 0] Raw GT: 'cừu', Raw Pred: 'Bò đực'
  [Sample 0] Cleaned GT: 'cừu', Cleaned Pred: 'bò đực'
  [Sample 1] Raw GT: 'cừu', Raw Pred: 'Lợn'
  [Sample 1] Cleaned GT: 'cừu', Cleaned Pred: 'lợn'
  [Sample 2] Raw GT: 'cừu', Raw Pred: 'Bê'
  [Sample 2] Cleaned GT: 'cừu', Cleaned Pred: 'bê'
  [Sample 3] Raw GT: 'cừu', Raw Pred: 'Lợn'
  [Sample 3] Cleaned GT: 'cừu', Cleaned Pred: 'lợn'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.3087 -> Reward=0.1544
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.1935 -> Reward=0.0967
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.1788 -> Reward=0.0894
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.1935 -> Reward=0.0967
   [Batch Stats] CLIP Min: 18.0037, CLIP Max: 18.7730
   [Sample 0] BERTScore=0.8057, CLIPRaw=18.24 (CLIPNorm=0.3020) -> Reward=0.5538
   [Sample 1] BERTScore=0.8180, CLIPRaw=18.00 (CLIPNorm=0.0000) -> Reward=0.4090
   [Sample 2] BERTScore=0.7724, CLIPRaw=18.77 (CLIPNorm=1.0000) -> Reward=0.8862
   [Sample 3] BERTScore=0.8067, CLIPRaw=18.14 (CLIPNorm=0.1811) -> Reward=0.4939
{'loss': 0.00130281, 'grad_norm': 0.99281651, 'learning_rate': 4.67e-06, 'reward': 1.36880922, 'reward_std': 0.25578186, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.10930557, 'rewards/CustomAccuracyReward/std': 0.0302367, 'rewards/CustomExplainationReward/mean': 0.58574188, 'rewards/CustomExplainationReward/std': 0.20894298, 'completions/mean_length': 157.0, 'completions/min_length': 144.0, 'completions/max_length': 171.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03257867, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '109/200', 'percentage': '54.50%', 'elapsed_time': '2h 4m 52s', 'remaining_time': '1h 44m 15s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014548}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 19.4950, CLIP Max: 19.7926
   [Sample 0] BERTScore=0.6899, CLIPRaw=19.79 (CLIPNorm=1.0000) -> Reward=0.8449
   [Sample 1] BERTScore=0.7164, CLIPRaw=19.50 (CLIPNorm=0.0000) -> Reward=0.3582
   [Sample 2] BERTScore=0.7273, CLIPRaw=19.54 (CLIPNorm=0.1662) -> Reward=0.4467
   [Sample 3] BERTScore=0.7111, CLIPRaw=19.70 (CLIPNorm=0.6905) -> Reward=0.7008
{'loss': 0.00113073, 'grad_norm': 1.00261235, 'learning_rate': 4.59e-06, 'reward': 1.66701651, 'reward_std': 0.40929648, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.34593609, 'rewards/CustomAccuracyReward/std': 0.43604264, 'rewards/CustomExplainationReward/mean': 0.58767712, 'rewards/CustomExplainationReward/std': 0.22473294, 'completions/mean_length': 152.5, 'completions/min_length': 128.0, 'completions/max_length': 163.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02827245, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '110/200', 'percentage': '55.00%', 'elapsed_time': '2h 5m 48s', 'remaining_time': '1h 42m 55s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014573}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 17.3300, CLIP Max: 20.8030
   [Sample 0] BERTScore=0.8039, CLIPRaw=17.33 (CLIPNorm=0.0000) -> Reward=0.4019
   [Sample 1] BERTScore=0.7864, CLIPRaw=19.12 (CLIPNorm=0.5157) -> Reward=0.6510
   [Sample 2] BERTScore=0.8096, CLIPRaw=18.79 (CLIPNorm=0.4204) -> Reward=0.6150
   [Sample 3] BERTScore=0.8564, CLIPRaw=20.80 (CLIPNorm=1.0000) -> Reward=0.9282
{'loss': 0.00107345, 'grad_norm': 0.89164507, 'learning_rate': 4.5e-06, 'reward': 2.56128693, 'reward_std': 0.27019238, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.64902943, 'rewards/CustomExplainationReward/std': 0.21615393, 'completions/mean_length': 169.5, 'completions/min_length': 152.0, 'completions/max_length': 184.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02682011, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '111/200', 'percentage': '55.50%', 'elapsed_time': '2h 6m 46s', 'remaining_time': '1h 41m 38s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014593}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.9464, CLIP Max: 19.6742
   [Sample 0] BERTScore=0.7931, CLIPRaw=18.95 (CLIPNorm=0.0000) -> Reward=0.3965
   [Sample 1] BERTScore=0.8026, CLIPRaw=19.67 (CLIPNorm=1.0000) -> Reward=0.9013
   [Sample 2] BERTScore=0.7795, CLIPRaw=19.60 (CLIPNorm=0.8920) -> Reward=0.8358
   [Sample 3] BERTScore=0.7792, CLIPRaw=19.24 (CLIPNorm=0.4035) -> Reward=0.5913
{'loss': 0.00130712, 'grad_norm': 0.8645454, 'learning_rate': 4.42e-06, 'reward': 2.60155296, 'reward_std': 0.289969, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.68124235, 'rewards/CustomExplainationReward/std': 0.23197515, 'completions/mean_length': 186.75, 'completions/min_length': 184.0, 'completions/max_length': 192.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03267831, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '112/200', 'percentage': '56.00%', 'elapsed_time': '2h 7m 40s', 'remaining_time': '1h 40m 19s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01462}
  [Sample 0] Raw GT: 'pizza', Raw Pred: 'Pizza'
  [Sample 0] Cleaned GT: 'pizza', Cleaned Pred: 'pizza'
  [Sample 1] Raw GT: 'pizza', Raw Pred: 'Pizza'
  [Sample 1] Cleaned GT: 'pizza', Cleaned Pred: 'pizza'
  [Sample 2] Raw GT: 'pizza', Raw Pred: 'Pizza'
  [Sample 2] Cleaned GT: 'pizza', Cleaned Pred: 'pizza'
  [Sample 3] Raw GT: 'pizza', Raw Pred: 'Pizza'
  [Sample 3] Cleaned GT: 'pizza', Cleaned Pred: 'pizza'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.3947, CLIP Max: 19.1247
   [Sample 0] BERTScore=0.7715, CLIPRaw=18.48 (CLIPNorm=0.1116) -> Reward=0.4415
   [Sample 1] BERTScore=0.7611, CLIPRaw=18.39 (CLIPNorm=0.0000) -> Reward=0.3805
   [Sample 2] BERTScore=0.8103, CLIPRaw=19.12 (CLIPNorm=1.0000) -> Reward=0.9052
   [Sample 3] BERTScore=0.7914, CLIPRaw=18.77 (CLIPNorm=0.5178) -> Reward=0.6546
{'loss': 0.00176595, 'grad_norm': 1.05260539, 'learning_rate': 4.34e-06, 'reward': 2.494313, 'reward_std': 0.29694963, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.59545046, 'rewards/CustomExplainationReward/std': 0.23755977, 'completions/mean_length': 122.25, 'completions/min_length': 113.0, 'completions/max_length': 135.0, 'completions/clipped_ratio': 0.0, 'kl': 0.04414829, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '113/200', 'percentage': '56.50%', 'elapsed_time': '2h 8m 43s', 'remaining_time': '1h 39m 6s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014631}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.2144, CLIP Max: 21.5655
   [Sample 0] BERTScore=0.8138, CLIPRaw=21.31 (CLIPNorm=0.8076) -> Reward=0.8107
   [Sample 1] BERTScore=0.7962, CLIPRaw=20.21 (CLIPNorm=0.0000) -> Reward=0.3981
   [Sample 2] BERTScore=0.8017, CLIPRaw=21.57 (CLIPNorm=1.0000) -> Reward=0.9008
   [Sample 3] BERTScore=0.8352, CLIPRaw=20.66 (CLIPNorm=0.3273) -> Reward=0.5813
{'loss': 0.0015137, 'grad_norm': 1.20908689, 'learning_rate': 4.26e-06, 'reward': 2.59089804, 'reward_std': 0.28401065, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.67271864, 'rewards/CustomExplainationReward/std': 0.22720844, 'completions/mean_length': 134.75, 'completions/min_length': 117.0, 'completions/max_length': 165.0, 'completions/clipped_ratio': 0.0, 'kl': 0.037859, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '114/200', 'percentage': '57.00%', 'elapsed_time': '2h 9m 38s', 'remaining_time': '1h 37m 48s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014656}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 17.6925, CLIP Max: 19.5218
   [Sample 0] BERTScore=0.7609, CLIPRaw=19.52 (CLIPNorm=1.0000) -> Reward=0.8805
   [Sample 1] BERTScore=0.7740, CLIPRaw=17.69 (CLIPNorm=0.0000) -> Reward=0.3870
   [Sample 2] BERTScore=0.8008, CLIPRaw=19.47 (CLIPNorm=0.9736) -> Reward=0.8872
   [Sample 3] BERTScore=0.8173, CLIPRaw=17.84 (CLIPNorm=0.0792) -> Reward=0.4482
{'loss': 0.00089966, 'grad_norm': 1.02435529, 'learning_rate': 4.18e-06, 'reward': 2.5633924, 'reward_std': 0.33793339, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.65071404, 'rewards/CustomExplainationReward/std': 0.27034679, 'completions/mean_length': 144.25, 'completions/min_length': 124.0, 'completions/max_length': 158.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02250024, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '115/200', 'percentage': '57.50%', 'elapsed_time': '2h 10m 31s', 'remaining_time': '1h 36m 28s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014684}
  [Sample 0] Raw GT: 'phòng ngủ', Raw Pred: 'Phòng ngủ'
  [Sample 0] Cleaned GT: 'phòng ngủ', Cleaned Pred: 'phòng ngủ'
  [Sample 1] Raw GT: 'phòng ngủ', Raw Pred: 'Phòng ngủ chung cư'
  [Sample 1] Cleaned GT: 'phòng ngủ', Cleaned Pred: 'phòng ngủ chung cư'
  [Sample 2] Raw GT: 'phòng ngủ', Raw Pred: 'Phòng lưu trú'
  [Sample 2] Cleaned GT: 'phòng ngủ', Cleaned Pred: 'phòng lưu trú'
  [Sample 3] Raw GT: 'phòng ngủ', Raw Pred: 'Căn phòng tầng'
  [Sample 3] Cleaned GT: 'phòng ngủ', Cleaned Pred: 'căn phòng tầng'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.7093, BERTScore=0.5466 -> Reward=0.6280
  [Sample 2] ROUGE-L=0.4150, BERTScore=0.4711 -> Reward=0.4430
  [Sample 3] ROUGE-L=0.4150, BERTScore=0.4845 -> Reward=0.4497
   [Batch Stats] CLIP Min: 19.8589, CLIP Max: 20.6653
   [Sample 0] BERTScore=0.8315, CLIPRaw=20.29 (CLIPNorm=0.5326) -> Reward=0.6821
   [Sample 1] BERTScore=0.8190, CLIPRaw=20.39 (CLIPNorm=0.6569) -> Reward=0.7380
   [Sample 2] BERTScore=0.8118, CLIPRaw=19.86 (CLIPNorm=0.0000) -> Reward=0.4059
   [Sample 3] BERTScore=0.8225, CLIPRaw=20.67 (CLIPNorm=1.0000) -> Reward=0.9113
{'loss': 0.00105282, 'grad_norm': 0.91683376, 'learning_rate': 4.1e-06, 'reward': 2.1431036, 'reward_std': 0.43099365, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.63017797, 'rewards/CustomAccuracyReward/std': 0.26099762, 'rewards/CustomExplainationReward/mean': 0.68430495, 'rewards/CustomExplainationReward/std': 0.20967782, 'completions/mean_length': 170.5, 'completions/min_length': 151.0, 'completions/max_length': 202.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02631952, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '116/200', 'percentage': '58.00%', 'elapsed_time': '2h 11m 50s', 'remaining_time': '1h 35m 28s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014664}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.6407, CLIP Max: 19.4391
   [Sample 0] BERTScore=0.8010, CLIPRaw=18.64 (CLIPNorm=0.0000) -> Reward=0.4005
   [Sample 1] BERTScore=0.8062, CLIPRaw=18.64 (CLIPNorm=0.0008) -> Reward=0.4035
   [Sample 2] BERTScore=0.7919, CLIPRaw=18.85 (CLIPNorm=0.2577) -> Reward=0.5248
   [Sample 3] BERTScore=0.7963, CLIPRaw=19.44 (CLIPNorm=1.0000) -> Reward=0.8982
{'loss': 0.00119954, 'grad_norm': 1.07154596, 'learning_rate': 4.01e-06, 'reward': 2.17341042, 'reward_std': 0.72282529, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.78197873, 'rewards/CustomAccuracyReward/std': 0.43604261, 'rewards/CustomExplainationReward/mean': 0.5567497, 'rewards/CustomExplainationReward/std': 0.23486261, 'completions/mean_length': 144.0, 'completions/min_length': 124.0, 'completions/max_length': 163.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02999139, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '117/200', 'percentage': '58.50%', 'elapsed_time': '2h 13m 5s', 'remaining_time': '1h 34m 25s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014651}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 16.8108, CLIP Max: 18.3616
   [Sample 0] BERTScore=0.7743, CLIPRaw=18.05 (CLIPNorm=0.7962) -> Reward=0.7852
   [Sample 1] BERTScore=0.7547, CLIPRaw=16.81 (CLIPNorm=0.0000) -> Reward=0.3773
   [Sample 2] BERTScore=0.7592, CLIPRaw=17.98 (CLIPNorm=0.7560) -> Reward=0.7576
   [Sample 3] BERTScore=0.7502, CLIPRaw=18.36 (CLIPNorm=1.0000) -> Reward=0.8751
{'loss': 0.00087917, 'grad_norm': 0.84913361, 'learning_rate': 3.93e-06, 'reward': 2.07846737, 'reward_std': 0.82785589, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.56395739, 'rewards/CustomAccuracyReward/std': 0.50349867, 'rewards/CustomExplainationReward/mean': 0.69881636, 'rewards/CustomExplainationReward/std': 0.2201166, 'completions/mean_length': 188.75, 'completions/min_length': 141.0, 'completions/max_length': 220.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02197537, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '118/200', 'percentage': '59.00%', 'elapsed_time': '2h 14m 31s', 'remaining_time': '1h 33m 28s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01462}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'có', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'có', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 20.0484, CLIP Max: 20.5721
   [Sample 0] BERTScore=0.8106, CLIPRaw=20.05 (CLIPNorm=0.0000) -> Reward=0.4053
   [Sample 1] BERTScore=0.7839, CLIPRaw=20.36 (CLIPNorm=0.6003) -> Reward=0.6921
   [Sample 2] BERTScore=0.7896, CLIPRaw=20.15 (CLIPNorm=0.2028) -> Reward=0.4962
   [Sample 3] BERTScore=0.7875, CLIPRaw=20.57 (CLIPNorm=1.0000) -> Reward=0.8938
{'loss': 0.00098253, 'grad_norm': 0.97874236, 'learning_rate': 3.85e-06, 'reward': 1.70971632, 'reward_std': 0.41723883, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.34593609, 'rewards/CustomAccuracyReward/std': 0.43604264, 'rewards/CustomExplainationReward/mean': 0.6218369, 'rewards/CustomExplainationReward/std': 0.21722825, 'completions/mean_length': 140.75, 'completions/min_length': 120.0, 'completions/max_length': 163.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02455944, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '119/200', 'percentage': '59.50%', 'elapsed_time': '2h 15m 41s', 'remaining_time': '1h 32m 21s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014617}
  [Sample 0] Raw GT: 'phòng tắm', Raw Pred: 'Phòng tắm'
  [Sample 0] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng tắm'
  [Sample 1] Raw GT: 'phòng tắm', Raw Pred: 'Phòng tắm'
  [Sample 1] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng tắm'
  [Sample 2] Raw GT: 'phòng tắm', Raw Pred: 'Phòng tắm'
  [Sample 2] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng tắm'
  [Sample 3] Raw GT: 'phòng tắm', Raw Pred: 'Nhà vệ sinh'
  [Sample 3] Cleaned GT: 'phòng tắm', Cleaned Pred: 'nhà vệ sinh'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.3888 -> Reward=0.1944
   [Batch Stats] CLIP Min: 20.2950, CLIP Max: 21.2087
   [Sample 0] BERTScore=0.8238, CLIPRaw=20.99 (CLIPNorm=0.7639) -> Reward=0.7939
   [Sample 1] BERTScore=0.8361, CLIPRaw=21.21 (CLIPNorm=1.0000) -> Reward=0.9180
   [Sample 2] BERTScore=0.8564, CLIPRaw=20.30 (CLIPNorm=0.0000) -> Reward=0.4282
   [Sample 3] BERTScore=0.8542, CLIPRaw=21.04 (CLIPNorm=0.8101) -> Reward=0.8322
{'loss': 0.00217438, 'grad_norm': 0.97753847, 'learning_rate': 3.77e-06, 'reward': 2.42707872, 'reward_std': 0.50180209, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.79860342, 'rewards/CustomAccuracyReward/std': 0.40279326, 'rewards/CustomExplainationReward/mean': 0.74305946, 'rewards/CustomExplainationReward/std': 0.21624483, 'completions/mean_length': 137.25, 'completions/min_length': 119.0, 'completions/max_length': 163.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0543523, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '120/200', 'percentage': '60.00%', 'elapsed_time': '2h 16m 27s', 'remaining_time': '1h 30m 58s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014656}
  [Sample 0] Raw GT: 'phòng bếp', Raw Pred: 'Khu vực bếp'
  [Sample 0] Cleaned GT: 'phòng bếp', Cleaned Pred: 'khu vực bếp'
  [Sample 1] Raw GT: 'phòng bếp', Raw Pred: 'Tuần hồi'
  [Sample 1] Cleaned GT: 'phòng bếp', Cleaned Pred: 'tuần hồi'
  [Sample 2] Raw GT: 'phòng bếp', Raw Pred: 'Các phòng bếp'
  [Sample 2] Cleaned GT: 'phòng bếp', Cleaned Pred: 'các phòng bếp'
  [Sample 3] Raw GT: 'phòng bếp', Raw Pred: 'Khách sạn'
  [Sample 3] Cleaned GT: 'phòng bếp', Cleaned Pred: 'khách sạn'
  [Sample 0] ROUGE-L=0.4150, BERTScore=0.2981 -> Reward=0.3565
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.5784 -> Reward=0.2892
  [Sample 2] ROUGE-L=0.8299, BERTScore=0.3381 -> Reward=0.5840
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.5665 -> Reward=0.2833
   [Batch Stats] CLIP Min: 20.4493, CLIP Max: 20.9619
   [Sample 0] BERTScore=0.7586, CLIPRaw=20.96 (CLIPNorm=1.0000) -> Reward=0.8793
   [Sample 1] BERTScore=0.7050, CLIPRaw=20.53 (CLIPNorm=0.1542) -> Reward=0.4296
   [Sample 2] BERTScore=0.7346, CLIPRaw=20.74 (CLIPNorm=0.5746) -> Reward=0.6546
   [Sample 3] BERTScore=0.7888, CLIPRaw=20.45 (CLIPNorm=0.0000) -> Reward=0.3944
{'loss': 0.00157267, 'grad_norm': 1.16671181, 'learning_rate': 3.69e-06, 'reward': 1.70966291, 'reward_std': 0.38954315, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.37824753, 'rewards/CustomAccuracyReward/std': 0.14114775, 'rewards/CustomExplainationReward/mean': 0.5894829, 'rewards/CustomExplainationReward/std': 0.22497939, 'completions/mean_length': 140.75, 'completions/min_length': 115.0, 'completions/max_length': 169.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03931935, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '121/200', 'percentage': '60.50%', 'elapsed_time': '2h 17m 39s', 'remaining_time': '1h 29m 52s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01465}
  [Sample 0] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 0] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 1] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 1] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 2] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 2] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 3] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 3] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 16.5780, CLIP Max: 17.7426
   [Sample 0] BERTScore=0.8221, CLIPRaw=17.74 (CLIPNorm=1.0000) -> Reward=0.9110
   [Sample 1] BERTScore=0.8082, CLIPRaw=16.73 (CLIPNorm=0.1315) -> Reward=0.4698
   [Sample 2] BERTScore=0.8190, CLIPRaw=16.58 (CLIPNorm=0.0000) -> Reward=0.4095
   [Sample 3] BERTScore=0.8302, CLIPRaw=17.60 (CLIPNorm=0.8800) -> Reward=0.8551
{'loss': 0.00082317, 'grad_norm': 0.79505467, 'learning_rate': 3.61e-06, 'reward': 2.57671642, 'reward_std': 0.32274571, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.66137326, 'rewards/CustomExplainationReward/std': 0.25819653, 'completions/mean_length': 209.75, 'completions/min_length': 162.0, 'completions/max_length': 253.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02058779, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '122/200', 'percentage': '61.00%', 'elapsed_time': '2h 19m 0s', 'remaining_time': '1h 28m 52s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014628}
  [Sample 0] Raw GT: 'u ám', Raw Pred: 'U ám'
  [Sample 0] Cleaned GT: 'u ám', Cleaned Pred: 'u ám'
  [Sample 1] Raw GT: 'u ám', Raw Pred: 'U ám'
  [Sample 1] Cleaned GT: 'u ám', Cleaned Pred: 'u ám'
  [Sample 2] Raw GT: 'u ám', Raw Pred: 'U ám'
  [Sample 2] Cleaned GT: 'u ám', Cleaned Pred: 'u ám'
  [Sample 3] Raw GT: 'u ám', Raw Pred: 'U ám'
  [Sample 3] Cleaned GT: 'u ám', Cleaned Pred: 'u ám'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.5188, CLIP Max: 20.7602
   [Sample 0] BERTScore=0.8464, CLIPRaw=19.52 (CLIPNorm=0.0000) -> Reward=0.4232
   [Sample 1] BERTScore=0.8400, CLIPRaw=20.08 (CLIPNorm=0.4520) -> Reward=0.6460
   [Sample 2] BERTScore=0.8230, CLIPRaw=19.99 (CLIPNorm=0.3823) -> Reward=0.6027
   [Sample 3] BERTScore=0.8288, CLIPRaw=20.76 (CLIPNorm=1.0000) -> Reward=0.9144
{'loss': 0.0012463, 'grad_norm': 1.15299261, 'learning_rate': 3.53e-06, 'reward': 2.55819082, 'reward_std': 0.2536906, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.64655268, 'rewards/CustomExplainationReward/std': 0.20295238, 'completions/mean_length': 149.5, 'completions/min_length': 134.0, 'completions/max_length': 164.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03115816, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '123/200', 'percentage': '61.50%', 'elapsed_time': '2h 20m 1s', 'remaining_time': '1h 27m 39s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014641}
  [Sample 0] Raw GT: 'quần vợt', Raw Pred: 'Tennis'
  [Sample 0] Cleaned GT: 'quần vợt', Cleaned Pred: 'tennis'
  [Sample 1] Raw GT: 'quần vợt', Raw Pred: 'Tennis'
  [Sample 1] Cleaned GT: 'quần vợt', Cleaned Pred: 'tennis'
  [Sample 2] Raw GT: 'quần vợt', Raw Pred: 'Tennis'
  [Sample 2] Cleaned GT: 'quần vợt', Cleaned Pred: 'tennis'
  [Sample 3] Raw GT: 'quần vợt', Raw Pred: 'Tennis'
  [Sample 3] Cleaned GT: 'quần vợt', Cleaned Pred: 'tennis'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.4712 -> Reward=0.2356
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.4712 -> Reward=0.2356
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.4712 -> Reward=0.2356
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.4712 -> Reward=0.2356
   [Batch Stats] CLIP Min: 16.2661, CLIP Max: 17.1282
   [Sample 0] BERTScore=0.7773, CLIPRaw=17.13 (CLIPNorm=1.0000) -> Reward=0.8887
   [Sample 1] BERTScore=0.7753, CLIPRaw=16.63 (CLIPNorm=0.4200) -> Reward=0.5976
   [Sample 2] BERTScore=0.7751, CLIPRaw=16.95 (CLIPNorm=0.7954) -> Reward=0.7852
   [Sample 3] BERTScore=0.7804, CLIPRaw=16.27 (CLIPNorm=0.0000) -> Reward=0.3902
{'loss': 0.00091639, 'grad_norm': 0.84442729, 'learning_rate': 3.45e-06, 'reward': 1.62630713, 'reward_std': 0.2743645, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.23562318, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.6654225, 'rewards/CustomExplainationReward/std': 0.21949163, 'completions/mean_length': 167.25, 'completions/min_length': 159.0, 'completions/max_length': 173.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02290796, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '124/200', 'percentage': '62.00%', 'elapsed_time': '2h 20m 58s', 'remaining_time': '1h 26m 24s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01466}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.2633, CLIP Max: 20.6770
   [Sample 0] BERTScore=0.8523, CLIPRaw=20.68 (CLIPNorm=1.0000) -> Reward=0.9261
   [Sample 1] BERTScore=0.8468, CLIPRaw=19.39 (CLIPNorm=0.0903) -> Reward=0.4685
   [Sample 2] BERTScore=0.8660, CLIPRaw=19.61 (CLIPNorm=0.2434) -> Reward=0.5547
   [Sample 3] BERTScore=0.8582, CLIPRaw=19.26 (CLIPNorm=0.0000) -> Reward=0.4291
{'loss': 0.00106224, 'grad_norm': 1.08308113, 'learning_rate': 3.38e-06, 'reward': 2.49329615, 'reward_std': 0.28392813, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.59463692, 'rewards/CustomExplainationReward/std': 0.22714244, 'completions/mean_length': 154.0, 'completions/min_length': 130.0, 'completions/max_length': 182.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02655078, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '125/200', 'percentage': '62.50%', 'elapsed_time': '2h 21m 50s', 'remaining_time': '1h 25m 6s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014688}
  [Sample 0] Raw GT: 'quần vợt', Raw Pred: 'Tennis'
  [Sample 0] Cleaned GT: 'quần vợt', Cleaned Pred: 'tennis'
  [Sample 1] Raw GT: 'quần vợt', Raw Pred: 'Tennis'
  [Sample 1] Cleaned GT: 'quần vợt', Cleaned Pred: 'tennis'
  [Sample 2] Raw GT: 'quần vợt', Raw Pred: 'Tennis'
  [Sample 2] Cleaned GT: 'quần vợt', Cleaned Pred: 'tennis'
  [Sample 3] Raw GT: 'quần vợt', Raw Pred: 'Tennis'
  [Sample 3] Cleaned GT: 'quần vợt', Cleaned Pred: 'tennis'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.4712 -> Reward=0.2356
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.4712 -> Reward=0.2356
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.4712 -> Reward=0.2356
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.4712 -> Reward=0.2356
   [Batch Stats] CLIP Min: 16.4028, CLIP Max: 18.2546
   [Sample 0] BERTScore=0.8363, CLIPRaw=18.01 (CLIPNorm=0.8668) -> Reward=0.8516
   [Sample 1] BERTScore=0.8400, CLIPRaw=16.40 (CLIPNorm=0.0000) -> Reward=0.4200
   [Sample 2] BERTScore=0.8376, CLIPRaw=18.16 (CLIPNorm=0.9466) -> Reward=0.8921
   [Sample 3] BERTScore=0.8311, CLIPRaw=18.25 (CLIPNorm=1.0000) -> Reward=0.9156
{'loss': 0.00105281, 'grad_norm': 0.88181967, 'learning_rate': 3.3e-06, 'reward': 1.75679159, 'reward_std': 0.29336739, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.23562318, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.7698102, 'rewards/CustomExplainationReward/std': 0.2346939, 'completions/mean_length': 149.75, 'completions/min_length': 126.0, 'completions/max_length': 176.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02632733, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '126/200', 'percentage': '63.00%', 'elapsed_time': '2h 23m 2s', 'remaining_time': '1h 24m 0s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014682}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.6717, CLIP Max: 22.2189
   [Sample 0] BERTScore=0.7542, CLIPRaw=20.81 (CLIPNorm=0.4456) -> Reward=0.5999
   [Sample 1] BERTScore=0.7771, CLIPRaw=20.09 (CLIPNorm=0.1654) -> Reward=0.4712
   [Sample 2] BERTScore=0.7936, CLIPRaw=22.22 (CLIPNorm=1.0000) -> Reward=0.8968
   [Sample 3] BERTScore=0.7890, CLIPRaw=19.67 (CLIPNorm=0.0000) -> Reward=0.3945
{'loss': 0.00115409, 'grad_norm': 1.3049382, 'learning_rate': 3.22e-06, 'reward': 2.48826885, 'reward_std': 0.27628708, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.59061527, 'rewards/CustomExplainationReward/std': 0.22102965, 'completions/mean_length': 120.75, 'completions/min_length': 98.0, 'completions/max_length': 158.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02886893, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '127/200', 'percentage': '63.50%', 'elapsed_time': '2h 24m 9s', 'remaining_time': '1h 22m 51s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014683}
  [Sample 0] Raw GT: 'phòng khách', Raw Pred: 'Phòng khách'
  [Sample 0] Cleaned GT: 'phòng khách', Cleaned Pred: 'phòng khách'
  [Sample 1] Raw GT: 'phòng khách', Raw Pred: 'Phòng khách'
  [Sample 1] Cleaned GT: 'phòng khách', Cleaned Pred: 'phòng khách'
  [Sample 2] Raw GT: 'phòng khách', Raw Pred: 'Căn phòng'
  [Sample 2] Cleaned GT: 'phòng khách', Cleaned Pred: 'căn phòng'
  [Sample 3] Raw GT: 'phòng khách', Raw Pred: 'Phòng ở'
  [Sample 3] Cleaned GT: 'phòng khách', Cleaned Pred: 'phòng ở'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=0.5000, BERTScore=0.6118 -> Reward=0.5559
  [Sample 3] ROUGE-L=0.5000, BERTScore=0.5424 -> Reward=0.5212
Error during BERTScore batch computation: The expanded size of the tensor (259) must match the existing size (258) at non-singleton dimension 1.  Target sizes: [4, 259].  Tensor sizes: [1, 258]
   [Batch Stats] CLIP Min: 18.6554, CLIP Max: 19.3110
   [Sample 0] BERTScore=0.0000, CLIPRaw=19.31 (CLIPNorm=1.0000) -> Reward=0.5000
   [Sample 1] BERTScore=0.0000, CLIPRaw=18.66 (CLIPNorm=0.0000) -> Reward=0.0000
   [Sample 2] BERTScore=0.0000, CLIPRaw=19.21 (CLIPNorm=0.8469) -> Reward=0.4235
   [Sample 3] BERTScore=0.0000, CLIPRaw=19.25 (CLIPNorm=0.9076) -> Reward=0.4538
{'loss': 0.0012164, 'grad_norm': 0.97200125, 'learning_rate': 3.14e-06, 'reward': 1.89198387, 'reward_std': 0.32229909, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.76927567, 'rewards/CustomAccuracyReward/std': 0.26679298, 'rewards/CustomExplainationReward/mean': 0.34431145, 'rewards/CustomExplainationReward/std': 0.23168851, 'completions/mean_length': 205.5, 'completions/min_length': 169.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03041279, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '128/200', 'percentage': '64.00%', 'elapsed_time': '2h 25m 26s', 'remaining_time': '1h 21m 48s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014669}
  [Sample 0] Raw GT: 'đọc', Raw Pred: 'Đang đọc báo'
  [Sample 0] Cleaned GT: 'đọc', Cleaned Pred: 'đang đọc báo'
  [Sample 1] Raw GT: 'đọc', Raw Pred: 'Nghỉ ngơi và đọc báo'
  [Sample 1] Cleaned GT: 'đọc', Cleaned Pred: 'nghỉ ngơi và đọc báo'
  [Sample 2] Raw GT: 'đọc', Raw Pred: 'Đang đọc báo'
  [Sample 2] Cleaned GT: 'đọc', Cleaned Pred: 'đang đọc báo'
  [Sample 3] Raw GT: 'đọc', Raw Pred: 'Nghỉ ngơi và đọc báo'
  [Sample 3] Cleaned GT: 'đọc', Cleaned Pred: 'nghỉ ngơi và đọc báo'
  [Sample 0] ROUGE-L=0.5495, BERTScore=0.5516 -> Reward=0.5506
  [Sample 1] ROUGE-L=0.3789, BERTScore=0.3785 -> Reward=0.3787
  [Sample 2] ROUGE-L=0.5495, BERTScore=0.5516 -> Reward=0.5506
  [Sample 3] ROUGE-L=0.3789, BERTScore=0.3785 -> Reward=0.3787
   [Batch Stats] CLIP Min: 17.3166, CLIP Max: 18.5470
   [Sample 0] BERTScore=0.8036, CLIPRaw=17.46 (CLIPNorm=0.1180) -> Reward=0.4608
   [Sample 1] BERTScore=0.8363, CLIPRaw=18.55 (CLIPNorm=1.0000) -> Reward=0.9182
   [Sample 2] BERTScore=0.7855, CLIPRaw=17.32 (CLIPNorm=0.0000) -> Reward=0.3928
   [Sample 3] BERTScore=0.8283, CLIPRaw=18.14 (CLIPNorm=0.6667) -> Reward=0.7475
{'loss': 0.00125689, 'grad_norm': 0.87786371, 'learning_rate': 3.07e-06, 'reward': 1.86803591, 'reward_std': 0.19327638, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.46462405, 'rewards/CustomAccuracyReward/std': 0.09922632, 'rewards/CustomExplainationReward/mean': 0.62980461, 'rewards/CustomExplainationReward/std': 0.24614021, 'completions/mean_length': 175.75, 'completions/min_length': 156.0, 'completions/max_length': 188.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03142183, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '129/200', 'percentage': '64.50%', 'elapsed_time': '2h 26m 27s', 'remaining_time': '1h 20m 36s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014681}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 18.2382, CLIP Max: 18.4217
   [Sample 0] BERTScore=0.7656, CLIPRaw=18.38 (CLIPNorm=0.7744) -> Reward=0.7700
   [Sample 1] BERTScore=0.7900, CLIPRaw=18.24 (CLIPNorm=0.0000) -> Reward=0.3950
   [Sample 2] BERTScore=0.7978, CLIPRaw=18.42 (CLIPNorm=1.0000) -> Reward=0.8989
   [Sample 3] BERTScore=0.7784, CLIPRaw=18.33 (CLIPNorm=0.5147) -> Reward=0.6466
{'loss': 0.00136063, 'grad_norm': 1.11101377, 'learning_rate': 2.99e-06, 'reward': 1.50692403, 'reward_std': 0.26841459, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.12791477, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.67762446, 'rewards/CustomExplainationReward/std': 0.21473165, 'completions/mean_length': 140.75, 'completions/min_length': 131.0, 'completions/max_length': 151.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03401827, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '130/200', 'percentage': '65.00%', 'elapsed_time': '2h 27m 18s', 'remaining_time': '1h 19m 19s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014709}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 19.0607, CLIP Max: 20.1147
   [Sample 0] BERTScore=0.7581, CLIPRaw=19.06 (CLIPNorm=0.0000) -> Reward=0.3791
   [Sample 1] BERTScore=0.7411, CLIPRaw=20.11 (CLIPNorm=1.0000) -> Reward=0.8705
   [Sample 2] BERTScore=0.7701, CLIPRaw=19.22 (CLIPNorm=0.1515) -> Reward=0.4608
   [Sample 3] BERTScore=0.7721, CLIPRaw=19.80 (CLIPNorm=0.7028) -> Reward=0.7375
{'loss': 0.00106853, 'grad_norm': 1.04214716, 'learning_rate': 2.92e-06, 'reward': 1.42484307, 'reward_std': 0.28841618, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.12791477, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.6119597, 'rewards/CustomExplainationReward/std': 0.23073296, 'completions/mean_length': 142.25, 'completions/min_length': 118.0, 'completions/max_length': 167.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02671981, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '131/200', 'percentage': '65.50%', 'elapsed_time': '2h 28m 36s', 'remaining_time': '1h 18m 16s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014692}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.9338, CLIP Max: 21.1298
   [Sample 0] BERTScore=0.7445, CLIPRaw=20.35 (CLIPNorm=0.3492) -> Reward=0.5469
   [Sample 1] BERTScore=0.7595, CLIPRaw=20.85 (CLIPNorm=0.7654) -> Reward=0.7624
   [Sample 2] BERTScore=0.7283, CLIPRaw=21.13 (CLIPNorm=1.0000) -> Reward=0.8641
   [Sample 3] BERTScore=0.7503, CLIPRaw=19.93 (CLIPNorm=0.0000) -> Reward=0.3752
{'loss': 0.00106204, 'grad_norm': 0.95608056, 'learning_rate': 2.84e-06, 'reward': 2.54643202, 'reward_std': 0.27387172, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.6371457, 'rewards/CustomExplainationReward/std': 0.21909736, 'completions/mean_length': 155.75, 'completions/min_length': 131.0, 'completions/max_length': 180.0, 'completions/clipped_ratio': 0.0, 'kl': 0.026563, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '132/200', 'percentage': '66.00%', 'elapsed_time': '2h 29m 38s', 'remaining_time': '1h 17m 5s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014702}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Không có'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=0.7093, BERTScore=0.1853 -> Reward=0.4473
   [Batch Stats] CLIP Min: 17.9608, CLIP Max: 18.9030
   [Sample 0] BERTScore=0.8120, CLIPRaw=18.90 (CLIPNorm=1.0000) -> Reward=0.9060
   [Sample 1] BERTScore=0.7977, CLIPRaw=18.01 (CLIPNorm=0.0536) -> Reward=0.4257
   [Sample 2] BERTScore=0.7989, CLIPRaw=17.96 (CLIPNorm=0.0000) -> Reward=0.3995
   [Sample 3] BERTScore=0.7912, CLIPRaw=18.78 (CLIPNorm=0.8689) -> Reward=0.8300
{'loss': 0.00118975, 'grad_norm': 1.06332099, 'learning_rate': 2.77e-06, 'reward': 2.37765121, 'reward_std': 0.34613755, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.86182463, 'rewards/CustomAccuracyReward/std': 0.2763508, 'rewards/CustomExplainationReward/mean': 0.64029658, 'rewards/CustomExplainationReward/std': 0.26500437, 'completions/mean_length': 134.0, 'completions/min_length': 127.0, 'completions/max_length': 146.0, 'completions/clipped_ratio': 0.0, 'kl': 0.029757, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '133/200', 'percentage': '66.50%', 'elapsed_time': '2h 30m 43s', 'remaining_time': '1h 15m 55s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014707}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.6466, CLIP Max: 20.0674
   [Sample 0] BERTScore=0.8200, CLIPRaw=19.65 (CLIPNorm=0.0000) -> Reward=0.4100
   [Sample 1] BERTScore=0.8427, CLIPRaw=20.07 (CLIPNorm=1.0000) -> Reward=0.9213
   [Sample 2] BERTScore=0.8176, CLIPRaw=19.75 (CLIPNorm=0.2417) -> Reward=0.5296
   [Sample 3] BERTScore=0.8421, CLIPRaw=19.97 (CLIPNorm=0.7630) -> Reward=0.8026
{'loss': 0.00085241, 'grad_norm': 0.99741364, 'learning_rate': 2.69e-06, 'reward': 2.58235502, 'reward_std': 0.2957848, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.6658839, 'rewards/CustomExplainationReward/std': 0.23662783, 'completions/mean_length': 135.25, 'completions/min_length': 108.0, 'completions/max_length': 186.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02130896, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '134/200', 'percentage': '67.00%', 'elapsed_time': '2h 31m 41s', 'remaining_time': '1h 14m 42s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014722}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 16.4827, CLIP Max: 17.2379
   [Sample 0] BERTScore=0.6953, CLIPRaw=16.64 (CLIPNorm=0.2131) -> Reward=0.4542
   [Sample 1] BERTScore=0.7044, CLIPRaw=16.63 (CLIPNorm=0.1933) -> Reward=0.4488
   [Sample 2] BERTScore=0.7397, CLIPRaw=16.48 (CLIPNorm=0.0000) -> Reward=0.3699
   [Sample 3] BERTScore=0.7161, CLIPRaw=17.24 (CLIPNorm=1.0000) -> Reward=0.8580
{'loss': 0.0015924, 'grad_norm': 0.97523415, 'learning_rate': 2.62e-06, 'reward': 2.41593218, 'reward_std': 0.27533159, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.53274596, 'rewards/CustomExplainationReward/std': 0.22026527, 'completions/mean_length': 186.5, 'completions/min_length': 159.0, 'completions/max_length': 224.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03982571, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '135/200', 'percentage': '67.50%', 'elapsed_time': '2h 32m 49s', 'remaining_time': '1h 13m 34s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014723}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.8878, CLIP Max: 21.7863
   [Sample 0] BERTScore=0.7519, CLIPRaw=20.89 (CLIPNorm=0.0000) -> Reward=0.3759
   [Sample 1] BERTScore=0.7622, CLIPRaw=21.79 (CLIPNorm=1.0000) -> Reward=0.8811
   [Sample 2] BERTScore=0.7677, CLIPRaw=21.47 (CLIPNorm=0.6466) -> Reward=0.7072
   [Sample 3] BERTScore=0.7555, CLIPRaw=20.98 (CLIPNorm=0.0972) -> Reward=0.4264
{'loss': 0.0007702, 'grad_norm': 1.01758015, 'learning_rate': 2.55e-06, 'reward': 2.49705744, 'reward_std': 0.29829738, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.59764588, 'rewards/CustomExplainationReward/std': 0.23863789, 'completions/mean_length': 175.25, 'completions/min_length': 137.0, 'completions/max_length': 207.0, 'completions/clipped_ratio': 0.0, 'kl': 0.019249, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '136/200', 'percentage': '68.00%', 'elapsed_time': '2h 33m 53s', 'remaining_time': '1h 12m 25s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014729}
  [Sample 0] Raw GT: 'nho', Raw Pred: 'Nho'
  [Sample 0] Cleaned GT: 'nho', Cleaned Pred: 'nho'
  [Sample 1] Raw GT: 'nho', Raw Pred: 'Lê'
  [Sample 1] Cleaned GT: 'nho', Cleaned Pred: 'lê'
  [Sample 2] Raw GT: 'nho', Raw Pred: 'Nho'
  [Sample 2] Cleaned GT: 'nho', Cleaned Pred: 'nho'
  [Sample 3] Raw GT: 'nho', Raw Pred: 'Mọng xanh'
  [Sample 3] Cleaned GT: 'nho', Cleaned Pred: 'mọng xanh'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.6173 -> Reward=0.3086
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.4458 -> Reward=0.2229
   [Batch Stats] CLIP Min: 22.7893, CLIP Max: 23.3117
   [Sample 0] BERTScore=0.6994, CLIPRaw=22.79 (CLIPNorm=0.0003) -> Reward=0.3499
   [Sample 1] BERTScore=0.7931, CLIPRaw=23.31 (CLIPNorm=1.0000) -> Reward=0.8965
   [Sample 2] BERTScore=0.7770, CLIPRaw=22.79 (CLIPNorm=0.0000) -> Reward=0.3885
   [Sample 3] BERTScore=0.7301, CLIPRaw=22.89 (CLIPNorm=0.1837) -> Reward=0.4569
{'loss': 0.00127497, 'grad_norm': 0.96999991, 'learning_rate': 2.48e-06, 'reward': 1.94480681, 'reward_std': 0.40874884, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.63288337, 'rewards/CustomAccuracyReward/std': 0.42535257, 'rewards/CustomExplainationReward/mean': 0.52296209, 'rewards/CustomExplainationReward/std': 0.25295916, 'completions/mean_length': 161.0, 'completions/min_length': 127.0, 'completions/max_length': 186.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0318739, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '137/200', 'percentage': '68.50%', 'elapsed_time': '2h 34m 56s', 'remaining_time': '1h 11m 14s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014737}
  [Sample 0] Raw GT: 'san francisco', Raw Pred: 'San Francisco'
  [Sample 0] Cleaned GT: 'san francisco', Cleaned Pred: 'san francisco'
  [Sample 1] Raw GT: 'san francisco', Raw Pred: 'San Francisco'
  [Sample 1] Cleaned GT: 'san francisco', Cleaned Pred: 'san francisco'
  [Sample 2] Raw GT: 'san francisco', Raw Pred: 'San Francisco'
  [Sample 2] Cleaned GT: 'san francisco', Cleaned Pred: 'san francisco'
  [Sample 3] Raw GT: 'san francisco', Raw Pred: 'San Francisco'
  [Sample 3] Cleaned GT: 'san francisco', Cleaned Pred: 'san francisco'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 21.0352, CLIP Max: 22.0317
   [Sample 0] BERTScore=0.8003, CLIPRaw=21.33 (CLIPNorm=0.2924) -> Reward=0.5464
   [Sample 1] BERTScore=0.8035, CLIPRaw=21.05 (CLIPNorm=0.0150) -> Reward=0.4093
   [Sample 2] BERTScore=0.7208, CLIPRaw=21.04 (CLIPNorm=0.0000) -> Reward=0.3604
   [Sample 3] BERTScore=0.7145, CLIPRaw=22.03 (CLIPNorm=1.0000) -> Reward=0.8572
{'loss': 0.00082922, 'grad_norm': 0.89770287, 'learning_rate': 2.41e-06, 'reward': 2.42914867, 'reward_std': 0.27949202, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.54331899, 'rewards/CustomExplainationReward/std': 0.22359358, 'completions/mean_length': 142.75, 'completions/min_length': 129.0, 'completions/max_length': 160.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02074058, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '138/200', 'percentage': '69.00%', 'elapsed_time': '2h 35m 51s', 'remaining_time': '1h 10m 1s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014756}
  [Sample 0] Raw GT: 'trượt tuyết', Raw Pred: 'Tuyên chiến bắn băng tốc độ'
  [Sample 0] Cleaned GT: 'trượt tuyết', Cleaned Pred: 'tuyên chiến bắn băng tốc độ'
  [Sample 1] Raw GT: 'trượt tuyết', Raw Pred: 'Trượt tuyết'
  [Sample 1] Cleaned GT: 'trượt tuyết', Cleaned Pred: 'trượt tuyết'
  [Sample 2] Raw GT: 'trượt tuyết', Raw Pred: 'Tuyết tách'
  [Sample 2] Cleaned GT: 'trượt tuyết', Cleaned Pred: 'tuyết tách'
  [Sample 3] Raw GT: 'trượt tuyết', Raw Pred: 'Vượt tuyết'
  [Sample 3] Cleaned GT: 'trượt tuyết', Cleaned Pred: 'vượt tuyết'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.6823 -> Reward=0.3411
  [Sample 1] ROUGE-L=1.0000, BERTScore=0.6291 -> Reward=0.8146
  [Sample 2] ROUGE-L=0.5000, BERTScore=0.3307 -> Reward=0.4154
  [Sample 3] ROUGE-L=0.5000, BERTScore=1.0000 -> Reward=0.7500
   [Batch Stats] CLIP Min: 18.6104, CLIP Max: 19.9217
   [Sample 0] BERTScore=0.7510, CLIPRaw=19.92 (CLIPNorm=1.0000) -> Reward=0.8755
   [Sample 1] BERTScore=0.7582, CLIPRaw=19.01 (CLIPNorm=0.3018) -> Reward=0.5300
   [Sample 2] BERTScore=0.7467, CLIPRaw=18.70 (CLIPNorm=0.0689) -> Reward=0.4078
   [Sample 3] BERTScore=0.7511, CLIPRaw=18.61 (CLIPNorm=0.0000) -> Reward=0.3755
{'loss': 0.00113327, 'grad_norm': 0.85274583, 'learning_rate': 2.33e-06, 'reward': 1.90933275, 'reward_std': 0.27736458, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.58026314, 'rewards/CustomAccuracyReward/std': 0.23669924, 'rewards/CustomExplainationReward/mean': 0.54720306, 'rewards/CustomExplainationReward/std': 0.22875805, 'completions/mean_length': 201.0, 'completions/min_length': 188.0, 'completions/max_length': 225.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02833558, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '139/200', 'percentage': '69.50%', 'elapsed_time': '2h 37m 5s', 'remaining_time': '1h 8m 56s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014747}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 21.2547, CLIP Max: 21.7356
   [Sample 0] BERTScore=0.8516, CLIPRaw=21.43 (CLIPNorm=0.3735) -> Reward=0.6126
   [Sample 1] BERTScore=0.8091, CLIPRaw=21.74 (CLIPNorm=1.0000) -> Reward=0.9046
   [Sample 2] BERTScore=0.8561, CLIPRaw=21.50 (CLIPNorm=0.5106) -> Reward=0.6834
   [Sample 3] BERTScore=0.8637, CLIPRaw=21.25 (CLIPNorm=0.0000) -> Reward=0.4318
{'loss': 0.0008578, 'grad_norm': 1.00944114, 'learning_rate': 2.27e-06, 'reward': 2.57260609, 'reward_std': 0.2443611, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.65808487, 'rewards/CustomExplainationReward/std': 0.19548883, 'completions/mean_length': 116.75, 'completions/min_length': 96.0, 'completions/max_length': 133.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02144518, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '140/200', 'percentage': '70.00%', 'elapsed_time': '2h 38m 7s', 'remaining_time': '1h 7m 45s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014757}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.8693, CLIP Max: 20.6102
   [Sample 0] BERTScore=0.7221, CLIPRaw=20.48 (CLIPNorm=0.8258) -> Reward=0.7739
   [Sample 1] BERTScore=0.8316, CLIPRaw=20.52 (CLIPNorm=0.8844) -> Reward=0.8580
   [Sample 2] BERTScore=0.7840, CLIPRaw=19.87 (CLIPNorm=0.0000) -> Reward=0.3920
   [Sample 3] BERTScore=0.7838, CLIPRaw=20.61 (CLIPNorm=1.0000) -> Reward=0.8919
{'loss': 0.00081278, 'grad_norm': 1.02490449, 'learning_rate': 2.2e-06, 'reward': 2.66117549, 'reward_std': 0.28755853, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.72894037, 'rewards/CustomExplainationReward/std': 0.2300469, 'completions/mean_length': 146.5, 'completions/min_length': 125.0, 'completions/max_length': 186.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02031464, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '141/200', 'percentage': '70.50%', 'elapsed_time': '2h 39m 20s', 'remaining_time': '1h 6m 40s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014748}
  [Sample 0] Raw GT: 'phòng khách', Raw Pred: 'Phòng khách'
  [Sample 0] Cleaned GT: 'phòng khách', Cleaned Pred: 'phòng khách'
  [Sample 1] Raw GT: 'phòng khách', Raw Pred: 'Phòng khách'
  [Sample 1] Cleaned GT: 'phòng khách', Cleaned Pred: 'phòng khách'
  [Sample 2] Raw GT: 'phòng khách', Raw Pred: 'Phòng khách'
  [Sample 2] Cleaned GT: 'phòng khách', Cleaned Pred: 'phòng khách'
  [Sample 3] Raw GT: 'phòng khách', Raw Pred: 'Phòng khách'
  [Sample 3] Cleaned GT: 'phòng khách', Cleaned Pred: 'phòng khách'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.8744, CLIP Max: 21.4200
   [Sample 0] BERTScore=0.7292, CLIPRaw=21.35 (CLIPNorm=0.8769) -> Reward=0.8031
   [Sample 1] BERTScore=0.7456, CLIPRaw=20.87 (CLIPNorm=0.0000) -> Reward=0.3728
   [Sample 2] BERTScore=0.7380, CLIPRaw=21.42 (CLIPNorm=1.0000) -> Reward=0.8690
   [Sample 3] BERTScore=0.7501, CLIPRaw=21.15 (CLIPNorm=0.4994) -> Reward=0.6247
{'loss': 0.00140736, 'grad_norm': 0.94850117, 'learning_rate': 2.13e-06, 'reward': 2.58424735, 'reward_std': 0.27731293, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.66739786, 'rewards/CustomExplainationReward/std': 0.22185037, 'completions/mean_length': 210.25, 'completions/min_length': 175.0, 'completions/max_length': 244.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03518307, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '142/200', 'percentage': '71.00%', 'elapsed_time': '2h 40m 45s', 'remaining_time': '1h 5m 39s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014722}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.2364, CLIP Max: 21.2939
   [Sample 0] BERTScore=0.8323, CLIPRaw=20.34 (CLIPNorm=0.1012) -> Reward=0.4668
   [Sample 1] BERTScore=0.8064, CLIPRaw=21.29 (CLIPNorm=1.0000) -> Reward=0.9032
   [Sample 2] BERTScore=0.8211, CLIPRaw=20.24 (CLIPNorm=0.0000) -> Reward=0.4106
   [Sample 3] BERTScore=0.8110, CLIPRaw=21.13 (CLIPNorm=0.8476) -> Reward=0.8293
{'loss': 0.0010924, 'grad_norm': 0.80011028, 'learning_rate': 2.06e-06, 'reward': 2.56558657, 'reward_std': 0.31219754, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.6524694, 'rewards/CustomExplainationReward/std': 0.24975801, 'completions/mean_length': 181.75, 'completions/min_length': 165.0, 'completions/max_length': 195.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02732528, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '143/200', 'percentage': '71.50%', 'elapsed_time': '2h 42m 11s', 'remaining_time': '1h 4m 39s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014694}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 17.9500, CLIP Max: 18.8457
   [Sample 0] BERTScore=0.7073, CLIPRaw=18.85 (CLIPNorm=1.0000) -> Reward=0.8537
   [Sample 1] BERTScore=0.7532, CLIPRaw=18.45 (CLIPNorm=0.5607) -> Reward=0.6569
   [Sample 2] BERTScore=0.7084, CLIPRaw=17.95 (CLIPNorm=0.0000) -> Reward=0.3542
   [Sample 3] BERTScore=0.7543, CLIPRaw=18.58 (CLIPNorm=0.6998) -> Reward=0.7271
{'loss': 0.00147957, 'grad_norm': 1.0019964, 'learning_rate': 1.99e-06, 'reward': 2.55996084, 'reward_std': 0.26510075, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.64796889, 'rewards/CustomExplainationReward/std': 0.2120806, 'completions/mean_length': 127.25, 'completions/min_length': 111.0, 'completions/max_length': 160.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03700575, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '144/200', 'percentage': '72.00%', 'elapsed_time': '2h 43m 5s', 'remaining_time': '1h 3m 25s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014716}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Không'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.9129, CLIP Max: 21.5053
   [Sample 0] BERTScore=0.7980, CLIPRaw=21.32 (CLIPNorm=0.6844) -> Reward=0.7412
   [Sample 1] BERTScore=0.7787, CLIPRaw=21.47 (CLIPNorm=0.9480) -> Reward=0.8634
   [Sample 2] BERTScore=0.8064, CLIPRaw=21.51 (CLIPNorm=1.0000) -> Reward=0.9032
   [Sample 3] BERTScore=0.8588, CLIPRaw=20.91 (CLIPNorm=0.0000) -> Reward=0.4294
{'loss': 0.00112171, 'grad_norm': 1.0237627, 'learning_rate': 1.93e-06, 'reward': 2.39533806, 'reward_std': 0.46434316, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.78197873, 'rewards/CustomAccuracyReward/std': 0.43604261, 'rewards/CustomExplainationReward/mean': 0.73429179, 'rewards/CustomExplainationReward/std': 0.21462648, 'completions/mean_length': 162.0, 'completions/min_length': 126.0, 'completions/max_length': 194.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02804484, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '145/200', 'percentage': '72.50%', 'elapsed_time': '2h 44m 30s', 'remaining_time': '1h 2m 23s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014691}
  [Sample 0] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 0] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 1] Raw GT: 'bóng chày', Raw Pred: 'Bóng đá'
  [Sample 1] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng đá'
  [Sample 2] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 2] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 3] Raw GT: 'bóng chày', Raw Pred: 'Bóng chày'
  [Sample 3] Cleaned GT: 'bóng chày', Cleaned Pred: 'bóng chày'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.5000, BERTScore=0.5582 -> Reward=0.5291
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 15.3862, CLIP Max: 18.3275
   [Sample 0] BERTScore=0.7518, CLIPRaw=17.62 (CLIPNorm=0.7588) -> Reward=0.7553
   [Sample 1] BERTScore=0.7798, CLIPRaw=18.33 (CLIPNorm=1.0000) -> Reward=0.8899
   [Sample 2] BERTScore=0.7834, CLIPRaw=15.39 (CLIPNorm=0.0000) -> Reward=0.3917
   [Sample 3] BERTScore=0.7928, CLIPRaw=17.93 (CLIPNorm=0.8659) -> Reward=0.8293
{'loss': 0.00084457, 'grad_norm': 0.88176233, 'learning_rate': 1.86e-06, 'reward': 2.49855423, 'reward_std': 0.28216827, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.88226926, 'rewards/CustomAccuracyReward/std': 0.2354615, 'rewards/CustomExplainationReward/mean': 0.71657407, 'rewards/CustomExplainationReward/std': 0.2234579, 'completions/mean_length': 200.75, 'completions/min_length': 151.0, 'completions/max_length': 246.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02110336, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '146/200', 'percentage': '73.00%', 'elapsed_time': '2h 45m 59s', 'remaining_time': '1h 1m 23s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01466}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.5145, CLIP Max: 20.9204
   [Sample 0] BERTScore=0.7353, CLIPRaw=19.51 (CLIPNorm=0.0000) -> Reward=0.3676
   [Sample 1] BERTScore=0.7626, CLIPRaw=19.62 (CLIPNorm=0.0719) -> Reward=0.4173
   [Sample 2] BERTScore=0.7428, CLIPRaw=20.18 (CLIPNorm=0.4736) -> Reward=0.6082
   [Sample 3] BERTScore=0.8362, CLIPRaw=20.92 (CLIPNorm=1.0000) -> Reward=0.9181
{'loss': 0.0012705, 'grad_norm': 0.979222, 'learning_rate': 1.8e-06, 'reward': 2.47225332, 'reward_std': 0.31179044, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.57780266, 'rewards/CustomExplainationReward/std': 0.24943236, 'completions/mean_length': 146.25, 'completions/min_length': 117.0, 'completions/max_length': 160.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03176261, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0, 'global_step/max_steps': '147/200', 'percentage': '73.50%', 'elapsed_time': '2h 47m 1s', 'remaining_time': '1h 0m 13s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014669}
  [Sample 0] Raw GT: 'cái ghế', Raw Pred: 'Chân đế ghế'
  [Sample 0] Cleaned GT: 'cái ghế', Cleaned Pred: 'chân đế ghế'
  [Sample 1] Raw GT: 'cái ghế', Raw Pred: 'Chiếc ghế'
  [Sample 1] Cleaned GT: 'cái ghế', Cleaned Pred: 'chiếc ghế'
  [Sample 2] Raw GT: 'cái ghế', Raw Pred: 'Chiếc ghế da'
  [Sample 2] Cleaned GT: 'cái ghế', Cleaned Pred: 'chiếc ghế da'
  [Sample 3] Raw GT: 'cái ghế', Raw Pred: 'Chiếc ghế dài'
  [Sample 3] Cleaned GT: 'cái ghế', Cleaned Pred: 'chiếc ghế dài'
  [Sample 0] ROUGE-L=0.4150, BERTScore=0.4683 -> Reward=0.4416
  [Sample 1] ROUGE-L=0.5000, BERTScore=0.4238 -> Reward=0.4619
  [Sample 2] ROUGE-L=0.4150, BERTScore=0.3925 -> Reward=0.4037
  [Sample 3] ROUGE-L=0.4150, BERTScore=0.4090 -> Reward=0.4120
   [Batch Stats] CLIP Min: 18.3431, CLIP Max: 19.4031
   [Sample 0] BERTScore=0.7809, CLIPRaw=19.02 (CLIPNorm=0.6371) -> Reward=0.7090
   [Sample 1] BERTScore=0.7321, CLIPRaw=18.38 (CLIPNorm=0.0332) -> Reward=0.3826
   [Sample 2] BERTScore=0.7556, CLIPRaw=18.34 (CLIPNorm=0.0000) -> Reward=0.3778
   [Sample 3] BERTScore=0.8159, CLIPRaw=19.40 (CLIPNorm=1.0000) -> Reward=0.9079
{'loss': 0.00134602, 'grad_norm': 0.9167614, 'learning_rate': 1.74e-06, 'reward': 1.78019464, 'reward_std': 0.31839204, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.42980647, 'rewards/CustomAccuracyReward/std': 0.02687527, 'rewards/CustomExplainationReward/mean': 0.59434927, 'rewards/CustomExplainationReward/std': 0.2602697, 'completions/mean_length': 211.0, 'completions/min_length': 201.0, 'completions/max_length': 228.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03365309, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '148/200', 'percentage': '74.00%', 'elapsed_time': '2h 48m 9s', 'remaining_time': '59m 4s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014669}
  [Sample 0] Raw GT: 'trượt tuyết', Raw Pred: 'Trượt tuyết'
  [Sample 0] Cleaned GT: 'trượt tuyết', Cleaned Pred: 'trượt tuyết'
  [Sample 1] Raw GT: 'trượt tuyết', Raw Pred: 'Snowboarding'
  [Sample 1] Cleaned GT: 'trượt tuyết', Cleaned Pred: 'snowboarding'
  [Sample 2] Raw GT: 'trượt tuyết', Raw Pred: 'Trượt tuyết'
  [Sample 2] Cleaned GT: 'trượt tuyết', Cleaned Pred: 'trượt tuyết'
  [Sample 3] Raw GT: 'trượt tuyết', Raw Pred: 'Lướt tuyết'
  [Sample 3] Cleaned GT: 'trượt tuyết', Cleaned Pred: 'lướt tuyết'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.7076 -> Reward=0.3538
  [Sample 2] ROUGE-L=1.0000, BERTScore=0.1948 -> Reward=0.5974
  [Sample 3] ROUGE-L=0.5000, BERTScore=1.0000 -> Reward=0.7500
   [Batch Stats] CLIP Min: 15.0096, CLIP Max: 17.0071
   [Sample 0] BERTScore=0.8167, CLIPRaw=16.63 (CLIPNorm=0.8090) -> Reward=0.8129
   [Sample 1] BERTScore=0.8239, CLIPRaw=17.01 (CLIPNorm=1.0000) -> Reward=0.9120
   [Sample 2] BERTScore=0.7834, CLIPRaw=15.01 (CLIPNorm=0.0000) -> Reward=0.3917
   [Sample 3] BERTScore=0.8171, CLIPRaw=16.28 (CLIPNorm=0.6344) -> Reward=0.7257
{'loss': 0.0011998, 'grad_norm': 0.80823624, 'learning_rate': 1.67e-06, 'reward': 2.23234224, 'reward_std': 0.43435168, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.67530811, 'rewards/CustomAccuracyReward/std': 0.27106217, 'rewards/CustomExplainationReward/mean': 0.71056551, 'rewards/CustomExplainationReward/std': 0.22577946, 'completions/mean_length': 190.5, 'completions/min_length': 154.0, 'completions/max_length': 210.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0299815, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '149/200', 'percentage': '74.50%', 'elapsed_time': '2h 49m 36s', 'remaining_time': '58m 3s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014641}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 19.1771, CLIP Max: 20.0187
   [Sample 0] BERTScore=0.8484, CLIPRaw=19.18 (CLIPNorm=0.0000) -> Reward=0.4242
   [Sample 1] BERTScore=0.7809, CLIPRaw=20.02 (CLIPNorm=1.0000) -> Reward=0.8904
   [Sample 2] BERTScore=0.8211, CLIPRaw=19.64 (CLIPNorm=0.5473) -> Reward=0.6842
   [Sample 3] BERTScore=0.8166, CLIPRaw=19.41 (CLIPNorm=0.2811) -> Reward=0.5489
{'loss': 0.00121477, 'grad_norm': 0.97503209, 'learning_rate': 1.61e-06, 'reward': 1.72857738, 'reward_std': 0.4075425, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.34593609, 'rewards/CustomAccuracyReward/std': 0.43604264, 'rewards/CustomExplainationReward/mean': 0.63692582, 'rewards/CustomExplainationReward/std': 0.19959041, 'completions/mean_length': 177.75, 'completions/min_length': 156.0, 'completions/max_length': 190.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03037091, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '150/200', 'percentage': '75.00%', 'elapsed_time': '2h 50m 28s', 'remaining_time': '56m 49s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014664}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.0143, CLIP Max: 22.2990
   [Sample 0] BERTScore=0.7890, CLIPRaw=20.01 (CLIPNorm=0.0000) -> Reward=0.3945
   [Sample 1] BERTScore=0.7820, CLIPRaw=21.31 (CLIPNorm=0.5676) -> Reward=0.6748
   [Sample 2] BERTScore=0.7624, CLIPRaw=22.30 (CLIPNorm=1.0000) -> Reward=0.8812
   [Sample 3] BERTScore=0.7664, CLIPRaw=22.25 (CLIPNorm=0.9806) -> Reward=0.8735
{'loss': 0.00122955, 'grad_norm': 0.9569841, 'learning_rate': 1.55e-06, 'reward': 2.63249874, 'reward_std': 0.28572917, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.70599914, 'rewards/CustomExplainationReward/std': 0.22858341, 'completions/mean_length': 157.5, 'completions/min_length': 137.0, 'completions/max_length': 182.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03074811, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '151/200', 'percentage': '75.50%', 'elapsed_time': '2h 51m 45s', 'remaining_time': '55m 44s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014652}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.3668, CLIP Max: 19.6804
   [Sample 0] BERTScore=0.8211, CLIPRaw=18.61 (CLIPNorm=0.1828) -> Reward=0.5020
   [Sample 1] BERTScore=0.8422, CLIPRaw=19.68 (CLIPNorm=1.0000) -> Reward=0.9211
   [Sample 2] BERTScore=0.7082, CLIPRaw=18.59 (CLIPNorm=0.1674) -> Reward=0.4378
   [Sample 3] BERTScore=0.8121, CLIPRaw=18.37 (CLIPNorm=0.0000) -> Reward=0.4060
{'loss': 0.0011462, 'grad_norm': 0.94158226, 'learning_rate': 1.49e-06, 'reward': 2.45840406, 'reward_std': 0.29948354, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.56672323, 'rewards/CustomExplainationReward/std': 0.23958682, 'completions/mean_length': 131.75, 'completions/min_length': 121.0, 'completions/max_length': 154.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02865446, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '152/200', 'percentage': '76.00%', 'elapsed_time': '2h 53m 3s', 'remaining_time': '54m 39s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014638}
  [Sample 0] Raw GT: 'phòng tắm', Raw Pred: 'Phòng tắm'
  [Sample 0] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng tắm'
  [Sample 1] Raw GT: 'phòng tắm', Raw Pred: 'Ta phòng tắm'
  [Sample 1] Cleaned GT: 'phòng tắm', Cleaned Pred: 'ta phòng tắm'
  [Sample 2] Raw GT: 'phòng tắm', Raw Pred: 'Phòng tắm'
  [Sample 2] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng tắm'
  [Sample 3] Raw GT: 'phòng tắm', Raw Pred: 'Phòng tắm'
  [Sample 3] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng tắm'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.8299, BERTScore=1.0000 -> Reward=0.9150
  [Sample 2] ROUGE-L=1.0000, BERTScore=0.7607 -> Reward=0.8803
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.3999, CLIP Max: 19.8640
   [Sample 0] BERTScore=0.7721, CLIPRaw=18.40 (CLIPNorm=0.0000) -> Reward=0.3860
   [Sample 1] BERTScore=0.7430, CLIPRaw=19.16 (CLIPNorm=0.5208) -> Reward=0.6319
   [Sample 2] BERTScore=0.7685, CLIPRaw=19.86 (CLIPNorm=1.0000) -> Reward=0.8842
   [Sample 3] BERTScore=0.7666, CLIPRaw=19.80 (CLIPNorm=0.9556) -> Reward=0.8611
{'loss': 0.00109008, 'grad_norm': 0.93656057, 'learning_rate': 1.44e-06, 'reward': 2.54954243, 'reward_std': 0.26768246, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.94882417, 'rewards/CustomAccuracyReward/std': 0.06076093, 'rewards/CustomExplainationReward/mean': 0.69080961, 'rewards/CustomExplainationReward/std': 0.23293053, 'completions/mean_length': 159.5, 'completions/min_length': 126.0, 'completions/max_length': 195.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02724071, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '153/200', 'percentage': '76.50%', 'elapsed_time': '2h 54m 4s', 'remaining_time': '53m 28s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014649}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 16.5998, CLIP Max: 18.1546
   [Sample 0] BERTScore=0.8076, CLIPRaw=18.15 (CLIPNorm=1.0000) -> Reward=0.9038
   [Sample 1] BERTScore=0.8054, CLIPRaw=17.30 (CLIPNorm=0.4494) -> Reward=0.6274
   [Sample 2] BERTScore=0.7958, CLIPRaw=17.18 (CLIPNorm=0.3714) -> Reward=0.5836
   [Sample 3] BERTScore=0.8032, CLIPRaw=16.60 (CLIPNorm=0.0000) -> Reward=0.4016
{'loss': 0.00112793, 'grad_norm': 0.82723027, 'learning_rate': 1.38e-06, 'reward': 2.53638601, 'reward_std': 0.25951189, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.62910879, 'rewards/CustomExplainationReward/std': 0.20760955, 'completions/mean_length': 130.25, 'completions/min_length': 101.0, 'completions/max_length': 174.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0281915, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '154/200', 'percentage': '77.00%', 'elapsed_time': '2h 55m 16s', 'remaining_time': '52m 21s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014644}
  [Sample 0] Raw GT: 'đá', Raw Pred: 'Tảng đá'
  [Sample 0] Cleaned GT: 'đá', Cleaned Pred: 'tảng đá'
  [Sample 1] Raw GT: 'đá', Raw Pred: 'Tảng đá'
  [Sample 1] Cleaned GT: 'đá', Cleaned Pred: 'tảng đá'
  [Sample 2] Raw GT: 'đá', Raw Pred: 'Một khối đá'
  [Sample 2] Cleaned GT: 'đá', Cleaned Pred: 'một khối đá'
  [Sample 3] Raw GT: 'đá', Raw Pred: 'Bông hoa súp tel'
  [Sample 3] Cleaned GT: 'đá', Cleaned Pred: 'bông hoa súp tel'
  [Sample 0] ROUGE-L=0.7093, BERTScore=0.0217 -> Reward=0.3655
  [Sample 1] ROUGE-L=0.7093, BERTScore=0.0217 -> Reward=0.3655
  [Sample 2] ROUGE-L=0.5495, BERTScore=0.0126 -> Reward=0.2811
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.0121 -> Reward=0.0060
   [Batch Stats] CLIP Min: 19.5244, CLIP Max: 20.4976
   [Sample 0] BERTScore=0.8709, CLIPRaw=19.52 (CLIPNorm=0.0000) -> Reward=0.4355
   [Sample 1] BERTScore=0.8670, CLIPRaw=19.52 (CLIPNorm=0.0000) -> Reward=0.4335
   [Sample 2] BERTScore=0.8687, CLIPRaw=20.50 (CLIPNorm=1.0000) -> Reward=0.9344
   [Sample 3] BERTScore=0.9003, CLIPRaw=19.90 (CLIPNorm=0.3896) -> Reward=0.6450
{'loss': 0.00136724, 'grad_norm': 1.1562531, 'learning_rate': 1.32e-06, 'reward': 1.5832628, 'reward_std': 0.30364782, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.25453401, 'rewards/CustomAccuracyReward/std': 0.17038924, 'rewards/CustomExplainationReward/mean': 0.61207628, 'rewards/CustomExplainationReward/std': 0.23665747, 'completions/mean_length': 149.75, 'completions/min_length': 130.0, 'completions/max_length': 175.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03418439, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '155/200', 'percentage': '77.50%', 'elapsed_time': '2h 56m 36s', 'remaining_time': '51m 16s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014627}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 16.6685, CLIP Max: 20.1827
   [Sample 0] BERTScore=0.7861, CLIPRaw=18.32 (CLIPNorm=0.4702) -> Reward=0.6281
   [Sample 1] BERTScore=0.8073, CLIPRaw=16.67 (CLIPNorm=0.0000) -> Reward=0.4037
   [Sample 2] BERTScore=0.7848, CLIPRaw=18.33 (CLIPNorm=0.4733) -> Reward=0.6291
   [Sample 3] BERTScore=0.7428, CLIPRaw=20.18 (CLIPNorm=1.0000) -> Reward=0.8714
{'loss': 0.00111806, 'grad_norm': 0.91173732, 'learning_rate': 1.27e-06, 'reward': 2.26879597, 'reward_std': 0.59882355, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.78197873, 'rewards/CustomAccuracyReward/std': 0.43604261, 'rewards/CustomExplainationReward/mean': 0.63305801, 'rewards/CustomExplainationReward/std': 0.1910281, 'completions/mean_length': 169.25, 'completions/min_length': 133.0, 'completions/max_length': 213.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02794929, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '156/200', 'percentage': '78.00%', 'elapsed_time': '2h 57m 58s', 'remaining_time': '50m 11s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014609}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.1030, CLIP Max: 18.8705
   [Sample 0] BERTScore=0.7867, CLIPRaw=18.72 (CLIPNorm=0.7997) -> Reward=0.7932
   [Sample 1] BERTScore=0.7649, CLIPRaw=18.45 (CLIPNorm=0.4502) -> Reward=0.6075
   [Sample 2] BERTScore=0.7972, CLIPRaw=18.87 (CLIPNorm=1.0000) -> Reward=0.8986
   [Sample 3] BERTScore=0.7790, CLIPRaw=18.10 (CLIPNorm=0.0000) -> Reward=0.3895
{'loss': 0.001239, 'grad_norm': 0.97179323, 'learning_rate': 1.21e-06, 'reward': 2.59026289, 'reward_std': 0.27950236, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.6722104, 'rewards/CustomExplainationReward/std': 0.22360186, 'completions/mean_length': 162.25, 'completions/min_length': 144.0, 'completions/max_length': 185.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03098047, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '157/200', 'percentage': '78.50%', 'elapsed_time': '2h 58m 58s', 'remaining_time': '49m 1s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014621}
  [Sample 0] Raw GT: 'lò vi sóng', Raw Pred: 'Lò vi sóng'
  [Sample 0] Cleaned GT: 'lò vi sóng', Cleaned Pred: 'lò vi sóng'
  [Sample 1] Raw GT: 'lò vi sóng', Raw Pred: 'Lò vi sóng'
  [Sample 1] Cleaned GT: 'lò vi sóng', Cleaned Pred: 'lò vi sóng'
  [Sample 2] Raw GT: 'lò vi sóng', Raw Pred: 'Lò vi sóng'
  [Sample 2] Cleaned GT: 'lò vi sóng', Cleaned Pred: 'lò vi sóng'
  [Sample 3] Raw GT: 'lò vi sóng', Raw Pred: 'Lò vi sóng'
  [Sample 3] Cleaned GT: 'lò vi sóng', Cleaned Pred: 'lò vi sóng'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.8441, CLIP Max: 21.1059
   [Sample 0] BERTScore=0.7334, CLIPRaw=21.03 (CLIPNorm=0.9416) -> Reward=0.8375
   [Sample 1] BERTScore=0.7034, CLIPRaw=19.84 (CLIPNorm=0.0000) -> Reward=0.3517
   [Sample 2] BERTScore=0.7367, CLIPRaw=20.99 (CLIPNorm=0.9075) -> Reward=0.8221
   [Sample 3] BERTScore=0.7395, CLIPRaw=21.11 (CLIPNorm=1.0000) -> Reward=0.8697
{'loss': 0.00130546, 'grad_norm': 0.93311238, 'learning_rate': 1.16e-06, 'reward': 2.65032554, 'reward_std': 0.30812368, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.72026038, 'rewards/CustomExplainationReward/std': 0.246499, 'completions/mean_length': 165.0, 'completions/min_length': 146.0, 'completions/max_length': 185.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03263647, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '158/200', 'percentage': '79.00%', 'elapsed_time': '3h 0m 13s', 'remaining_time': '47m 54s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014611}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Không rõ ràng'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không rõ ràng'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Không rõ'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không rõ'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Có thể'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'có thể'
  [Sample 0] ROUGE-L=0.5495, BERTScore=0.4192 -> Reward=0.4844
  [Sample 1] ROUGE-L=1.0000, BERTScore=0.1606 -> Reward=0.5803
  [Sample 2] ROUGE-L=0.7093, BERTScore=0.2130 -> Reward=0.4611
  [Sample 3] ROUGE-L=0.0000, BERTScore=1.0000 -> Reward=0.5000
   [Batch Stats] CLIP Min: 19.3938, CLIP Max: 20.3504
   [Sample 0] BERTScore=0.7952, CLIPRaw=19.67 (CLIPNorm=0.2885) -> Reward=0.5418
   [Sample 1] BERTScore=0.8015, CLIPRaw=19.39 (CLIPNorm=0.0000) -> Reward=0.4007
   [Sample 2] BERTScore=0.8063, CLIPRaw=20.32 (CLIPNorm=0.9681) -> Reward=0.8872
   [Sample 3] BERTScore=0.7993, CLIPRaw=20.35 (CLIPNorm=1.0000) -> Reward=0.8996
{'loss': 0.00082897, 'grad_norm': 0.87383753, 'learning_rate': 1.11e-06, 'reward': 1.98601949, 'reward_std': 0.26956838, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.50645161, 'rewards/CustomAccuracyReward/std': 0.05175571, 'rewards/CustomExplainationReward/mean': 0.68236411, 'rewards/CustomExplainationReward/std': 0.25049591, 'completions/mean_length': 206.0, 'completions/min_length': 155.0, 'completions/max_length': 272.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02073222, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '159/200', 'percentage': '79.50%', 'elapsed_time': '3h 1m 42s', 'remaining_time': '46m 51s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014584}
  [Sample 0] Raw GT: 'thả diều', Raw Pred: 'Thả dù'
  [Sample 0] Cleaned GT: 'thả diều', Cleaned Pred: 'thả dù'
  [Sample 1] Raw GT: 'thả diều', Raw Pred: 'Cho dù'
  [Sample 1] Cleaned GT: 'thả diều', Cleaned Pred: 'cho dù'
  [Sample 2] Raw GT: 'thả diều', Raw Pred: 'Thả ô dù'
  [Sample 2] Cleaned GT: 'thả diều', Cleaned Pred: 'thả ô dù'
  [Sample 3] Raw GT: 'thả diều', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'thả diều', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=0.5000, BERTScore=0.4755 -> Reward=0.4877
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.3167 -> Reward=0.1584
  [Sample 2] ROUGE-L=0.4150, BERTScore=0.4760 -> Reward=0.4455
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.3506 -> Reward=0.1753
   [Batch Stats] CLIP Min: 19.2033, CLIP Max: 20.0113
   [Sample 0] BERTScore=0.8233, CLIPRaw=19.35 (CLIPNorm=0.1861) -> Reward=0.5047
   [Sample 1] BERTScore=0.8162, CLIPRaw=20.01 (CLIPNorm=0.9983) -> Reward=0.9072
   [Sample 2] BERTScore=0.8023, CLIPRaw=20.01 (CLIPNorm=1.0000) -> Reward=0.9011
   [Sample 3] BERTScore=0.8263, CLIPRaw=19.20 (CLIPNorm=0.0000) -> Reward=0.4132
{'loss': 0.0011346, 'grad_norm': 1.03003037, 'learning_rate': 1.05e-06, 'reward': 1.74785519, 'reward_std': 0.39122275, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.3167235, 'rewards/CustomAccuracyReward/std': 0.17407984, 'rewards/CustomExplainationReward/mean': 0.68156064, 'rewards/CustomExplainationReward/std': 0.25978124, 'completions/mean_length': 156.75, 'completions/min_length': 115.0, 'completions/max_length': 211.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0283598, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '160/200', 'percentage': '80.00%', 'elapsed_time': '3h 2m 58s', 'remaining_time': '45m 44s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014573}
  [Sample 0] Raw GT: 'nắng', Raw Pred: 'Không thể xác định'
  [Sample 0] Cleaned GT: 'nắng', Cleaned Pred: 'không thể xác định'
  [Sample 1] Raw GT: 'nắng', Raw Pred: 'Không thể xác định'
  [Sample 1] Cleaned GT: 'nắng', Cleaned Pred: 'không thể xác định'
  [Sample 2] Raw GT: 'nắng', Raw Pred: 'Vui lòng đợi'
  [Sample 2] Cleaned GT: 'nắng', Cleaned Pred: 'vui lòng đợi'
  [Sample 3] Raw GT: 'nắng', Raw Pred: 'No日'
  [Sample 3] Cleaned GT: 'nắng', Cleaned Pred: 'no日'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2230 -> Reward=0.1115
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2230 -> Reward=0.1115
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2646 -> Reward=0.1323
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2805 -> Reward=0.1402
   [Batch Stats] CLIP Min: 19.4616, CLIP Max: 20.5456
   [Sample 0] BERTScore=0.7931, CLIPRaw=19.69 (CLIPNorm=0.2110) -> Reward=0.5020
   [Sample 1] BERTScore=0.8145, CLIPRaw=20.47 (CLIPNorm=0.9312) -> Reward=0.8728
   [Sample 2] BERTScore=0.7808, CLIPRaw=19.46 (CLIPNorm=0.0000) -> Reward=0.3904
   [Sample 3] BERTScore=0.8068, CLIPRaw=20.55 (CLIPNorm=1.0000) -> Reward=0.9034
{'loss': 0.00096267, 'grad_norm': 0.97693223, 'learning_rate': 1e-06, 'reward': 1.4888072, 'reward_std': 0.32647806, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.12388129, 'rewards/CustomAccuracyReward/std': 0.01468353, 'rewards/CustomExplainationReward/mean': 0.66716456, 'rewards/CustomExplainationReward/std': 0.25945362, 'completions/mean_length': 106.75, 'completions/min_length': 92.0, 'completions/max_length': 118.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02406687, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '161/200', 'percentage': '80.50%', 'elapsed_time': '3h 3m 49s', 'remaining_time': '44m 31s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014597}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 21.0077, CLIP Max: 22.2683
   [Sample 0] BERTScore=0.8036, CLIPRaw=21.71 (CLIPNorm=0.5601) -> Reward=0.6819
   [Sample 1] BERTScore=0.7776, CLIPRaw=21.01 (CLIPNorm=0.0000) -> Reward=0.3888
   [Sample 2] BERTScore=0.7938, CLIPRaw=21.66 (CLIPNorm=0.5175) -> Reward=0.6557
   [Sample 3] BERTScore=0.8077, CLIPRaw=22.27 (CLIPNorm=1.0000) -> Reward=0.9038
{'loss': 0.00113389, 'grad_norm': 1.11645997, 'learning_rate': 9.5e-07, 'reward': 2.57192612, 'reward_std': 0.2636584, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.6575408, 'rewards/CustomExplainationReward/std': 0.21092673, 'completions/mean_length': 96.25, 'completions/min_length': 87.0, 'completions/max_length': 102.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02833685, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '162/200', 'percentage': '81.00%', 'elapsed_time': '3h 4m 48s', 'remaining_time': '43m 20s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01461}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có thể'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có thể'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=0.7093, BERTScore=0.2558 -> Reward=0.4826
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.3096 -> Reward=0.1548
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.8324, CLIP Max: 21.2859
   [Sample 0] BERTScore=0.7416, CLIPRaw=20.89 (CLIPNorm=0.1348) -> Reward=0.4382
   [Sample 1] BERTScore=0.7595, CLIPRaw=21.29 (CLIPNorm=1.0000) -> Reward=0.8797
   [Sample 2] BERTScore=0.7766, CLIPRaw=20.93 (CLIPNorm=0.2163) -> Reward=0.4965
   [Sample 3] BERTScore=0.7822, CLIPRaw=20.83 (CLIPNorm=0.0000) -> Reward=0.3911
{'loss': 0.00128159, 'grad_norm': 0.99515045, 'learning_rate': 9.1e-07, 'reward': 2.01341391, 'reward_std': 0.67591703, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.6593461, 'rewards/CustomAccuracyReward/std': 0.41548747, 'rewards/CustomExplainationReward/mean': 0.55138493, 'rewards/CustomExplainationReward/std': 0.22310859, 'completions/mean_length': 151.0, 'completions/min_length': 130.0, 'completions/max_length': 167.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03203848, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '163/200', 'percentage': '81.50%', 'elapsed_time': '3h 5m 42s', 'remaining_time': '42m 9s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014628}
  [Sample 0] Raw GT: 'visor', Raw Pred: 'Mũ lưỡi trai'
  [Sample 0] Cleaned GT: 'visor', Cleaned Pred: 'mũ lưỡi trai'
  [Sample 1] Raw GT: 'visor', Raw Pred: 'Mũ lưỡi trai'
  [Sample 1] Cleaned GT: 'visor', Cleaned Pred: 'mũ lưỡi trai'
  [Sample 2] Raw GT: 'visor', Raw Pred: 'Mũ xanh dương nhạt'
  [Sample 2] Cleaned GT: 'visor', Cleaned Pred: 'mũ xanh dương nhạt'
  [Sample 3] Raw GT: 'visor', Raw Pred: 'Mũ xanh dương'
  [Sample 3] Cleaned GT: 'visor', Cleaned Pred: 'mũ xanh dương'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2676 -> Reward=0.1338
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2676 -> Reward=0.1338
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2609 -> Reward=0.1304
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.3004 -> Reward=0.1502
   [Batch Stats] CLIP Min: 17.9397, CLIP Max: 19.9502
   [Sample 0] BERTScore=0.8412, CLIPRaw=18.69 (CLIPNorm=0.3733) -> Reward=0.6073
   [Sample 1] BERTScore=0.8521, CLIPRaw=19.95 (CLIPNorm=1.0000) -> Reward=0.9261
   [Sample 2] BERTScore=0.8158, CLIPRaw=17.94 (CLIPNorm=0.0000) -> Reward=0.4079
   [Sample 3] BERTScore=0.8311, CLIPRaw=19.14 (CLIPNorm=0.5946) -> Reward=0.7129
{'loss': 0.00124532, 'grad_norm': 1.01001978, 'learning_rate': 8.6e-07, 'reward': 1.50072753, 'reward_std': 0.27335224, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.13706255, 'rewards/CustomAccuracyReward/std': 0.00888788, 'rewards/CustomExplainationReward/mean': 0.66351944, 'rewards/CustomExplainationReward/std': 0.21593367, 'completions/mean_length': 152.0, 'completions/min_length': 151.0, 'completions/max_length': 153.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03113082, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '164/200', 'percentage': '82.00%', 'elapsed_time': '3h 6m 50s', 'remaining_time': '41m 0s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01463}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 15.8197, CLIP Max: 16.9107
   [Sample 0] BERTScore=0.7857, CLIPRaw=15.82 (CLIPNorm=0.0000) -> Reward=0.3928
   [Sample 1] BERTScore=0.7618, CLIPRaw=16.25 (CLIPNorm=0.3931) -> Reward=0.5774
   [Sample 2] BERTScore=0.7788, CLIPRaw=16.91 (CLIPNorm=1.0000) -> Reward=0.8894
   [Sample 3] BERTScore=0.7891, CLIPRaw=16.44 (CLIPNorm=0.5713) -> Reward=0.6802
{'loss': 0.00096923, 'grad_norm': 1.12586832, 'learning_rate': 8.1e-07, 'reward': 2.5437243, 'reward_std': 0.25892749, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.63497949, 'rewards/CustomExplainationReward/std': 0.20714192, 'completions/mean_length': 142.75, 'completions/min_length': 131.0, 'completions/max_length': 170.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02423606, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '165/200', 'percentage': '82.50%', 'elapsed_time': '3h 7m 45s', 'remaining_time': '39m 49s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014647}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 21.2965, CLIP Max: 21.8565
   [Sample 0] BERTScore=0.7731, CLIPRaw=21.86 (CLIPNorm=1.0000) -> Reward=0.8865
   [Sample 1] BERTScore=0.8153, CLIPRaw=21.75 (CLIPNorm=0.8067) -> Reward=0.8110
   [Sample 2] BERTScore=0.7814, CLIPRaw=21.48 (CLIPNorm=0.3219) -> Reward=0.5517
   [Sample 3] BERTScore=0.8276, CLIPRaw=21.30 (CLIPNorm=0.0000) -> Reward=0.4138
{'loss': 0.00139573, 'grad_norm': 1.03355789, 'learning_rate': 7.7e-07, 'reward': 2.58217812, 'reward_std': 0.27608073, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.6657424, 'rewards/CustomExplainationReward/std': 0.22086465, 'completions/mean_length': 158.25, 'completions/min_length': 132.0, 'completions/max_length': 201.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03488094, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '166/200', 'percentage': '83.00%', 'elapsed_time': '3h 9m 3s', 'remaining_time': '38m 43s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014634}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Nhất'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'nhất'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Không'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.4575 -> Reward=0.2287
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 21.0986, CLIP Max: 21.6679
   [Sample 0] BERTScore=0.7973, CLIPRaw=21.67 (CLIPNorm=1.0000) -> Reward=0.8986
   [Sample 1] BERTScore=0.7853, CLIPRaw=21.62 (CLIPNorm=0.9242) -> Reward=0.8547
   [Sample 2] BERTScore=0.7438, CLIPRaw=21.10 (CLIPNorm=0.0000) -> Reward=0.3719
   [Sample 3] BERTScore=0.7603, CLIPRaw=21.24 (CLIPNorm=0.2421) -> Reward=0.5012
{'loss': 0.00115262, 'grad_norm': 0.88307333, 'learning_rate': 7.3e-07, 'reward': 1.78469038, 'reward_std': 0.73073232, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.37114385, 'rewards/CustomAccuracyReward/std': 0.4219234, 'rewards/CustomExplainationReward/mean': 0.6566084, 'rewards/CustomExplainationReward/std': 0.26015371, 'completions/mean_length': 199.0, 'completions/min_length': 170.0, 'completions/max_length': 223.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02881557, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '167/200', 'percentage': '83.50%', 'elapsed_time': '3h 10m 11s', 'remaining_time': '37m 34s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014635}
  [Sample 0] Raw GT: 'mùa đông', Raw Pred: 'Mùa đông'
  [Sample 0] Cleaned GT: 'mùa đông', Cleaned Pred: 'mùa đông'
  [Sample 1] Raw GT: 'mùa đông', Raw Pred: 'Mùa đông'
  [Sample 1] Cleaned GT: 'mùa đông', Cleaned Pred: 'mùa đông'
  [Sample 2] Raw GT: 'mùa đông', Raw Pred: 'Mùa đông'
  [Sample 2] Cleaned GT: 'mùa đông', Cleaned Pred: 'mùa đông'
  [Sample 3] Raw GT: 'mùa đông', Raw Pred: 'Mùa đông'
  [Sample 3] Cleaned GT: 'mùa đông', Cleaned Pred: 'mùa đông'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 15.7954, CLIP Max: 17.4689
   [Sample 0] BERTScore=0.7613, CLIPRaw=17.47 (CLIPNorm=1.0000) -> Reward=0.8806
   [Sample 1] BERTScore=0.7626, CLIPRaw=15.80 (CLIPNorm=0.0000) -> Reward=0.3813
   [Sample 2] BERTScore=0.7406, CLIPRaw=17.00 (CLIPNorm=0.7209) -> Reward=0.7308
   [Sample 3] BERTScore=0.7204, CLIPRaw=16.68 (CLIPNorm=0.5305) -> Reward=0.6255
{'loss': 0.00097805, 'grad_norm': 0.90923113, 'learning_rate': 6.8e-07, 'reward': 2.56817579, 'reward_std': 0.26263618, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.6545406, 'rewards/CustomExplainationReward/std': 0.21010892, 'completions/mean_length': 135.75, 'completions/min_length': 95.0, 'completions/max_length': 152.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02444523, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '168/200', 'percentage': '84.00%', 'elapsed_time': '3h 11m 17s', 'remaining_time': '36m 26s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014638}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 18.6240, CLIP Max: 19.5898
   [Sample 0] BERTScore=0.7774, CLIPRaw=19.59 (CLIPNorm=0.9982) -> Reward=0.8878
   [Sample 1] BERTScore=0.7804, CLIPRaw=19.59 (CLIPNorm=1.0000) -> Reward=0.8902
   [Sample 2] BERTScore=0.7586, CLIPRaw=18.62 (CLIPNorm=0.0000) -> Reward=0.3793
   [Sample 3] BERTScore=0.7611, CLIPRaw=18.89 (CLIPNorm=0.2763) -> Reward=0.5187
{'loss': 0.0012992, 'grad_norm': 1.01727736, 'learning_rate': 6.4e-07, 'reward': 1.49612808, 'reward_std': 0.32542908, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.12791477, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.66898763, 'rewards/CustomExplainationReward/std': 0.26034325, 'completions/mean_length': 138.0, 'completions/min_length': 112.0, 'completions/max_length': 169.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03247438, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '169/200', 'percentage': '84.50%', 'elapsed_time': '3h 12m 12s', 'remaining_time': '35m 15s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014654}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 21.9768, CLIP Max: 22.7339
   [Sample 0] BERTScore=0.8504, CLIPRaw=22.11 (CLIPNorm=0.1700) -> Reward=0.5102
   [Sample 1] BERTScore=0.8970, CLIPRaw=22.31 (CLIPNorm=0.4425) -> Reward=0.6697
   [Sample 2] BERTScore=0.8708, CLIPRaw=22.73 (CLIPNorm=1.0000) -> Reward=0.9354
   [Sample 3] BERTScore=0.8833, CLIPRaw=21.98 (CLIPNorm=0.0000) -> Reward=0.4416
{'loss': 0.00116384, 'grad_norm': 0.94316733, 'learning_rate': 6e-07, 'reward': 2.5490489, 'reward_std': 0.27418384, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.63923919, 'rewards/CustomExplainationReward/std': 0.219347, 'completions/mean_length': 175.25, 'completions/min_length': 141.0, 'completions/max_length': 199.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02909614, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '170/200', 'percentage': '85.00%', 'elapsed_time': '3h 13m 30s', 'remaining_time': '34m 8s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014642}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.3004, CLIP Max: 21.2896
   [Sample 0] BERTScore=0.8617, CLIPRaw=21.29 (CLIPNorm=1.0000) -> Reward=0.9308
   [Sample 1] BERTScore=0.8412, CLIPRaw=20.30 (CLIPNorm=0.0000) -> Reward=0.4206
   [Sample 2] BERTScore=0.8358, CLIPRaw=21.11 (CLIPNorm=0.8147) -> Reward=0.8253
   [Sample 3] BERTScore=0.8223, CLIPRaw=20.41 (CLIPNorm=0.1153) -> Reward=0.4688
{'loss': 0.00072612, 'grad_norm': 0.89355534, 'learning_rate': 5.6e-07, 'reward': 2.57670641, 'reward_std': 0.31831932, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.66136491, 'rewards/CustomExplainationReward/std': 0.25465548, 'completions/mean_length': 142.0, 'completions/min_length': 129.0, 'completions/max_length': 170.0, 'completions/clipped_ratio': 0.0, 'kl': 0.01813851, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '171/200', 'percentage': '85.50%', 'elapsed_time': '3h 14m 25s', 'remaining_time': '32m 58s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014658}
  [Sample 0] Raw GT: 'frisbee', Raw Pred: 'Frisbee'
  [Sample 0] Cleaned GT: 'frisbee', Cleaned Pred: 'frisbee'
  [Sample 1] Raw GT: 'frisbee', Raw Pred: 'Frisbee'
  [Sample 1] Cleaned GT: 'frisbee', Cleaned Pred: 'frisbee'
  [Sample 2] Raw GT: 'frisbee', Raw Pred: 'Chơi cút kèn'
  [Sample 2] Cleaned GT: 'frisbee', Cleaned Pred: 'chơi cút kèn'
  [Sample 3] Raw GT: 'frisbee', Raw Pred: 'Bóng bay'
  [Sample 3] Cleaned GT: 'frisbee', Cleaned Pred: 'bóng bay'
  [Sample 0] ROUGE-L=1.0000, BERTScore=0.2906 -> Reward=0.6453
  [Sample 1] ROUGE-L=1.0000, BERTScore=0.2874 -> Reward=0.6437
  [Sample 2] ROUGE-L=0.0000, BERTScore=1.0000 -> Reward=0.5000
  [Sample 3] ROUGE-L=0.0000, BERTScore=1.0000 -> Reward=0.5000
   [Batch Stats] CLIP Min: 15.4698, CLIP Max: 17.4750
   [Sample 0] BERTScore=0.7887, CLIPRaw=16.31 (CLIPNorm=0.4180) -> Reward=0.6033
   [Sample 1] BERTScore=0.7905, CLIPRaw=17.48 (CLIPNorm=1.0000) -> Reward=0.8952
   [Sample 2] BERTScore=0.8141, CLIPRaw=17.06 (CLIPNorm=0.7953) -> Reward=0.8047
   [Sample 3] BERTScore=0.7844, CLIPRaw=15.47 (CLIPNorm=0.0000) -> Reward=0.3922
{'loss': 0.00110024, 'grad_norm': 0.95252657, 'learning_rate': 5.3e-07, 'reward': 2.05764723, 'reward_std': 0.33418599, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.57225132, 'rewards/CustomAccuracyReward/std': 0.08343128, 'rewards/CustomExplainationReward/mean': 0.67386639, 'rewards/CustomExplainationReward/std': 0.22392383, 'completions/mean_length': 183.25, 'completions/min_length': 170.0, 'completions/max_length': 194.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0274949, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '172/200', 'percentage': '86.00%', 'elapsed_time': '3h 15m 26s', 'remaining_time': '31m 48s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014668}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'không', Raw Pred: 'Nhận định'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'nhận định'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2051 -> Reward=0.1026
   [Batch Stats] CLIP Min: 19.3350, CLIP Max: 20.1090
   [Sample 0] BERTScore=0.7695, CLIPRaw=19.57 (CLIPNorm=0.2978) -> Reward=0.5337
   [Sample 1] BERTScore=0.8026, CLIPRaw=20.11 (CLIPNorm=1.0000) -> Reward=0.9013
   [Sample 2] BERTScore=0.7946, CLIPRaw=19.33 (CLIPNorm=0.0000) -> Reward=0.3973
   [Sample 3] BERTScore=0.7925, CLIPRaw=20.11 (CLIPNorm=0.9953) -> Reward=0.8939
{'loss': 0.00163958, 'grad_norm': 1.34873569, 'learning_rate': 4.9e-07, 'reward': 1.7764287, 'reward_std': 0.77408952, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.33960092, 'rewards/CustomAccuracyReward/std': 0.44042811, 'rewards/CustomExplainationReward/mean': 0.68154204, 'rewards/CustomExplainationReward/std': 0.25563639, 'completions/mean_length': 107.75, 'completions/min_length': 99.0, 'completions/max_length': 118.0, 'completions/clipped_ratio': 0.0, 'kl': 0.04099143, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '173/200', 'percentage': '86.50%', 'elapsed_time': '3h 16m 24s', 'remaining_time': '30m 39s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01468}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.5532, CLIP Max: 20.5381
   [Sample 0] BERTScore=0.7580, CLIPRaw=20.22 (CLIPNorm=0.6727) -> Reward=0.7153
   [Sample 1] BERTScore=0.7942, CLIPRaw=19.55 (CLIPNorm=0.0000) -> Reward=0.3971
   [Sample 2] BERTScore=0.7599, CLIPRaw=20.31 (CLIPNorm=0.7707) -> Reward=0.7653
   [Sample 3] BERTScore=0.7810, CLIPRaw=20.54 (CLIPNorm=1.0000) -> Reward=0.8905
{'loss': 0.00108665, 'grad_norm': 1.20660532, 'learning_rate': 4.5e-07, 'reward': 2.61508417, 'reward_std': 0.26247609, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.69206715, 'rewards/CustomExplainationReward/std': 0.20998082, 'completions/mean_length': 162.25, 'completions/min_length': 134.0, 'completions/max_length': 194.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02714905, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '174/200', 'percentage': '87.00%', 'elapsed_time': '3h 17m 24s', 'remaining_time': '29m 29s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01469}
  [Sample 0] Raw GT: 'phải', Raw Pred: 'Chỉ có một bàn tay'
  [Sample 0] Cleaned GT: 'phải', Cleaned Pred: 'chỉ có một bàn tay'
  [Sample 1] Raw GT: 'phải', Raw Pred: 'Trái tay'
  [Sample 1] Cleaned GT: 'phải', Cleaned Pred: 'trái tay'
  [Sample 2] Raw GT: 'phải', Raw Pred: 'Tay trái'
  [Sample 2] Cleaned GT: 'phải', Cleaned Pred: 'tay trái'
  [Sample 3] Raw GT: 'phải', Raw Pred: 'Trái tay'
  [Sample 3] Cleaned GT: 'phải', Cleaned Pred: 'trái tay'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.3527 -> Reward=0.1763
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2812 -> Reward=0.1406
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.1951 -> Reward=0.0976
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2812 -> Reward=0.1406
   [Batch Stats] CLIP Min: 20.2413, CLIP Max: 21.4005
   [Sample 0] BERTScore=0.8419, CLIPRaw=21.40 (CLIPNorm=1.0000) -> Reward=0.9209
   [Sample 1] BERTScore=0.8329, CLIPRaw=20.57 (CLIPNorm=0.2793) -> Reward=0.5561
   [Sample 2] BERTScore=0.8371, CLIPRaw=20.67 (CLIPNorm=0.3676) -> Reward=0.6024
   [Sample 3] BERTScore=0.8292, CLIPRaw=20.24 (CLIPNorm=0.0000) -> Reward=0.4146
{'loss': 0.00144492, 'grad_norm': 1.14661455, 'learning_rate': 4.2e-07, 'reward': 1.45285797, 'reward_std': 0.29162395, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.13877121, 'rewards/CustomAccuracyReward/std': 0.03222709, 'rewards/CustomExplainationReward/mean': 0.62351513, 'rewards/CustomExplainationReward/std': 0.2137661, 'completions/mean_length': 121.5, 'completions/min_length': 112.0, 'completions/max_length': 145.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03612017, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '175/200', 'percentage': '87.50%', 'elapsed_time': '3h 18m 14s', 'remaining_time': '28m 19s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014713}
  [Sample 0] Raw GT: 'hươu cao cổ', Raw Pred: 'Ngựa vằn'
  [Sample 0] Cleaned GT: 'hươu cao cổ', Cleaned Pred: 'ngựa vằn'
  [Sample 1] Raw GT: 'hươu cao cổ', Raw Pred: 'Ngựa hươu'
  [Sample 1] Cleaned GT: 'hươu cao cổ', Cleaned Pred: 'ngựa hươu'
  [Sample 2] Raw GT: 'hươu cao cổ', Raw Pred: 'Hươu cao cổ'
  [Sample 2] Cleaned GT: 'hươu cao cổ', Cleaned Pred: 'hươu cao cổ'
  [Sample 3] Raw GT: 'hươu cao cổ', Raw Pred: 'Hươu cao cổ'
  [Sample 3] Cleaned GT: 'hươu cao cổ', Cleaned Pred: 'hươu cao cổ'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.4228 -> Reward=0.2114
  [Sample 1] ROUGE-L=0.3861, BERTScore=0.5382 -> Reward=0.4622
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.2351, CLIP Max: 20.3736
   [Sample 0] BERTScore=0.8149, CLIPRaw=19.35 (CLIPNorm=0.1013) -> Reward=0.4581
   [Sample 1] BERTScore=0.8017, CLIPRaw=19.97 (CLIPNorm=0.6479) -> Reward=0.7248
   [Sample 2] BERTScore=0.8143, CLIPRaw=20.37 (CLIPNorm=1.0000) -> Reward=0.9071
   [Sample 3] BERTScore=0.8071, CLIPRaw=19.24 (CLIPNorm=0.0000) -> Reward=0.4035
{'loss': 0.00118231, 'grad_norm': 0.85364091, 'learning_rate': 3.9e-07, 'reward': 2.11473536, 'reward_std': 0.64117491, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.66838908, 'rewards/CustomAccuracyReward/std': 0.39636049, 'rewards/CustomExplainationReward/mean': 0.62339926, 'rewards/CustomExplainationReward/std': 0.23554105, 'completions/mean_length': 178.5, 'completions/min_length': 159.0, 'completions/max_length': 193.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02956273, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '176/200', 'percentage': '88.00%', 'elapsed_time': '3h 19m 4s', 'remaining_time': '27m 8s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014735}
  [Sample 0] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'có', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 23.0285, CLIP Max: 23.3988
   [Sample 0] BERTScore=0.7787, CLIPRaw=23.34 (CLIPNorm=0.8496) -> Reward=0.8142
   [Sample 1] BERTScore=0.7558, CLIPRaw=23.24 (CLIPNorm=0.5648) -> Reward=0.6603
   [Sample 2] BERTScore=0.7433, CLIPRaw=23.03 (CLIPNorm=0.0000) -> Reward=0.3716
   [Sample 3] BERTScore=0.7732, CLIPRaw=23.40 (CLIPNorm=1.0000) -> Reward=0.8866
{'loss': 0.00115833, 'grad_norm': 0.91386241, 'learning_rate': 3.6e-07, 'reward': 2.60397196, 'reward_std': 0.28515175, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.68317759, 'rewards/CustomExplainationReward/std': 0.22812136, 'completions/mean_length': 173.0, 'completions/min_length': 157.0, 'completions/max_length': 201.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0289687, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '177/200', 'percentage': '88.50%', 'elapsed_time': '3h 20m 6s', 'remaining_time': '26m 0s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014742}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Không'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 17.9782, CLIP Max: 18.6413
   [Sample 0] BERTScore=0.7824, CLIPRaw=18.57 (CLIPNorm=0.8910) -> Reward=0.8367
   [Sample 1] BERTScore=0.7417, CLIPRaw=18.63 (CLIPNorm=0.9871) -> Reward=0.8644
   [Sample 2] BERTScore=0.7651, CLIPRaw=17.98 (CLIPNorm=0.0000) -> Reward=0.3825
   [Sample 3] BERTScore=0.7805, CLIPRaw=18.64 (CLIPNorm=1.0000) -> Reward=0.8902
{'loss': 0.00122038, 'grad_norm': 0.97974014, 'learning_rate': 3.3e-07, 'reward': 1.86173987, 'reward_std': 0.72224337, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.34593609, 'rewards/CustomAccuracyReward/std': 0.43604264, 'rewards/CustomExplainationReward/mean': 0.74345577, 'rewards/CustomExplainationReward/std': 0.24161051, 'completions/mean_length': 176.5, 'completions/min_length': 138.0, 'completions/max_length': 224.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0305076, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '178/200', 'percentage': '89.00%', 'elapsed_time': '3h 21m 5s', 'remaining_time': '24m 51s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014753}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.7397, CLIP Max: 20.3001
   [Sample 0] BERTScore=0.8368, CLIPRaw=19.74 (CLIPNorm=0.0000) -> Reward=0.4184
   [Sample 1] BERTScore=0.8487, CLIPRaw=20.11 (CLIPNorm=0.6597) -> Reward=0.7542
   [Sample 2] BERTScore=0.8420, CLIPRaw=19.91 (CLIPNorm=0.3022) -> Reward=0.5721
   [Sample 3] BERTScore=0.8242, CLIPRaw=20.30 (CLIPNorm=1.0000) -> Reward=0.9121
{'loss': 0.0010024, 'grad_norm': 0.9577117, 'learning_rate': 3e-07, 'reward': 2.58024931, 'reward_std': 0.26853457, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.66419941, 'rewards/CustomExplainationReward/std': 0.21482764, 'completions/mean_length': 136.0, 'completions/min_length': 111.0, 'completions/max_length': 147.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02505432, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '179/200', 'percentage': '89.50%', 'elapsed_time': '3h 22m 9s', 'remaining_time': '23m 43s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014757}
  [Sample 0] Raw GT: 'pizza', Raw Pred: 'Pizza'
  [Sample 0] Cleaned GT: 'pizza', Cleaned Pred: 'pizza'
  [Sample 1] Raw GT: 'pizza', Raw Pred: 'Một chiếc pizza nhỏ'
  [Sample 1] Cleaned GT: 'pizza', Cleaned Pred: 'một chiếc pizza nhỏ'
  [Sample 2] Raw GT: 'pizza', Raw Pred: 'Bánh pizza'
  [Sample 2] Cleaned GT: 'pizza', Cleaned Pred: 'bánh pizza'
  [Sample 3] Raw GT: 'pizza', Raw Pred: 'Pizza'
  [Sample 3] Cleaned GT: 'pizza', Cleaned Pred: 'pizza'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.4485, BERTScore=0.4010 -> Reward=0.4248
  [Sample 2] ROUGE-L=0.7093, BERTScore=0.5638 -> Reward=0.6365
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.7375, CLIP Max: 19.9501
   [Sample 0] BERTScore=0.8021, CLIPRaw=19.95 (CLIPNorm=1.0000) -> Reward=0.9010
   [Sample 1] BERTScore=0.8137, CLIPRaw=18.74 (CLIPNorm=0.0000) -> Reward=0.4068
   [Sample 2] BERTScore=0.8034, CLIPRaw=19.35 (CLIPNorm=0.5059) -> Reward=0.6546
   [Sample 3] BERTScore=0.8018, CLIPRaw=19.73 (CLIPNorm=0.8149) -> Reward=0.8084
{'loss': 0.00124221, 'grad_norm': 1.08615685, 'learning_rate': 2.7e-07, 'reward': 2.3225565, 'reward_std': 0.62050253, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.76532412, 'rewards/CustomAccuracyReward/std': 0.28443918, 'rewards/CustomExplainationReward/mean': 0.69272125, 'rewards/CustomExplainationReward/std': 0.21598449, 'completions/mean_length': 143.75, 'completions/min_length': 116.0, 'completions/max_length': 165.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03105919, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '180/200', 'percentage': '90.00%', 'elapsed_time': '3h 23m 19s', 'remaining_time': '22m 35s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014755}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.4838, CLIP Max: 20.1128
   [Sample 0] BERTScore=0.8087, CLIPRaw=20.00 (CLIPNorm=0.9293) -> Reward=0.8690
   [Sample 1] BERTScore=0.8265, CLIPRaw=19.89 (CLIPNorm=0.8639) -> Reward=0.8452
   [Sample 2] BERTScore=0.8382, CLIPRaw=20.11 (CLIPNorm=1.0000) -> Reward=0.9191
   [Sample 3] BERTScore=0.8117, CLIPRaw=18.48 (CLIPNorm=0.0000) -> Reward=0.4058
{'loss': 0.0011839, 'grad_norm': 1.1175344, 'learning_rate': 2.4e-07, 'reward': 2.69973707, 'reward_std': 0.29745263, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.75978959, 'rewards/CustomExplainationReward/std': 0.23796208, 'completions/mean_length': 150.75, 'completions/min_length': 117.0, 'completions/max_length': 217.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02959682, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '181/200', 'percentage': '90.50%', 'elapsed_time': '3h 24m 41s', 'remaining_time': '21m 29s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014738}
  [Sample 0] Raw GT: 'có', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'có', Raw Pred: 'Không'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'có', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'có', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 20.3566, CLIP Max: 21.1388
   [Sample 0] BERTScore=0.7799, CLIPRaw=20.43 (CLIPNorm=0.0991) -> Reward=0.4395
   [Sample 1] BERTScore=0.7965, CLIPRaw=20.61 (CLIPNorm=0.3259) -> Reward=0.5612
   [Sample 2] BERTScore=0.7614, CLIPRaw=21.14 (CLIPNorm=1.0000) -> Reward=0.8807
   [Sample 3] BERTScore=0.8022, CLIPRaw=20.36 (CLIPNorm=0.0000) -> Reward=0.4011
{'loss': 0.0008343, 'grad_norm': 0.78079295, 'learning_rate': 2.2e-07, 'reward': 1.37317193, 'reward_std': 0.27209297, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.12791477, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.57062268, 'rewards/CustomExplainationReward/std': 0.21767436, 'completions/mean_length': 214.0, 'completions/min_length': 206.0, 'completions/max_length': 229.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02085261, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '182/200', 'percentage': '91.00%', 'elapsed_time': '3h 25m 51s', 'remaining_time': '20m 21s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014735}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.1517, CLIP Max: 19.9555
   [Sample 0] BERTScore=0.7446, CLIPRaw=19.15 (CLIPNorm=0.0000) -> Reward=0.3723
   [Sample 1] BERTScore=0.7566, CLIPRaw=19.96 (CLIPNorm=1.0000) -> Reward=0.8783
   [Sample 2] BERTScore=0.7792, CLIPRaw=19.72 (CLIPNorm=0.7130) -> Reward=0.7461
   [Sample 3] BERTScore=0.7450, CLIPRaw=19.37 (CLIPNorm=0.2754) -> Reward=0.5102
{'loss': 0.00088045, 'grad_norm': 1.03879178, 'learning_rate': 2e-07, 'reward': 1.9883492, 'reward_std': 0.37459618, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.56395739, 'rewards/CustomAccuracyReward/std': 0.50349867, 'rewards/CustomExplainationReward/mean': 0.62672198, 'rewards/CustomExplainationReward/std': 0.22791772, 'completions/mean_length': 186.25, 'completions/min_length': 146.0, 'completions/max_length': 216.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02201476, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '183/200', 'percentage': '91.50%', 'elapsed_time': '3h 27m 12s', 'remaining_time': '19m 14s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014719}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.6790, CLIP Max: 21.5663
   [Sample 0] BERTScore=0.7509, CLIPRaw=21.57 (CLIPNorm=1.0000) -> Reward=0.8754
   [Sample 1] BERTScore=0.7717, CLIPRaw=21.01 (CLIPNorm=0.3690) -> Reward=0.5703
   [Sample 2] BERTScore=0.7484, CLIPRaw=20.96 (CLIPNorm=0.3190) -> Reward=0.5337
   [Sample 3] BERTScore=0.7336, CLIPRaw=20.68 (CLIPNorm=0.0000) -> Reward=0.3668
{'loss': 0.00129178, 'grad_norm': 1.15608931, 'learning_rate': 1.7e-07, 'reward': 2.4832015, 'reward_std': 0.26497155, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.58656132, 'rewards/CustomExplainationReward/std': 0.21197732, 'completions/mean_length': 117.25, 'completions/min_length': 110.0, 'completions/max_length': 122.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03231313, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '184/200', 'percentage': '92.00%', 'elapsed_time': '3h 28m 12s', 'remaining_time': '18m 6s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014729}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Không'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 20.6195, CLIP Max: 20.9190
   [Sample 0] BERTScore=0.8080, CLIPRaw=20.92 (CLIPNorm=1.0000) -> Reward=0.9040
   [Sample 1] BERTScore=0.7725, CLIPRaw=20.75 (CLIPNorm=0.4422) -> Reward=0.6073
   [Sample 2] BERTScore=0.7576, CLIPRaw=20.62 (CLIPNorm=0.0000) -> Reward=0.3788
   [Sample 3] BERTScore=0.7703, CLIPRaw=20.91 (CLIPNorm=0.9848) -> Reward=0.8776
{'loss': 0.00087497, 'grad_norm': 0.89989734, 'learning_rate': 1.5e-07, 'reward': 2.34236264, 'reward_std': 0.44787961, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.78197873, 'rewards/CustomAccuracyReward/std': 0.43604261, 'rewards/CustomExplainationReward/mean': 0.69191146, 'rewards/CustomExplainationReward/std': 0.24808609, 'completions/mean_length': 178.0, 'completions/min_length': 160.0, 'completions/max_length': 207.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02187467, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '185/200', 'percentage': '92.50%', 'elapsed_time': '3h 29m 31s', 'remaining_time': '16m 59s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014716}
  [Sample 0] Raw GT: 'bông cải xanh', Raw Pred: 'Rau xanh'
  [Sample 0] Cleaned GT: 'bông cải xanh', Cleaned Pred: 'rau xanh'
  [Sample 1] Raw GT: 'bông cải xanh', Raw Pred: 'Bông cải xanh'
  [Sample 1] Cleaned GT: 'bông cải xanh', Cleaned Pred: 'bông cải xanh'
  [Sample 2] Raw GT: 'bông cải xanh', Raw Pred: 'Bông cải xanh'
  [Sample 2] Cleaned GT: 'bông cải xanh', Cleaned Pred: 'bông cải xanh'
  [Sample 3] Raw GT: 'bông cải xanh', Raw Pred: 'Bông cải'
  [Sample 3] Cleaned GT: 'bông cải xanh', Cleaned Pred: 'bông cải'
  [Sample 0] ROUGE-L=0.3861, BERTScore=0.6413 -> Reward=0.5137
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=0.7231 -> Reward=0.8616
  [Sample 3] ROUGE-L=0.7722, BERTScore=1.0000 -> Reward=0.8861
   [Batch Stats] CLIP Min: 19.3832, CLIP Max: 20.4029
   [Sample 0] BERTScore=0.7592, CLIPRaw=19.38 (CLIPNorm=0.0000) -> Reward=0.3796
   [Sample 1] BERTScore=0.7964, CLIPRaw=19.93 (CLIPNorm=0.5325) -> Reward=0.6645
   [Sample 2] BERTScore=0.7623, CLIPRaw=20.40 (CLIPNorm=1.0000) -> Reward=0.8812
   [Sample 3] BERTScore=0.8099, CLIPRaw=20.03 (CLIPNorm=0.6302) -> Reward=0.7200
{'loss': 0.0008669, 'grad_norm': 0.8926422, 'learning_rate': 1.3e-07, 'reward': 2.34581876, 'reward_std': 0.49114203, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.8153373, 'rewards/CustomAccuracyReward/std': 0.20994283, 'rewards/CustomExplainationReward/mean': 0.66131771, 'rewards/CustomExplainationReward/std': 0.2090895, 'completions/mean_length': 170.0, 'completions/min_length': 137.0, 'completions/max_length': 205.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02167539, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '186/200', 'percentage': '93.00%', 'elapsed_time': '3h 30m 51s', 'remaining_time': '15m 52s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014702}
  [Sample 0] Raw GT: 'không', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'không', Raw Pred: 'Không'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.2295, CLIP Max: 21.3272
   [Sample 0] BERTScore=0.7640, CLIPRaw=19.23 (CLIPNorm=0.0000) -> Reward=0.3820
   [Sample 1] BERTScore=0.7841, CLIPRaw=20.50 (CLIPNorm=0.6041) -> Reward=0.6941
   [Sample 2] BERTScore=0.7921, CLIPRaw=21.24 (CLIPNorm=0.9598) -> Reward=0.8760
   [Sample 3] BERTScore=0.7797, CLIPRaw=21.33 (CLIPNorm=1.0000) -> Reward=0.8898
{'loss': 0.00082102, 'grad_norm': 1.06451571, 'learning_rate': 1.2e-07, 'reward': 2.36556339, 'reward_std': 0.82633603, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.78197873, 'rewards/CustomAccuracyReward/std': 0.43604261, 'rewards/CustomExplainationReward/mean': 0.71047199, 'rewards/CustomExplainationReward/std': 0.23644656, 'completions/mean_length': 134.5, 'completions/min_length': 128.0, 'completions/max_length': 151.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02052516, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '187/200', 'percentage': '93.50%', 'elapsed_time': '3h 31m 43s', 'remaining_time': '14m 43s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01472}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
   [Batch Stats] CLIP Min: 18.7128, CLIP Max: 19.5253
   [Sample 0] BERTScore=0.8071, CLIPRaw=19.53 (CLIPNorm=1.0000) -> Reward=0.9035
   [Sample 1] BERTScore=0.7861, CLIPRaw=19.35 (CLIPNorm=0.7817) -> Reward=0.7839
   [Sample 2] BERTScore=0.8518, CLIPRaw=18.71 (CLIPNorm=0.0000) -> Reward=0.4259
   [Sample 3] BERTScore=0.8135, CLIPRaw=19.08 (CLIPNorm=0.4531) -> Reward=0.6333
{'loss': 0.00105456, 'grad_norm': 0.93119138, 'learning_rate': 1e-07, 'reward': 2.33580351, 'reward_std': 0.64177227, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.78197873, 'rewards/CustomAccuracyReward/std': 0.43604261, 'rewards/CustomExplainationReward/mean': 0.68666399, 'rewards/CustomExplainationReward/std': 0.20601678, 'completions/mean_length': 174.0, 'completions/min_length': 139.0, 'completions/max_length': 219.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02636174, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '188/200', 'percentage': '94.00%', 'elapsed_time': '3h 32m 54s', 'remaining_time': '13m 35s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014717}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Đúng'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.2558 -> Reward=0.1279
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 15.4236, CLIP Max: 16.9390
   [Sample 0] BERTScore=0.8062, CLIPRaw=16.94 (CLIPNorm=1.0000) -> Reward=0.9031
   [Sample 1] BERTScore=0.8244, CLIPRaw=16.00 (CLIPNorm=0.3771) -> Reward=0.6007
   [Sample 2] BERTScore=0.8104, CLIPRaw=15.94 (CLIPNorm=0.3401) -> Reward=0.5753
   [Sample 3] BERTScore=0.8357, CLIPRaw=15.42 (CLIPNorm=0.0000) -> Reward=0.4178
{'loss': 0.00127893, 'grad_norm': 1.26391637, 'learning_rate': 8e-08, 'reward': 2.25777555, 'reward_std': 0.32862034, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.78197873, 'rewards/CustomAccuracyReward/std': 0.43604261, 'rewards/CustomExplainationReward/mean': 0.62424177, 'rewards/CustomExplainationReward/std': 0.20273414, 'completions/mean_length': 135.25, 'completions/min_length': 110.0, 'completions/max_length': 161.0, 'completions/clipped_ratio': 0.0, 'kl': 0.0319756, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '189/200', 'percentage': '94.50%', 'elapsed_time': '3h 34m 3s', 'remaining_time': '12m 27s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014715}
  [Sample 0] Raw GT: 'trượt ván', Raw Pred: 'Xe thể thao'
  [Sample 0] Cleaned GT: 'trượt ván', Cleaned Pred: 'xe thể thao'
  [Sample 1] Raw GT: 'trượt ván', Raw Pred: 'Tê giác'
  [Sample 1] Cleaned GT: 'trượt ván', Cleaned Pred: 'tê giác'
  [Sample 2] Raw GT: 'trượt ván', Raw Pred: 'Xe lướt'
  [Sample 2] Cleaned GT: 'trượt ván', Cleaned Pred: 'xe lướt'
  [Sample 3] Raw GT: 'trượt ván', Raw Pred: 'Xe đạp leo trơn'
  [Sample 3] Cleaned GT: 'trượt ván', Cleaned Pred: 'xe đạp leo trơn'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.4753 -> Reward=0.2376
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.5014 -> Reward=0.2507
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.3801 -> Reward=0.1901
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.4611 -> Reward=0.2306
   [Batch Stats] CLIP Min: 17.7924, CLIP Max: 18.0809
   [Sample 0] BERTScore=0.7719, CLIPRaw=18.02 (CLIPNorm=0.7765) -> Reward=0.7742
   [Sample 1] BERTScore=0.7412, CLIPRaw=17.79 (CLIPNorm=0.0000) -> Reward=0.3706
   [Sample 2] BERTScore=0.7644, CLIPRaw=18.08 (CLIPNorm=1.0000) -> Reward=0.8822
   [Sample 3] BERTScore=0.7509, CLIPRaw=17.97 (CLIPNorm=0.6049) -> Reward=0.6779
{'loss': 0.00107008, 'grad_norm': 1.03410244, 'learning_rate': 7e-08, 'reward': 1.62931991, 'reward_std': 0.24989854, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.22723386, 'rewards/CustomAccuracyReward/std': 0.02614414, 'rewards/CustomExplainationReward/mean': 0.67622209, 'rewards/CustomExplainationReward/std': 0.22018732, 'completions/mean_length': 179.25, 'completions/min_length': 141.0, 'completions/max_length': 210.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02674879, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '190/200', 'percentage': '95.00%', 'elapsed_time': '3h 35m 8s', 'remaining_time': '11m 19s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014719}
  [Sample 0] Raw GT: 'bên ngoài', Raw Pred: 'Căn nhà'
  [Sample 0] Cleaned GT: 'bên ngoài', Cleaned Pred: 'căn nhà'
  [Sample 1] Raw GT: 'bên ngoài', Raw Pred: 'Không có phòng nào.'
  [Sample 1] Cleaned GT: 'bên ngoài', Cleaned Pred: 'không có phòng nào'
  [Sample 2] Raw GT: 'bên ngoài', Raw Pred: 'Không xác định được'
  [Sample 2] Cleaned GT: 'bên ngoài', Cleaned Pred: 'không xác định được'
  [Sample 3] Raw GT: 'bên ngoài', Raw Pred: 'Không thể xác định'
  [Sample 3] Cleaned GT: 'bên ngoài', Cleaned Pred: 'không thể xác định'
  [Sample 0] ROUGE-L=0.0000, BERTScore=0.3987 -> Reward=0.1994
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.3280 -> Reward=0.1640
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.3132 -> Reward=0.1566
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.2765 -> Reward=0.1383
   [Batch Stats] CLIP Min: 21.2106, CLIP Max: 21.9043
   [Sample 0] BERTScore=0.7977, CLIPRaw=21.21 (CLIPNorm=0.0000) -> Reward=0.3989
   [Sample 1] BERTScore=0.8146, CLIPRaw=21.90 (CLIPNorm=1.0000) -> Reward=0.9073
   [Sample 2] BERTScore=0.7724, CLIPRaw=21.66 (CLIPNorm=0.6437) -> Reward=0.7080
   [Sample 3] BERTScore=0.7973, CLIPRaw=21.27 (CLIPNorm=0.0813) -> Reward=0.4393
{'loss': 0.00136526, 'grad_norm': 0.99314272, 'learning_rate': 6e-08, 'reward': 1.47239721, 'reward_std': 0.29406857, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.16454355, 'rewards/CustomAccuracyReward/std': 0.02560173, 'rewards/CustomExplainationReward/mean': 0.61337423, 'rewards/CustomExplainationReward/std': 0.23922302, 'completions/mean_length': 136.75, 'completions/min_length': 110.0, 'completions/max_length': 170.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03413114, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '191/200', 'percentage': '95.50%', 'elapsed_time': '3h 36m 24s', 'remaining_time': '10m 11s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01471}
  [Sample 0] Raw GT: 'chó', Raw Pred: 'Một con chó'
  [Sample 0] Cleaned GT: 'chó', Cleaned Pred: 'một con chó'
  [Sample 1] Raw GT: 'chó', Raw Pred: 'Chó'
  [Sample 1] Cleaned GT: 'chó', Cleaned Pred: 'chó'
  [Sample 2] Raw GT: 'chó', Raw Pred: 'Chó'
  [Sample 2] Cleaned GT: 'chó', Cleaned Pred: 'chó'
  [Sample 3] Raw GT: 'chó', Raw Pred: 'Chó'
  [Sample 3] Cleaned GT: 'chó', Cleaned Pred: 'chó'
  [Sample 0] ROUGE-L=0.5495, BERTScore=1.0000 -> Reward=0.7748
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=0.4322 -> Reward=0.7161
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.1267, CLIP Max: 19.1334
   [Sample 0] BERTScore=0.7665, CLIPRaw=19.13 (CLIPNorm=1.0000) -> Reward=0.8833
   [Sample 1] BERTScore=0.7309, CLIPRaw=18.83 (CLIPNorm=0.6997) -> Reward=0.7153
   [Sample 2] BERTScore=0.7481, CLIPRaw=18.13 (CLIPNorm=0.0000) -> Reward=0.3740
   [Sample 3] BERTScore=0.7809, CLIPRaw=18.27 (CLIPNorm=0.1399) -> Reward=0.4604
{'loss': 0.0015371, 'grad_norm': 1.17375767, 'learning_rate': 4e-08, 'reward': 2.35118747, 'reward_std': 0.35314304, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.87271369, 'rewards/CustomAccuracyReward/std': 0.14891808, 'rewards/CustomExplainationReward/mean': 0.60823643, 'rewards/CustomExplainationReward/std': 0.23367201, 'completions/mean_length': 149.75, 'completions/min_length': 129.0, 'completions/max_length': 170.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03843658, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '192/200', 'percentage': '96.00%', 'elapsed_time': '3h 37m 36s', 'remaining_time': '9m 4s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014706}
  [Sample 0] Raw GT: 'phòng ngủ', Raw Pred: 'Có thể là phòng ngủ hoặc phòng nghỉ'
  [Sample 0] Cleaned GT: 'phòng ngủ', Cleaned Pred: 'có thể là phòng ngủ hoặc phòng nghỉ'
  [Sample 1] Raw GT: 'phòng ngủ', Raw Pred: 'Căn hộ'
  [Sample 1] Cleaned GT: 'phòng ngủ', Cleaned Pred: 'căn hộ'
  [Sample 2] Raw GT: 'phòng ngủ', Raw Pred: 'Phòng khách'
  [Sample 2] Cleaned GT: 'phòng ngủ', Cleaned Pred: 'phòng khách'
  [Sample 3] Raw GT: 'phòng ngủ', Raw Pred: 'Sản phẩm'
  [Sample 3] Cleaned GT: 'phòng ngủ', Cleaned Pred: 'sản phẩm'
  [Sample 0] ROUGE-L=0.4485, BERTScore=0.7404 -> Reward=0.5945
  [Sample 1] ROUGE-L=0.0000, BERTScore=0.3621 -> Reward=0.1811
  [Sample 2] ROUGE-L=0.5000, BERTScore=0.4794 -> Reward=0.4897
  [Sample 3] ROUGE-L=0.0000, BERTScore=0.4778 -> Reward=0.2389
   [Batch Stats] CLIP Min: 20.6553, CLIP Max: 22.0530
   [Sample 0] BERTScore=0.7959, CLIPRaw=20.95 (CLIPNorm=0.2105) -> Reward=0.5032
   [Sample 1] BERTScore=0.8395, CLIPRaw=20.66 (CLIPNorm=0.0053) -> Reward=0.4224
   [Sample 2] BERTScore=0.8076, CLIPRaw=20.66 (CLIPNorm=0.0000) -> Reward=0.4038
   [Sample 3] BERTScore=0.7574, CLIPRaw=22.05 (CLIPNorm=1.0000) -> Reward=0.8787
{'loss': 0.00128284, 'grad_norm': 1.08327675, 'learning_rate': 3e-08, 'reward': 1.66008973, 'reward_std': 0.29865763, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.37604159, 'rewards/CustomAccuracyReward/std': 0.19786999, 'rewards/CustomExplainationReward/mean': 0.55203021, 'rewards/CustomExplainationReward/std': 0.22201602, 'completions/mean_length': 144.25, 'completions/min_length': 101.0, 'completions/max_length': 178.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03207016, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '193/200', 'percentage': '96.50%', 'elapsed_time': '3h 38m 48s', 'remaining_time': '7m 56s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014701}
  [Sample 0] Raw GT: 'ngựa vằn', Raw Pred: 'Ngựa vằn'
  [Sample 0] Cleaned GT: 'ngựa vằn', Cleaned Pred: 'ngựa vằn'
  [Sample 1] Raw GT: 'ngựa vằn', Raw Pred: 'Ngựa vằn'
  [Sample 1] Cleaned GT: 'ngựa vằn', Cleaned Pred: 'ngựa vằn'
  [Sample 2] Raw GT: 'ngựa vằn', Raw Pred: 'Hươu cao cổ'
  [Sample 2] Cleaned GT: 'ngựa vằn', Cleaned Pred: 'hươu cao cổ'
  [Sample 3] Raw GT: 'ngựa vằn', Raw Pred: 'Vằn sơn trắng'
  [Sample 3] Cleaned GT: 'ngựa vằn', Cleaned Pred: 'vằn sơn trắng'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=0.0000, BERTScore=0.4228 -> Reward=0.2114
  [Sample 3] ROUGE-L=0.4150, BERTScore=0.5135 -> Reward=0.4643
   [Batch Stats] CLIP Min: 21.6346, CLIP Max: 22.0214
   [Sample 0] BERTScore=0.7669, CLIPRaw=22.02 (CLIPNorm=1.0000) -> Reward=0.8834
   [Sample 1] BERTScore=0.7766, CLIPRaw=21.69 (CLIPNorm=0.1326) -> Reward=0.4546
   [Sample 2] BERTScore=0.7811, CLIPRaw=21.98 (CLIPNorm=0.8880) -> Reward=0.8345
   [Sample 3] BERTScore=0.8332, CLIPRaw=21.63 (CLIPNorm=0.0000) -> Reward=0.4166
{'loss': 0.00081785, 'grad_norm': 0.99614114, 'learning_rate': 2e-08, 'reward': 2.14524174, 'reward_std': 0.56064439, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.66891158, 'rewards/CustomAccuracyReward/std': 0.39599928, 'rewards/CustomExplainationReward/mean': 0.647282, 'rewards/CustomExplainationReward/std': 0.2457552, 'completions/mean_length': 180.5, 'completions/min_length': 159.0, 'completions/max_length': 224.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02045574, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '194/200', 'percentage': '97.00%', 'elapsed_time': '3h 40m 11s', 'remaining_time': '6m 48s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014684}
  [Sample 0] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 0] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 1] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 1] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 2] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 2] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 3] Raw GT: 'đúng', Raw Pred: 'Có'
  [Sample 3] Cleaned GT: 'có', Cleaned Pred: 'có'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 21.0035, CLIP Max: 21.9723
   [Sample 0] BERTScore=0.7772, CLIPRaw=21.46 (CLIPNorm=0.4701) -> Reward=0.6237
   [Sample 1] BERTScore=0.8034, CLIPRaw=21.09 (CLIPNorm=0.0850) -> Reward=0.4442
   [Sample 2] BERTScore=0.8114, CLIPRaw=21.97 (CLIPNorm=1.0000) -> Reward=0.9057
   [Sample 3] BERTScore=0.8071, CLIPRaw=21.00 (CLIPNorm=0.0000) -> Reward=0.4036
{'loss': 0.00110374, 'grad_norm': 0.98102731, 'learning_rate': 2e-08, 'reward': 2.49285793, 'reward_std': 0.28573212, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.59428614, 'rewards/CustomExplainationReward/std': 0.22858569, 'completions/mean_length': 154.25, 'completions/min_length': 127.0, 'completions/max_length': 180.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02758289, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '195/200', 'percentage': '97.50%', 'elapsed_time': '3h 41m 26s', 'remaining_time': '5m 40s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014677}
  [Sample 0] Raw GT: 'phòng tắm', Raw Pred: 'Phòng tắm'
  [Sample 0] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng tắm'
  [Sample 1] Raw GT: 'phòng tắm', Raw Pred: 'Phòng tắm'
  [Sample 1] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng tắm'
  [Sample 2] Raw GT: 'phòng tắm', Raw Pred: 'Phòng nhà vệ sinh'
  [Sample 2] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng nhà vệ sinh'
  [Sample 3] Raw GT: 'phòng tắm', Raw Pred: 'Phòng tắm'
  [Sample 3] Cleaned GT: 'phòng tắm', Cleaned Pred: 'phòng tắm'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=0.3547, BERTScore=0.4624 -> Reward=0.4085
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.6814, CLIP Max: 19.7267
   [Sample 0] BERTScore=0.8396, CLIPRaw=18.68 (CLIPNorm=0.0000) -> Reward=0.4198
   [Sample 1] BERTScore=0.8295, CLIPRaw=19.06 (CLIPNorm=0.3652) -> Reward=0.5974
   [Sample 2] BERTScore=0.8450, CLIPRaw=18.83 (CLIPNorm=0.1402) -> Reward=0.4926
   [Sample 3] BERTScore=0.8405, CLIPRaw=19.73 (CLIPNorm=1.0000) -> Reward=0.9202
{'loss': 0.00185856, 'grad_norm': 0.79199946, 'learning_rate': 1e-08, 'reward': 2.32454586, 'reward_std': 0.53259349, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.85212553, 'rewards/CustomAccuracyReward/std': 0.29574892, 'rewards/CustomExplainationReward/mean': 0.60751116, 'rewards/CustomExplainationReward/std': 0.2208557, 'completions/mean_length': 147.5, 'completions/min_length': 132.0, 'completions/max_length': 162.0, 'completions/clipped_ratio': 0.0, 'kl': 0.04646626, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '196/200', 'percentage': '98.00%', 'elapsed_time': '3h 42m 20s', 'remaining_time': '4m 32s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014692}
  [Sample 0] Raw GT: 'gạch', Raw Pred: 'Gạch và tôn'
  [Sample 0] Cleaned GT: 'gạch', Cleaned Pred: 'gạch và tôn'
  [Sample 1] Raw GT: 'gạch', Raw Pred: 'Gạch'
  [Sample 1] Cleaned GT: 'gạch', Cleaned Pred: 'gạch'
  [Sample 2] Raw GT: 'gạch', Raw Pred: 'Gạch'
  [Sample 2] Cleaned GT: 'gạch', Cleaned Pred: 'gạch'
  [Sample 3] Raw GT: 'gạch', Raw Pred: 'Gạch'
  [Sample 3] Cleaned GT: 'gạch', Cleaned Pred: 'gạch'
  [Sample 0] ROUGE-L=0.5495, BERTScore=1.0000 -> Reward=0.7748
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=0.5651 -> Reward=0.7825
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.3341, CLIP Max: 20.0264
   [Sample 0] BERTScore=0.7696, CLIPRaw=18.33 (CLIPNorm=0.0000) -> Reward=0.3848
   [Sample 1] BERTScore=0.7841, CLIPRaw=20.03 (CLIPNorm=1.0000) -> Reward=0.8920
   [Sample 2] BERTScore=0.7906, CLIPRaw=19.06 (CLIPNorm=0.4319) -> Reward=0.6113
   [Sample 3] BERTScore=0.7825, CLIPRaw=18.55 (CLIPNorm=0.1291) -> Reward=0.4558
{'loss': 0.00090361, 'grad_norm': 0.96584868, 'learning_rate': 1e-08, 'reward': 2.34412313, 'reward_std': 0.38213667, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 0.88932878, 'rewards/CustomAccuracyReward/std': 0.12783144, 'rewards/CustomExplainationReward/mean': 0.58596969, 'rewards/CustomExplainationReward/std': 0.22489506, 'completions/mean_length': 144.5, 'completions/min_length': 128.0, 'completions/max_length': 160.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02258954, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '197/200', 'percentage': '98.50%', 'elapsed_time': '3h 43m 28s', 'remaining_time': '3m 24s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014693}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.9318, CLIP Max: 20.1966
   [Sample 0] BERTScore=0.8707, CLIPRaw=20.20 (CLIPNorm=1.0000) -> Reward=0.9354
   [Sample 1] BERTScore=0.7954, CLIPRaw=20.02 (CLIPNorm=0.8565) -> Reward=0.8260
   [Sample 2] BERTScore=0.8458, CLIPRaw=18.93 (CLIPNorm=0.0000) -> Reward=0.4229
   [Sample 3] BERTScore=0.8576, CLIPRaw=20.14 (CLIPNorm=0.9543) -> Reward=0.9060
{'loss': 0.00177918, 'grad_norm': 1.30386031, 'learning_rate': 0.0, 'reward': 2.71568489, 'reward_std': 0.29705444, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.77254784, 'rewards/CustomExplainationReward/std': 0.2376435, 'completions/mean_length': 109.0, 'completions/min_length': 96.0, 'completions/max_length': 132.0, 'completions/clipped_ratio': 0.0, 'kl': 0.04446509, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '198/200', 'percentage': '99.00%', 'elapsed_time': '3h 44m 15s', 'remaining_time': '2m 15s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014716}
  [Sample 0] Raw GT: 'phòng khách', Raw Pred: 'Phòng khách'
  [Sample 0] Cleaned GT: 'phòng khách', Cleaned Pred: 'phòng khách'
  [Sample 1] Raw GT: 'phòng khách', Raw Pred: 'Phòng khách'
  [Sample 1] Cleaned GT: 'phòng khách', Cleaned Pred: 'phòng khách'
  [Sample 2] Raw GT: 'phòng khách', Raw Pred: 'Phòng khách'
  [Sample 2] Cleaned GT: 'phòng khách', Cleaned Pred: 'phòng khách'
  [Sample 3] Raw GT: 'phòng khách', Raw Pred: 'Phòng khách'
  [Sample 3] Cleaned GT: 'phòng khách', Cleaned Pred: 'phòng khách'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 18.5206, CLIP Max: 19.2340
   [Sample 0] BERTScore=0.7541, CLIPRaw=18.80 (CLIPNorm=0.3872) -> Reward=0.5706
   [Sample 1] BERTScore=0.7747, CLIPRaw=18.52 (CLIPNorm=0.0000) -> Reward=0.3873
   [Sample 2] BERTScore=0.8152, CLIPRaw=18.58 (CLIPNorm=0.0792) -> Reward=0.4472
   [Sample 3] BERTScore=0.7908, CLIPRaw=19.23 (CLIPNorm=1.0000) -> Reward=0.8954
{'loss': 0.0013656, 'grad_norm': 0.85074466, 'learning_rate': 0.0, 'reward': 2.46892548, 'reward_std': 0.28343299, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.57514036, 'rewards/CustomExplainationReward/std': 0.22674644, 'completions/mean_length': 204.25, 'completions/min_length': 176.0, 'completions/max_length': 232.0, 'completions/clipped_ratio': 0.0, 'kl': 0.03414028, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '199/200', 'percentage': '99.50%', 'elapsed_time': '3h 45m 25s', 'remaining_time': '1m 7s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014713}
  [Sample 0] Raw GT: 'không', Raw Pred: 'No'
  [Sample 0] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 1] Raw GT: 'không', Raw Pred: 'No'
  [Sample 1] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 2] Raw GT: 'không', Raw Pred: 'No'
  [Sample 2] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 3] Raw GT: 'không', Raw Pred: 'No'
  [Sample 3] Cleaned GT: 'không', Cleaned Pred: 'không'
  [Sample 0] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 1] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 2] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
  [Sample 3] ROUGE-L=1.0000, BERTScore=1.0000 -> Reward=1.0000
   [Batch Stats] CLIP Min: 19.5212, CLIP Max: 21.4200
   [Sample 0] BERTScore=0.7775, CLIPRaw=21.42 (CLIPNorm=1.0000) -> Reward=0.8887
   [Sample 1] BERTScore=0.7252, CLIPRaw=21.13 (CLIPNorm=0.8477) -> Reward=0.7864
   [Sample 2] BERTScore=0.7646, CLIPRaw=19.52 (CLIPNorm=0.0000) -> Reward=0.3823
   [Sample 3] BERTScore=0.7669, CLIPRaw=19.89 (CLIPNorm=0.1964) -> Reward=0.4817
{'loss': 0.00110154, 'grad_norm': 1.02895379, 'learning_rate': 0.0, 'reward': 2.54348326, 'reward_std': 0.30163616, 'frac_reward_zero_std': 0.0, 'rewards/CustomFormatReward_ViVQA_X/mean': 1.0, 'rewards/CustomFormatReward_ViVQA_X/std': 0.0, 'rewards/CustomAccuracyReward/mean': 1.0, 'rewards/CustomAccuracyReward/std': 0.0, 'rewards/CustomExplainationReward/mean': 0.63478655, 'rewards/CustomExplainationReward/std': 0.24130888, 'completions/mean_length': 144.0, 'completions/min_length': 133.0, 'completions/max_length': 152.0, 'completions/clipped_ratio': 0.0, 'kl': 0.02753511, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01, 'global_step/max_steps': '200/200', 'percentage': '100.00%', 'elapsed_time': '3h 46m 17s', 'remaining_time': '0s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.01473}
{'train_runtime': 13582.0167, 'train_samples_per_second': 0.059, 'train_steps_per_second': 0.015, 'train_loss': 0.00103388, 'epoch': 0.01, 'global_step/max_steps': '200/200', 'percentage': '100.00%', 'elapsed_time': '3h 46m 20s', 'remaining_time': '0s', 'memory(GiB)': 15.7, 'train_speed(iter/s)': 0.014727}
[1;34mwandb[0m: 🚀 View run [33m/home/vlai-vqa-nle/minhtq/vqa-nle/ms-swift/examples/train/grpo/output/dat-vinternvl3B/v5-20251110-235619[0m at: [34mhttps://wandb.ai/minhdeptrai/huggingface/runs/scu4ve4m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251110_235646-scu4ve4m/logs[0m
Hoàn thành huấn luyện GRPO Stage 1 - Format + Accuracy + Explanation
